{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b0a1cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4710afb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x122c25940>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = \"/Users/anand.kumar/.cache/zotcite/copy_of_bbt.sqlite\"\n",
    "z = \"/Users/anand.kumar/Zotero/zotero.sqlite\"\n",
    "\n",
    "conn = sqlite3.connect(z)\n",
    "cur  = conn.cursor()\n",
    "queryA = f\"ATTACH DATABASE '{b}' AS bbt;\"\n",
    "cur.execute(queryA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b38f2e98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BBT Tables: [('fieldFormats',), ('charsets',), ('fileTypes',), ('fileTypeMimeTypes',), ('syncObjectTypes',), ('itemTypes',), ('itemTypesCombined',), ('fields',), ('fieldsCombined',), ('itemTypeFields',), ('itemTypeFieldsCombined',), ('baseFieldMappings',), ('baseFieldMappingsCombined',), ('creatorTypes',), ('itemTypeCreatorTypes',), ('version',), ('settings',), ('syncedSettings',), ('items',), ('itemDataValues',), ('itemData',), ('itemNotes',), ('itemAttachments',), ('itemAnnotations',), ('tags',), ('itemRelations',), ('itemTags',), ('creators',), ('itemCreators',), ('collections',), ('collectionItems',), ('collectionRelations',), ('feeds',), ('feedItems',), ('savedSearches',), ('savedSearchConditions',), ('deletedCollections',), ('deletedItems',), ('deletedSearches',), ('libraries',), ('users',), ('groups',), ('groupItems',), ('publicationsItems',), ('retractedItems',), ('fulltextItems',), ('fulltextWords',), ('fulltextItemWords',), ('syncCache',), ('syncDeleteLog',), ('syncQueue',), ('storageDeleteLog',), ('proxies',), ('proxyHosts',), ('relationPredicates',), ('customItemTypes',), ('customFields',), ('customItemTypeFields',), ('customBaseFieldMappings',), ('translatorCache',), ('dbDebug1',)]\n"
     ]
    }
   ],
   "source": [
    "print(\"BBT Tables:\", cur.execute(\"SELECT name FROM bbt.sqlite_master WHERE type='table';\").fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "030a1777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'itemID', '', 1, None, 1),\n",
       " (1, 'itemKey', '', 1, None, 0),\n",
       " (2, 'libraryID', '', 1, None, 0),\n",
       " (3, 'citationKey', '', 1, None, 0),\n",
       " (4, 'pinned', '', 0, None, 0)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur.execute(\"PRAGMA table_info(citationkey);\").fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ee94ba51",
   "metadata": {},
   "outputs": [],
   "source": [
    "citekeys = cur.execute(\"select itemID, itemKey, citationKey from citationkey\").fetchall()\n",
    "items    = cur.execute(\"select itemID, key from items\").fetchall()\n",
    "itemData  = cur.execute(\"select fieldID, valueID from itemData\").fetchall()\n",
    "itemDataValue = cur.execute(\"select value, valueID from itemDataValues\").fetchall()\n",
    "fields = cur.execute(\"select  fieldID, fieldName from fields\").fetchall()\n",
    "# conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ebab73ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 'KHW2498F', 'Polak2017'),\n",
       " (4, 'QZ4IPAQV', 'Polak2016'),\n",
       " (6, '6DGRHLYU', 'Sangid2013'),\n",
       " (8, 'YGGUDQCL', 'Polak1987'),\n",
       " (10, '4MFHUNBV', 'Polak2014'),\n",
       " (14, 'KN3GNAXU', 'Acharya'),\n",
       " (18, 'WQ2CIXZK', 'Argyropoulos2015'),\n",
       " (20, '84HAVXWN', 'Mitzenmacher1988'),\n",
       " (22, 'NYN9JAFT', 'Cai2006'),\n",
       " (24, 'XGN74PFB', 'PeterWriggers2012'),\n",
       " (28, 'AH3PKCWJ', 'Press2000'),\n",
       " (30, 'LYZ2SXKL', 'Cotter2009'),\n",
       " (31, 'T57E9LWP', 'Kerfriden2019'),\n",
       " (32, '37AE3RYY', 'Cotter2010'),\n",
       " (33, 'Z3P3I79V', 'Stuart2010'),\n",
       " (35, 'P6YBSNEN', 'Page2007'),\n",
       " (37, 'MHWV4VWA', 'Kac1966'),\n",
       " (39, 'C24R4FL4', 'Mallick2011'),\n",
       " (41, '8T89CPZT', 'Challamel2000'),\n",
       " (43, 'X5NTIP8Q', 'Flanagan1959'),\n",
       " (45, 'QT7I59WW', 'Kabanikhin2008'),\n",
       " (47, '8JHH63I2', 'Billinge2019'),\n",
       " (56, 'JP828P7I', 'Vincent1990'),\n",
       " (58, 'XVP6ESUA', 'Vishnoi2015'),\n",
       " (61, 'ZV3MJCRK', 'Dixit2012'),\n",
       " (91, 'KMHD79T9', 'Hopenhayn2016'),\n",
       " (98, 'KVHB88B3', 'Pitchik2009'),\n",
       " (102, 'TM77ICP7', 'Nov2018'),\n",
       " (105, '2UD7Y8NR', 'Xun2012'),\n",
       " (107, '4J8VQFF3', 'Panageas2016a'),\n",
       " (133, 'A36NLYGH', 'Beilina1390'),\n",
       " (139, 'TLRSTBP4', 'Vishnoi2013'),\n",
       " (141, 'V7IPA6PF', 'Schindler2003'),\n",
       " (143, 'RJQX4CAD', 'Eigen1988'),\n",
       " (145, 'W34DX4WH', 'Panageas2016'),\n",
       " (147, 'RRJJLKRQ', 'Diaconis2009'),\n",
       " (149, '8GHW6KVJ', 'Ghanem2017'),\n",
       " (270, 'HJ2GJUKM', 'Stuart2017'),\n",
       " (275, '7CEDI6HD', 'Clement2016'),\n",
       " (277, 'C85XBEGL', 'Pedregal2017'),\n",
       " (279, 'FQAEDEPB', 'Kumar2020'),\n",
       " (281, 'ZWI5BE2X', 'Royden2010'),\n",
       " (290, 'UN4UEVVP', 'Evans2010'),\n",
       " (292, 'WRHMBIR4', 'Benning2018'),\n",
       " (295, '89QEPBBX', 'Bredies2008'),\n",
       " (297, 'WY8SJYFL', 'Hestenes1969'),\n",
       " (301, 'GIUQPQU6', 'Rockafellar1973'),\n",
       " (302, 'CYTFSGT7', 'Fortin2000'),\n",
       " (304, 'JQV64ZJ9', 'Figueiredo2009'),\n",
       " (306, '5SW9HXKK', 'Rulla2015'),\n",
       " (308, 'EEVMXG2E', 'Lin2015'),\n",
       " (310, 'ZSMZTCTJ', 'Kale2014'),\n",
       " (312, '88HI884G', 'Dai2015'),\n",
       " (314, 'UIKWGIQ8', 'Terra2012'),\n",
       " (320, 'ZIJP72GB', 'Aboud2019'),\n",
       " (322, 'WVB3X3QG', 'Aharoni1984'),\n",
       " (325, 'G9459S3D', 'Zhao2020'),\n",
       " (327, '5R635KPF', 'Haddock2019a'),\n",
       " (329, 'YBSBB3MW', 'Richtarik2017'),\n",
       " (331, '9CXPLTPC', 'Hefny2017'),\n",
       " (333, 'Y3XFX8EP', 'Tropp2016'),\n",
       " (335, 'DIQ7LN5L', 'DeLoera2017'),\n",
       " (337, '79HTE7YU', 'Haddou2020'),\n",
       " (339, 'TX9IQDAE', 'Haddock2019'),\n",
       " (341, 'DH56LMMZ', 'Candes2008a'),\n",
       " (343, '8M6PSFR3', 'Sheriff2020'),\n",
       " (345, 'T6ALGK5Z', 'Chen2018'),\n",
       " (347, 'LVTDCDNJ', 'Strohmer2009'),\n",
       " (353, 'IME9ZZQJ', 'Necoara2019'),\n",
       " (356, 'BUHNFE55', 'Cegielski2020'),\n",
       " (359, 'VPRN5GHF', 'Lorenz2018'),\n",
       " (361, 'BEYVQHE4', 'Chen2001'),\n",
       " (363, 'VUTD2L26', 'Lorenz2014'),\n",
       " (365, 'RF983IJ3', 'Bernstein2020'),\n",
       " (367, '5EDIRAG2', 'Fornasier2015'),\n",
       " (369, 'PSU4NWDX', 'Nagahara2016'),\n",
       " (371, 'Z79VBMX2', 'Natarajan1995'),\n",
       " (374, 'C74YT8SQ', 'Chen2012'),\n",
       " (376, '4T9KPRM5', 'Sra2006'),\n",
       " (378, 'SYH8KZNE', 'Lorenz2015'),\n",
       " (380, 'RLMUI567', 'Kaczmarz1937'),\n",
       " (386, 'VGZGUCNJ', 'Borgard2020'),\n",
       " (388, 'D4MI8XVT', 'Tanabe1971'),\n",
       " (405, 'WDLVFYRL', 'Natterer2001a'),\n",
       " (407, 'MMP9VPRQ', 'Lin2020'),\n",
       " (409, 'EYTHARPP', 'Dey2018'),\n",
       " (411, 'XE8H3Y3H', 'Hazimeh2020'),\n",
       " (413, 'TWW88JF8', 'Gubin1967'),\n",
       " (415, 'AL9X9BHE', 'Elfving1980'),\n",
       " (417, 'MT6UGWKU', 'Censor1981'),\n",
       " (419, 'A6NAGGAQ', 'Tewarson1969'),\n",
       " (423, 'PF5E9JAI', 'Censor1983'),\n",
       " (425, 'QMPN8RRB', 'Natterer2001'),\n",
       " (427, '34J9TKNK', 'Lorenz2014a'),\n",
       " (429, 'MF96NTJF', 'Aly2016'),\n",
       " (431, 'TX46X33P', 'Bai2018'),\n",
       " (432, 'VSS69KW9', 'Bai2018a'),\n",
       " (433, 'HHLDD6VN', 'Bai2019'),\n",
       " (435, 'VZRC3QKE', 'Censor2001a'),\n",
       " (436, '95D9N8YW', 'Censor2001'),\n",
       " (437, '7CZVWJYY', 'Dokmanic2013'),\n",
       " (438, '7NVZXNFN', 'Durgin2019'),\n",
       " (439, 'QB74Q2ZG', 'Lieb2020'),\n",
       " (441, 'LDBE7W2Q', 'Lunglmayr2018'),\n",
       " (442, 'NW34PSF3', 'Qu2018'),\n",
       " (446, 'Z9S8PHGD', 'Crandall2014'),\n",
       " (448, 'SUWRC5Y9', 'Moorman2021'),\n",
       " (450, 'HQUMWS5X', 'Li2018'),\n",
       " (452, 'GI6925RD', 'Vandenberghe2013'),\n",
       " (454, 'YEPXJDM3', 'StefanKidermann'),\n",
       " (456, 'MLJV6IUI', 'Nesterov2021'),\n",
       " (458, 'VSMIFTFG', 'Nesterov2013'),\n",
       " (460, 'J3D4KXAP', 'Nesterov2007'),\n",
       " (462, 'QM8QLV4W', 'HanneKekkonen2019'),\n",
       " (465, 'WETI5VIH', 'Guan2020'),\n",
       " (467, 'VNIYDAD9', 'Huang2020'),\n",
       " (469, 'FUT5TMMB', 'Nutini2016'),\n",
       " (471, '6E5SB8X8', 'Oswald2015'),\n",
       " (473, 'ENVTBAC7', 'Popa2018'),\n",
       " (475, '8P3KFZIB', 'Beck2009'),\n",
       " (477, 'RVDFLBZR', 'Foucart2013'),\n",
       " (479, 'Z5LR3UYK', 'Rektorys1977'),\n",
       " (485, '5FHLXDFU', 'WalterGautschi2012'),\n",
       " (487, '9FRTCCVW', 'Candes2008'),\n",
       " (489, 'TRNCG7SV', 'Wilson2018'),\n",
       " (491, 'E3I2UYZD', 'Combettes2005'),\n",
       " (493, 'BNY6DVKZ', 'Ikeda2019'),\n",
       " (495, 'HG8KKJXW', 'Shah2018a'),\n",
       " (497, 'J3L29JHJ', 'Nagahara2020'),\n",
       " (499, '7FKHG94Q', 'Cho2003'),\n",
       " (501, 'IFISIJDE', 'Zyoud2017'),\n",
       " (503, 'KCGTZC8X', 'Sciences2014'),\n",
       " (505, 'TFA9C9J3', 'Bian2017'),\n",
       " (507, '7GYZYRFR', 'Korhonen2016'),\n",
       " (509, 'E7PULVTQ', 'Saaty2013'),\n",
       " (513, 'V67NRB5K', 'Saaty2003'),\n",
       " (515, '8SLY8AR8', 'Xu2018'),\n",
       " (517, 'SJSRIGG4', 'BanaECosta2008'),\n",
       " (519, 'I323E2P2', 'Emrouznejad2017'),\n",
       " (521, '82ZMDNIX', 'Wang2006'),\n",
       " (523, '6KLGI39Z', 'Perez2006'),\n",
       " (525, 'HM8M7WRJ', 'Wang2017'),\n",
       " (527, 'P8SC88WX', 'Yang2019'),\n",
       " (529, 'K3G2NVWK', 'Moreno-jimenez2016'),\n",
       " (531, 'VFP28VF9', 'Feichtinger1992'),\n",
       " (534, 'W4QW6JRQ', 'Crandall1951'),\n",
       " (538, 'HLCYPHXQ', 'Steinerberger2021a'),\n",
       " (544, 'BXLPZVV6', 'Pelillo2012'),\n",
       " (546, 'HZTU3C3U', 'Jiao2017'),\n",
       " (548, 'TPFVSV5G', 'Jin2019'),\n",
       " (550, '7FCS6PNW', 'Shah2018'),\n",
       " (556, 'ZMN6RFF5', 'Heckel2018'),\n",
       " (558, 'UEQL35AD', 'Pardalos2017'),\n",
       " (560, '8UQYDKBD', 'Mandal2018'),\n",
       " (564, 'FQV6EMP5', 'Dempe1987'),\n",
       " (566, 'U7EYCXPJ', 'Yuille2001'),\n",
       " (568, '57F2NNHD', 'Debnath2020'),\n",
       " (570, 'M5ZNE2YE', 'Sriperumbudur2012'),\n",
       " (572, '25ZYRQ7W', 'Shen2016'),\n",
       " (574, 'UZMJ8VMK', 'Ahmadi2018'),\n",
       " (576, 'W5LFHVT6', 'Sriperumbudur2009'),\n",
       " (578, 'LUQX94JI', 'Carrasco-gutierrez2019'),\n",
       " (580, 'M7S7S55S', 'Lipp2016'),\n",
       " (582, 'CRLGH2XJ', 'Mjolsness1990'),\n",
       " (584, 'R6HPJ6C5', 'Rangarajan1996'),\n",
       " (586, '6KGNZXBY', 'Ralphs2009'),\n",
       " (588, 'F7I8V8EZ', 'WEn1991'),\n",
       " (590, 'RPU6LD4M', 'Yeh1996'),\n",
       " (592, 'EAHRA8M8', 'Colson2005'),\n",
       " (594, 'DM8QYP53', 'Ben-ayed1990'),\n",
       " (596, 'SD7778N4', 'Dempe2020'),\n",
       " (603, '253SZQ85', 'Fliege2006'),\n",
       " (606, 'ZF6XYRPG', 'Bard1983'),\n",
       " (608, 'XSW4NMKH', 'Blair1992'),\n",
       " (610, 'ET3MS7NC', 'Gale2016'),\n",
       " (612, '5U45VJFY', 'Candler1988'),\n",
       " (615, 'XICLQE69', 'Wen1989'),\n",
       " (618, 'TM8EZDRS', 'unlu1987'),\n",
       " (622, 'XGAGTYCV', 'Gallo1977'),\n",
       " (624, '4P8XUHCA', 'Dempe2005'),\n",
       " (628, 'N4F8UE57', 'Chatterjee2005'),\n",
       " (630, '3RZCATAN', 'Gupta2022'),\n",
       " (633, 'F86H99HR', 'Sinha2018'),\n",
       " (635, 'LBNYQCYI', 'Helou2017'),\n",
       " (637, 'YPVY67LG', 'Xu2023'),\n",
       " (639, 'RUGZRURD', 'Dagreou2023'),\n",
       " (641, '6EJKG24B', 'Antil2023'),\n",
       " (643, 'WWPIMYFJ', 'Alesiani2023'),\n",
       " (645, 'K3KMVWJU', 'Li2023a'),\n",
       " (647, 'FA7WG6GX', 'Zhou2022'),\n",
       " (649, 'XDKFE4C6', 'Ji2023'),\n",
       " (651, 'ADGV57P4', 'Lu2023'),\n",
       " (653, 'X9GETDTD', 'Shen2023'),\n",
       " (655, 'YIBEDJ89', 'Li2023'),\n",
       " (657, 'YJRCTNBF', 'Huang2023'),\n",
       " (659, 'MB3PV964', 'Mehlitz2022'),\n",
       " (661, 'UL4U7BAC', 'Doron2022'),\n",
       " (663, 'FQB6LZJ4', 'Cleach2022'),\n",
       " (665, 'DXKQUNMN', 'Cunis2022'),\n",
       " (667, 'CFXI8JAW', 'Guglielmi2023'),\n",
       " (669, 'BMZEGTY7', 'Lu2023a'),\n",
       " (671, 'GDC37DJN', 'Chen2023'),\n",
       " (673, 'KAE6K34D', 'Bai2022'),\n",
       " (675, 'CESDTW8Q', 'Jiang2022'),\n",
       " (677, 'IQ825D6K', 'Shen2022'),\n",
       " (679, '5367LCVU', 'Yang2022'),\n",
       " (681, '5Q6BWBWV', 'Lamontagne2022'),\n",
       " (683, 'NK5ANRRN', 'Chen2022a'),\n",
       " (685, 'S68TSLNF', 'Chen2022b'),\n",
       " (687, 'DT386PQ6', 'Sharrock2022'),\n",
       " (689, 'SXLZI77J', 'Gao2022a'),\n",
       " (691, 'AGQTPT2D', 'Hu2022'),\n",
       " (693, 'XQ492XTK', 'Ding2022'),\n",
       " (695, '6TXXEUBF', 'Liu2022'),\n",
       " (697, '87SPSVBW', 'Chen2022c'),\n",
       " (699, '3QN8LT7X', 'Gao2022'),\n",
       " (701, '7WAJK947', 'Tarzanagh2022'),\n",
       " (703, 'HINCII67', 'Yu2023'),\n",
       " (705, 'TL4JSHM6', 'Sow2022'),\n",
       " (707, 'U7VSMGYS', 'Ghosh2022'),\n",
       " (709, 'EST4982Y', 'Shirai2022'),\n",
       " (711, 'PIINZZSZ', 'Bian2022'),\n",
       " (713, 'T9IPBG26', 'Pan2022'),\n",
       " (715, 'YS9ZQ4HQ', 'Han2021'),\n",
       " (717, '8GI6HS77', 'Suonpera2022'),\n",
       " (719, '72DJH6VL', 'Dagreou2022'),\n",
       " (721, 'V6KYW76N', 'Lee2022'),\n",
       " (723, 'GFD27SSZ', 'Huang2022'),\n",
       " (725, 'ZJYHTBDY', 'Liu2022a'),\n",
       " (727, 'E557DDDB', 'Li2022'),\n",
       " (729, 'MCJ7CNMD', 'En-naciri2022'),\n",
       " (731, 'DCNALP8B', 'Friedemann2022'),\n",
       " (733, 'ZTE5RHYL', 'Garcia2022'),\n",
       " (735, 'IZJ6SK2J', 'Dyro2022'),\n",
       " (737, 'FTX5EYAJ', 'Zucchet2022'),\n",
       " (739, 'U4FTS88Y', 'Huang2021'),\n",
       " (741, 'BWLLQG63', 'Chen2021'),\n",
       " (743, 'ZHVKVS69', 'Huang2022a'),\n",
       " (746, '5K3AE3BT', 'Sato2021'),\n",
       " (748, 'KYW7LMT4', 'Poon2021'),\n",
       " (750, '2M38SR4F', 'Bui2022'),\n",
       " (752, 'JLKZL69I', 'Bao2021'),\n",
       " (754, 'RCTEHWMF', 'Fan2021'),\n",
       " (756, 'EFARBJCI', 'Crockett2022'),\n",
       " (758, 'IWCFKRWW', 'Liu2021'),\n",
       " (760, 'DPGI86W5', 'Sundar2021'),\n",
       " (762, 'XCLF7C4H', 'Ke2022'),\n",
       " (764, 'I5CUKMSD', 'Ma2021'),\n",
       " (766, '2G9MIUPZ', 'Ji2021'),\n",
       " (768, 'E64CWC84', 'Buchheim2022'),\n",
       " (770, '4FGKJDIE', 'Mehlitz2021'),\n",
       " (772, 'JLYLC8QJ', 'Basciftci2020'),\n",
       " (774, 'WW7N4GQ6', 'Fliege2021'),\n",
       " (776, 'AJWYDT2V', 'Bai2022a'),\n",
       " (778, '7M5M5AFA', 'Taninmis2019'),\n",
       " (780, 'R4XU57TF', 'Mehlitz2020'),\n",
       " (782, 'KX88B488', 'Mordukhovich2020'),\n",
       " (784, 'MAVUFTDA', 'Ye2020'),\n",
       " (786, 'WTDSXS2A', 'Sessa2020'),\n",
       " (788, 'GL3T9J78', 'Sinha2020'),\n",
       " (790, 'JBN9BIKY', 'Benko2021'),\n",
       " (792, 'BNVSNCSI', 'Yousefian2021'),\n",
       " (794, 'J3F734QF', 'Wang2020'),\n",
       " (796, '2Z7RQCNR', 'Hao2020'),\n",
       " (798, 'LXJZFP6M', 'Gao2020'),\n",
       " (800, '4QQ8SMFX', 'Bae2020'),\n",
       " (802, 'PCAWQ2X3', 'Shaker2021'),\n",
       " (804, 'C7M2VSLM', 'Tong2022'),\n",
       " (806, '8TDTCJW6', 'Li2020'),\n",
       " (808, 'FCM6R2BF', 'Chang2021'),\n",
       " (810, 'KW42Z4UD', 'Ji2020'),\n",
       " (812, 'V296MRRP', 'Borsos2021'),\n",
       " (814, 'SUTM5Q9U', 'Picallo2022'),\n",
       " (816, 'IZG9TLMD', 'Seth2021'),\n",
       " (818, 'QTV3M7T9', 'Ji2021a'),\n",
       " (822, 'T23ECC4V', 'Kosmas2022'),\n",
       " (824, '74QDVUN8', 'Grazzi2020'),\n",
       " (826, 'QLGS3FMX', 'Kontonis2020'),\n",
       " (828, 'WSU6GPVE', 'Roh2020'),\n",
       " (830, '2CW4LMYK', 'Bergounioux2006'),\n",
       " (832, 'RZVEM7E9', 'Gutfraind2011'),\n",
       " (834, 'YYCFLDCQ', 'Ochs2016'),\n",
       " (836, 'VLEB98Z3', 'Mackay2019'),\n",
       " (838, 'USKLZNPB', 'Khanduri2021'),\n",
       " (840, 'IQUPQ2Y2', 'Mori2021'),\n",
       " (842, 'T5IYSD49', 'Cao2022'),\n",
       " (844, 'JFMRWU7H', 'Sun2022'),\n",
       " (848, 'UQ3ZP9WV', 'Sinha2020a'),\n",
       " (850, 'U4WGC3PK', 'Ouattara2018'),\n",
       " (852, 'QD2ZTDNF', 'Brannstrom2016'),\n",
       " (855, 'BIBXG6VL', 'Liu2016'),\n",
       " (857, 'TIMTYM6M', 'Holler2018'),\n",
       " (859, 'R22I3PLQ', 'Ye1997'),\n",
       " (870, 'KX96NT9Y', 'Lin2014'),\n",
       " (872, '8VNM7A2I', 'Dempe2012'),\n",
       " (879, 'B64TVTKP', 'Iyer'),\n",
       " (881, '9Y76YMZI', 'Rockafellar1993'),\n",
       " (883, 'WBVL9TSH', 'Borwein2016'),\n",
       " (885, '23TVFPBQ', 'Nunemacher2003'),\n",
       " (887, 'HYPCVZJM', 'Brezhneva2012'),\n",
       " (891, 'ULQGKA8I', 'Borwein1981'),\n",
       " (893, 'DYHMYXDI', 'Parikh2014'),\n",
       " (896, 'GX8DJPYF', 'Das2022'),\n",
       " (915, 'WRNQAA7T', 'Ehrhardt2021'),\n",
       " (928, 'J6UB5VSV', 'Tomkins1968'),\n",
       " (930, '88L5JPY3', 'Lei2018'),\n",
       " (932, 'CZL7RGSJ', 'Tanaka1981'),\n",
       " (935, 'R69KWBZB', 'Crandall1971'),\n",
       " (937, 'N44RWBNR', 'Ikeda2021'),\n",
       " (941, 'GVSL54Z6', 'Mousavi2016'),\n",
       " (944, 'TD6I25LG', 'Bubeck2015'),\n",
       " (947, 'VXNQ6MRC', 'Aggarwal2014'),\n",
       " (956, 'TV93ENB2', 'Schopfer2019'),\n",
       " (959, 'NDN3GLWN', 'Deutsch1997'),\n",
       " (961, 'H6ZP6LB5', 'Donoho2006'),\n",
       " (963, 'GUWQGCPC', 'Mansour2013'),\n",
       " (965, 'U9D53TQ3', 'Schmidt2015'),\n",
       " (967, '4KF9M5N8', 'Ben-ayed1993'),\n",
       " (969, 'TLT6W8WR', 'Moscoso2012'),\n",
       " (971, 'AEKQM9AP', 'Hegde2019'),\n",
       " (972, 'XP2LVELR', 'Mailhe2011'),\n",
       " (974, '9VZSGX4V', 'Du2020'),\n",
       " (976, 'ACY2Z6BX', 'Yuan2014'),\n",
       " (978, 'SYLW8VD7', 'Dax2006'),\n",
       " (980, 'KNYR4QI4', 'Osher2023'),\n",
       " (983, '2PLBN8GG', 'Rockafellar2006'),\n",
       " (985, 'LMIT9F5F', 'Haurie1990'),\n",
       " (987, 'BXQFVNZG', 'Deutsch2001'),\n",
       " (989, 'G83MTYHE', 'Finn2017'),\n",
       " (991, 'BQAQ6S6P', 'Domke2012'),\n",
       " (993, 'BGFRXZZU', 'Hataya2023'),\n",
       " (1000, 'AVTWNRQ4', 'Hong2023'),\n",
       " (1002, '32XPMGIN', 'Yang2021'),\n",
       " (1004, 'LK4LNP5M', 'Ghadimi2018'),\n",
       " (1008, 'R2GG9C3C', 'Bazaraa2006'),\n",
       " (1014, 'HZ6RZ4WV', 'Fletcher2000'),\n",
       " (1022, 'FZ2J6UW9', 'Colson2007'),\n",
       " (1024, '6RXCS6CX', 'Borkar1997'),\n",
       " (1026, 'DSBFU4IC', 'Avrachenkov2022'),\n",
       " (1029, 'HG63K2JX', 'Chen2022'),\n",
       " (1031, 'NF9328RX', 'Ghadimi2013'),\n",
       " (1556, 'SX9HFZV6', 'Ailon2012'),\n",
       " (1574, 'AZSERNWI', 'Ailon2008'),\n",
       " (1610, '5VAKT6FZ', 'Adams2012'),\n",
       " (1617, '4BFIEB8W', 'Agaskar2015'),\n",
       " (1648, '2JZ9EPED', 'Parlett1974'),\n",
       " (1841, '8ZG6NSPD', 'Rudin2008'),\n",
       " (1892, 'G9GAP343', 'Rajeswaran2019'),\n",
       " (1904, '4TCHH3DV', 'Grant2018'),\n",
       " (1930, 'NDAXQPTT', 'okten2005'),\n",
       " (1931, 'WKEIB3P2', 'Wu2019'),\n",
       " (1932, 'SI7RCS5P', 'Acebron2020'),\n",
       " (1933, 'PZQ39PPS', 'Wasow1952'),\n",
       " (1934, 'VUTZNP8L', 'Forsythe1950'),\n",
       " (1937, 'M5QUNHC3', 'Strassen1969'),\n",
       " (1940, 'M8742NRY', 'Bertsekas2016'),\n",
       " (1980, 'IXUQD4AS', 'Udell2016'),\n",
       " (1985, 'MVNAP3IH', 'Tardella1990'),\n",
       " (1988, 'RPTLPDGS', 'Kerdreux2018'),\n",
       " (1989, 'UITHCF6S', 'Aubin1976'),\n",
       " (1990, 'R8NYDN9B', 'Cassels1975'),\n",
       " (1999, 'XCXC32JT', 'Khan1974'),\n",
       " (2001, 'J6T5QPNQ', 'DiGuglielmo1977'),\n",
       " (2002, '68KXJH92', 'Kerdreux2020'),\n",
       " (2021, 'RPWK3AG3', 'Starr2008'),\n",
       " (2023, '2RCYD5AE', 'Kumar2023'),\n",
       " (2025, '6G2NIYY2', 'Aubin2009'),\n",
       " (2049, '733KMJE5', 'Boyd2009'),\n",
       " (2050, 'UAMAF28K', 'Blasjo2005'),\n",
       " (2052, 'TYT7VIK5', 'Duchi2021'),\n",
       " (2060, 'TIHSDGWC', 'Bulicek2014'),\n",
       " (2062, 'MJBT3QGE', 'Bressan2014'),\n",
       " (2064, '7YBJL2W4', 'Allen2014'),\n",
       " (2066, 'NIS9MWY6', 'Couellan2016'),\n",
       " (2068, 'H4SILD9W', 'Akhtar2022'),\n",
       " (2071, 'CA5RTJC3', 'Danilova2022'),\n",
       " (2074, 'HHSUQXD6', 'Bush2011'),\n",
       " (2076, 'GLKPGHK9', 'Edelman2005'),\n",
       " (2077, '89X4LYKI', 'Livan2018'),\n",
       " (2081, 'NAZ52QSA', 'Quadrat2006'),\n",
       " (2083, 'GXRUTHIA', 'Quadrat2006a'),\n",
       " (2085, 'Y5ZRIZFC', 'Quadrat2003'),\n",
       " (2088, 'RSZDH8XB', 'Quadrat2003a'),\n",
       " (2091, 'QWUDCTW7', 'Lewis2016'),\n",
       " (2092, 'SSG4LNHE', 'Terrell2018'),\n",
       " (2097, 'AM5T36AN', 'Terrell1999'),\n",
       " (2100, '4AI3NRNX', 'Evans2021'),\n",
       " (2102, 'W53XHJFE', 'Evans2021a'),\n",
       " (2106, 'UVDSIKLJ', 'Netrapalli2014'),\n",
       " (2113, 'NUIVC8XC', 'Kirchheim2009'),\n",
       " (2116, 'CUN9HMSZ', 'Meshulam1996'),\n",
       " (2139, 'MYG5HYEB', 'DeOliveira2020'),\n",
       " (2141, 'L84AZXSF', 'Lasserre2002'),\n",
       " (2142, 'LFHGKH5R', 'zotero-2142'),\n",
       " (2145, 'NEVPSSTE', 'Polyakova'),\n",
       " (2147, 'NDDCVQTC', 'zotero-2147'),\n",
       " (2159, 'QHYBBJVD', 'zotero-2159'),\n",
       " (2162, 'SYGSGKBS', 'Whitesides2004'),\n",
       " (2164, 'C25WS4RA', 'Roscoe'),\n",
       " (2166, 'BGW8YZT4', 'keshav'),\n",
       " (2199, 'ZGBMABRG', 'Diakonikolas2019'),\n",
       " (2206, 'IBGVHNXB', 'Borkar2023a'),\n",
       " (2008, 'QDVZRKW6', 'Fradelizi2017'),\n",
       " (2213, '9R87NLTA', 'Gardner2002'),\n",
       " (2223, 'B3M9YCEK', 'Milenkovic2007'),\n",
       " (2224, '3HUSHI9S', 'Guibas1986'),\n",
       " (2225, '3N8SVK7A', 'sir2006'),\n",
       " (2219, 'QAXRATHA', 'Peternell2007'),\n",
       " (2245, 'XMRSNPK4', 'Oberman2007'),\n",
       " (2246, 'QYXVKXWR', 'Abbasi2018'),\n",
       " (2247, 'V3I97W65', 'Carlier2012'),\n",
       " (2248, 'TAC5CCVG', 'Li2021'),\n",
       " (2249, 'TKXBP9NY', 'Oberman2008'),\n",
       " (2250, 'Y2ZTWYCK', 'Oberman2017'),\n",
       " (2251, 'ZI55TRFC', 'Oberman2011'),\n",
       " (2252, 'ZKNC9EJH', 'Vese1999'),\n",
       " (2263, 'P3Q77HTP', 'Griewank1990'),\n",
       " (2265, 'SZFQTK9W', 'Brighi1994'),\n",
       " (2267, 'RQLTTV5T', 'Lucet1996'),\n",
       " (2271, '94Y24AA9', 'Lucet1997'),\n",
       " (2274, '6WN9AGEY', 'Touchette'),\n",
       " (2280, 'T3XYVZIP', 'Dyke2014'),\n",
       " (2284, 'BPS4WRHB', 'Chizat'),\n",
       " (2289, 'EB7CBIL2', 'Ma2019'),\n",
       " (2290, '9EY452RP', 'Hoeksema2023'),\n",
       " (2293, 'ZHUPKXN2', 'Chizat2020'),\n",
       " (2297, 'DL84AV32', 'Borkar2005'),\n",
       " (2323, 'G5RGAS9N', 'Zhao2023'),\n",
       " (2326, '3ADSLNVF', 'Chung2000'),\n",
       " (2329, 'DPXUVVDG', 'Bietti2021'),\n",
       " (2332, '53RJAB6B', 'Villani2023'),\n",
       " (2409, 'LHNL6N4K', 'Brezis2011'),\n",
       " (2415, 'U8DG8YD2', 'Folland1999'),\n",
       " (2420, 'JHARL7CQ', 'Malliavin1995'),\n",
       " (2423, 'TDBC275P', 'Billingsley1995'),\n",
       " (2424, '8KUMGKB2', 'Hajek2015'),\n",
       " (2425, '7VJIRWUM', 'Grimmett2007'),\n",
       " (2426, 'IWI8WTJT', 'Kumar2012'),\n",
       " (2462, '9Q4PUE3N', 'Thorpe'),\n",
       " (2463, 'E2XJGMA3', 'Bach'),\n",
       " (2464, 'WQHNK93Z', 'Jain2017'),\n",
       " (2475, 'HVV5DMT2', 'Ross2013'),\n",
       " (2478, '4JXH2XM2', 'Sontag1998'),\n",
       " (2480, 'VPXP2AK5', 'Spivak1965'),\n",
       " (2512, 'YVSPZIPB', 'Liberzon2012'),\n",
       " (2514, 'CYF3H42W', 'Zhao2022'),\n",
       " (2520, 'J8RIPNJ2', 'Salsa2016'),\n",
       " (2526, 'ULZP4LQ7', 'Jiang2023'),\n",
       " (2539, 'ASN6GNPX', 'Perko2009'),\n",
       " (2545, 'QME944RQ', 'Santambrogio2015'),\n",
       " (2549, 'BLK28USM', 'Howe2012'),\n",
       " (6494, '9DHKR7F5', 'Ross2013a'),\n",
       " (6502, 'L67A9ZG3', 'oksendal2013'),\n",
       " (6504, '8FISQTEU', 'zotero-6504'),\n",
       " (6512, '3MLSF4GH', 'Dewaskar'),\n",
       " (6513, 'REDSUE33', 'Li2020a'),\n",
       " (6520, '7UWS8YYW', 'Neumann1979'),\n",
       " (6523, 'HUFFMRIC', 'Nisan2007'),\n",
       " (6525, 'G8AXY2EP', 'Jurg1992'),\n",
       " (6526, 'GH7IVJP2', 'Raghavan1970'),\n",
       " (6534, 'JDDMSGAY', 'Axler2024'),\n",
       " (6535, 'F79MNIM5', 'Ekeland2006'),\n",
       " (6537, 'X6U8BEQ8', 'Parthasarathy1971'),\n",
       " (6541, 'XVZDCPUL', 'Rockafellar1996'),\n",
       " (6546, 'MU9HSBIG', 'zotero-6546'),\n",
       " (6564, 'VM8XSFG6', 'Mangasarian1964'),\n",
       " (6550, 'WRSZCDCC', 'Mclennan2010'),\n",
       " (6548, 'FCCGDB8L', 'Lemke1964'),\n",
       " (6566, 'FTG8QBMD', 'Kuhn'),\n",
       " (6569, '987PGFQV', 'zotero-6569'),\n",
       " (6580, 'NHM9EKT4', 'zotero-6580'),\n",
       " (6582, 'LW9DN8ED', '2018'),\n",
       " (6589, 'Z6Q6ESC9', 'Beck2013'),\n",
       " (6590, 'ZK757MRG', 'Chen2020'),\n",
       " (6598, 'DDK2DSG6', 'Kontogiannis2011'),\n",
       " (6596, 'NPG6855A', 'Robbins1951'),\n",
       " (6604, 'FGKUKTI2', 'Schrijver2011'),\n",
       " (6605, 'KK8UI6P3', 'Borkar1995'),\n",
       " (6606, 'UTQJR277', 'Norris1998'),\n",
       " (6640, '8HIJLTYX', 'Avis2010'),\n",
       " (6656, 'WQMZB9Y5', 'Pauwels2016'),\n",
       " (6658, '2JSZ378F', 'Dashti2017'),\n",
       " (6670, 'ZPYIZLPA', 'Bertsimas2020'),\n",
       " (6671, '6TGWFKRI', 'Dey2021'),\n",
       " (6678, 'JP7PBGDM', 'Hermer2022'),\n",
       " (6681, 'PWP6TRJY', 'Joseph2020'),\n",
       " (6683, 'LRHT9IA4', 'Bedi2021'),\n",
       " (6685, 'TGZS2GAG', 'Dillon2016'),\n",
       " (6687, 'CDWNYPIU', 'Wang2011'),\n",
       " (6689, 'XNB9BHCQ', 'Dong2020'),\n",
       " (2535, 'SHUUHMFJ', 'Hirsch2013'),\n",
       " (6820, 'EUTTM3QP', 'Norris'),\n",
       " (7219, 'YE8AL5EC', 'Pang2010'),\n",
       " (7383, 'TN7ALP6S', 'Karimi2016'),\n",
       " (2104, 'L6GK6MFG', 'Evans'),\n",
       " (7463, 'GUT68ITQ', 'Ardizzone2019'),\n",
       " (7559, 'LHLWVIXR', 'Kucukyavuz2022'),\n",
       " (7560, 'VNHXNV7B', 'Vancroonenburg2019'),\n",
       " (7561, 'T2BS2YNW', 'Peng'),\n",
       " (12, 'RK9FV68J', 'Aminikhanghahi2017'),\n",
       " (16, 'XD2QDF6R', 'Truong2018'),\n",
       " (26, 'JRFNKVVT', 'Andrew1998'),\n",
       " (7583, 'WU63C4YN', 'Lampariello2017'),\n",
       " (7586, 'P7YVHZEW', 'Harwood2023'),\n",
       " (7589, '4IYUWGHI', 'Garg2021'),\n",
       " (7592, 'DCD846RR', 'Garg2023'),\n",
       " (7594, 'ULW867T9', 'Baranwal2023'),\n",
       " (7598, 'QGKF53E9', 'Chen2020a'),\n",
       " (7600, 'GAIIR9TL', 'Santambrogio2017'),\n",
       " (7602, '4YZDPSDI', 'Garg2022'),\n",
       " (7604, 'MGA3TLYL', 'Garg2023a'),\n",
       " (7606, '6GVSFAUV', 'Budhraja2022'),\n",
       " (7609, 'P7LFRG9U', 'Polyakov2022'),\n",
       " (7611, 'SVUGDZK5', 'Polyakov2019'),\n",
       " (7613, 'U6NKG6QQ', 'Polyakov2012'),\n",
       " (7615, 'XKAKTMFV', 'Degroot1974'),\n",
       " (7677, 'P4GT2D7Q', 'Ramirez2024'),\n",
       " (7680, 'DJXYCKFC', 'Beznosikov2023'),\n",
       " (7686, 'KLLYDBHK', 'Gorbunov2022'),\n",
       " (7707, 'DCJY5JKH', 'Meng'),\n",
       " (7726, '76TH2GMP', 'Karimi2020'),\n",
       " (7738, '56PZGRND', 'Nesterov2017'),\n",
       " (7742, 'LXBUSJ2D', 'He2023'),\n",
       " (7748, 'ZWLANXSB', 'Chakrabarti2021'),\n",
       " (7750, 'FPQPQFTU', 'Chakrabarti2020'),\n",
       " (7753, 'TR46MDJT', 'Chakrabarti2021a'),\n",
       " (7756, 'VXLLIZZD', 'Chakrabarti2024'),\n",
       " (7759, 'KCAMICM6', 'Pearlmutter1994'),\n",
       " (7761, 'Y236DCLL', 'Bernard2018'),\n",
       " (7763, 'V2YWWR4I', 'Cerone2024'),\n",
       " (7766, 'XJ3JSDG6', 'Lessard2016'),\n",
       " (7770, 'FF4VZCZB', 'Borkar2024'),\n",
       " (7773, '3JG2TTU3', 'Sohrabi2024'),\n",
       " (7776, 'N2QWIIRS', 'Stooke2020'),\n",
       " (7783, '3SKSEPUU', 'Casti2024'),\n",
       " (7785, '9V8V8MKM', 'Recht'),\n",
       " (7787, 'BAXV54PN', 'Platt1987'),\n",
       " (7793, 'LVAI3AJ6', 'Chambolle2015'),\n",
       " (7795, 'HDYUPIKM', 'Muthukumar'),\n",
       " (7797, 'AYX2NTTI', 'Braides1993'),\n",
       " (7799, 'BQLRGK8U', 'Calder'),\n",
       " (7801, 'B47L87RA', 'Attouch1996'),\n",
       " (7811, 'DBSLNPCI', 'Nguyen'),\n",
       " (7813, 'D582IJA7', 'Arsov2019'),\n",
       " (7816, 'UDXME7M7', 'Hinton2002'),\n",
       " (7818, '4BFFT2QW', 'Maaten2009'),\n",
       " (7820, '5CWI3RNN', 'SvenLeyffer2024'),\n",
       " (7822, 'DZDT7HTU', 'Daskalakis'),\n",
       " (7825, '4Q4F4I5D', 'Codenotti2011'),\n",
       " (7834, 'T5QRULZF', 'Harwood2017'),\n",
       " (7837, '9778BF5F', 'Guignard1969'),\n",
       " (7840, 'ZINNGGYJ', 'Izmailov2009'),\n",
       " (7862, 'A9E84PHB', 'Dorsch2012'),\n",
       " (7864, '7H27TX7X', 'Hoheisel2008'),\n",
       " (7866, '5LGH473B', 'Hoheisel2009'),\n",
       " (7876, 'ZXQF4GQP', 'Dussault2019'),\n",
       " (7910, 'C3XLGBTZ', 'Achtziger2008'),\n",
       " (7944, 'ANXWV3IN', 'Mangasarian1993'),\n",
       " (7946, 'VZ7BRXYN', 'Andreani2011'),\n",
       " (7948, 'MNBZI49C', 'Izmailov2012'),\n",
       " (7950, 'XZGSSPUZ', 'Guo2022'),\n",
       " (7952, 'L6UJV5FP', 'Luo2010'),\n",
       " (7954, '48ZT7QX6', 'Andreani2018'),\n",
       " (7958, 'X9ZJMGIT', 'Hoheisel2012'),\n",
       " (7965, '83GLC3UG', 'Steffensen2010'),\n",
       " (7968, '4QIX7V6J', 'Scholtes2001'),\n",
       " (7983, 'TL9T9MBD', 'Fukushima1999'),\n",
       " (7991, '74JC6NWT', 'Jongen1991'),\n",
       " (7999, 'FS9BMUIT', 'Robinson1975'),\n",
       " (8006, 'JA893DVY', 'Robinson1976'),\n",
       " (8016, 'FNYZEJYX', 'Chen1996'),\n",
       " (8022, 'U7CQVDD6', 'Scheel2000'),\n",
       " (8034, 'IVMUTJ2K', 'Scholtes2004'),\n",
       " (8044, 'LWG9WJYQ', 'Outrata1998'),\n",
       " (8048, '28UMGK7G', 'Fletcher2004'),\n",
       " (8064, 'HE2JLYX2', 'Wilson2021'),\n",
       " (7768, 'A2RABNVG', 'Hu2017'),\n",
       " (7736, 'XRZVNQ8U', 'Zhang2022'),\n",
       " (540, 'CQ7HWCU7', 'Stewart1999'),\n",
       " (542, 'R25M9S8R', 'Stewart1998'),\n",
       " (7617, 'RCMHIXNS', 'Baranwal2024'),\n",
       " (255, 'Q7YDEKGJ', 'Yosida1967'),\n",
       " (273, '7I2IXUME', 'Li2017'),\n",
       " (135, '6XJXGAJK', 'Lebedev2003'),\n",
       " (8069, '4XY63AV6', 'Gale1951'),\n",
       " (2469, 'X6FZTY3K', 'Srinath2009'),\n",
       " (2388, 'TAGXADRL', 'Saxe2001'),\n",
       " (2305, 'TV7EF8NR', 'Sterman2000'),\n",
       " (2401, 'EXSNRXTC', 'Kolmogorov1954'),\n",
       " (2396, '59EDJEQK', 'Kolmogorov1960'),\n",
       " (2317, 'V7HQ9MXB', 'Pritchard2010'),\n",
       " (49, 'CDY9R2U2', 'Palmieri2020'),\n",
       " (2405, '87MZZDKG', 'Kreyszig1989'),\n",
       " (2315, 'HC48X7LX', 'Neamen2010'),\n",
       " (2465, 'X5T7DYXD', 'Das2017'),\n",
       " (2479, 'KYBHKAVS', 'Timoshenko2009'),\n",
       " (6654, 'NPWJRJZ5', 'Costin'),\n",
       " (2509, 'UNBW8EBI', 'Raginsky2019'),\n",
       " (2411, 'SG3VP3PZ', 'Balamurugan2019'),\n",
       " (2428, '45JPUPDP', 'Borkar2020'),\n",
       " (2427, 'G6MQSPLP', 'K2020'),\n",
       " (2312, 'TL5MUV7C', 'Reddy2014'),\n",
       " (6593, '9G5G4W5K', 'Tibshirani'),\n",
       " (7702, '5K3XAJ6L', 'Cherukuri2016'),\n",
       " (7697, 'WSG92MG3', 'Cortes2019'),\n",
       " (7706, 'J9K8KK2I', 'Lygeros2003'),\n",
       " (7712, 'LH7T69A3', 'Rockafellar1971'),\n",
       " (7720, 'SQ726MI9', 'Benzi2005'),\n",
       " (7689, 'TQ4QXSCX', 'Qu2019'),\n",
       " (7723, 'RA3DP3NL', 'Bergamaschi2004'),\n",
       " (7709, 'UQHZW2Y2', 'Tang2020'),\n",
       " (7704, 'IBETDKI2', 'Feijer2010'),\n",
       " (7699, '74BMW36R', 'Dhingra2019'),\n",
       " (8077, '6RNZQYKF', 'Shen2023a'),\n",
       " (949, '4KFX7LWU', 'Bard1984'),\n",
       " (8079, 'QB3EMVYW', 'Du2019'),\n",
       " (8085, 'LBHMTUID', 'Camponogara2016'),\n",
       " (8087, '3JMLJXSI', 'Dauphin2014'),\n",
       " (8091, '7DSXUVHI', 'Pillo1994'),\n",
       " (511, 'SRTWUGVT', 'Steinerberger2021'),\n",
       " (7886, '8LDBL5VL', 'Hoheisel2009a'),\n",
       " (1682, 'AZCSU4V9', 'Khalil2014'),\n",
       " (483, 'VMURAD7R', 'Nesterov2018'),\n",
       " (562, 'JX3BNSI3', 'Centre1985'),\n",
       " (889, 'UX3MGI8B', 'Dutta2011'),\n",
       " (996, 'Y9MKWRDB', 'Shapiro2021'),\n",
       " (998, '2WMXG5V8', 'Borkar2008'),\n",
       " (1012, 'K3B6252W', 'Bazaraa2009'),\n",
       " (1018, '7YSSSS5Y', 'Forst2010'),\n",
       " (1020, 'V47UFD67', 'Nocedal2006'),\n",
       " (1010, 'XLNV8YQT', 'Padregal2003'),\n",
       " (1951, 'EC8HECN7', 'Tihomirov1990'),\n",
       " (1846, 'CQFE96PZ', 'Bertsekas2009'),\n",
       " (1859, 'R37HP73W', 'Boyd2004'),\n",
       " (1961, 'RGL5BVFW', 'Hardt2018'),\n",
       " (1968, 'RNYA5SAG', 'Borkar2023'),\n",
       " (1977, 'LGIPAX7M', 'Bonnans2019'),\n",
       " (2211, 'YPF8RK89', 'Schneider2014'),\n",
       " (7144, 'NA5UHIE8', 'Bazaraa1976'),\n",
       " (7217, 'AZ9RD4MH', 'Borwein2006'),\n",
       " (7485, 'LQW3DPKF', 'Bertsekas1996'),\n",
       " (8083, 'WJP8WVDN', 'Rothvoss2020'),\n",
       " (8051, '5ZVK5NKM', 'RyanTibshirani2019'),\n",
       " (8130, 'HALFM5Q3', 'Magal2022'),\n",
       " (8132, 'LRIPR8GQ', 'Magal2022a'),\n",
       " (8134, '6EZ984XQ', 'Magal2022b'),\n",
       " (8159, 'RQK3L7C9', 'Lei'),\n",
       " (8170, '5ECNRFJ4', 'Pata2019'),\n",
       " (8173, 'RBGAFZ8Z', 'Ray2023'),\n",
       " (8165, 'X2CC7EL5', 'Bach2016'),\n",
       " (8175, 'E3HHTNYJ', 'Anderson1972'),\n",
       " (8188, 'SFR4JTHQ', 'Tibshirani2011'),\n",
       " (8189, 'DRKIX57Q', 'Kovalev2022'),\n",
       " (8199, '78JBZTIU', 'Li2019'),\n",
       " (8201, 'JFRTDCNU', 'Tupitsa2020'),\n",
       " (8205, 'TDY57UIG', 'Nemeth2015'),\n",
       " (8209, 'C8GVBIK8', 'Salmon2024'),\n",
       " (8210, 'UDJBKUTD', 'Pedregosa2013'),\n",
       " (8214, '8IQK94FZ', 'Wang2022'),\n",
       " (8215, 'AE56J47N', 'Yang2019a'),\n",
       " (8217, 'T4EM8DU9', 'Brunk1970'),\n",
       " (8219, '5JU6B2BV', 'Han1988'),\n",
       " (8225, 'H68YWB9L', 'Beck2015'),\n",
       " (8230, '488DV58T', 'Beck2009b'),\n",
       " (8071, 'DLDZGVE5', 'Lu2024'),\n",
       " (8074, 'SDIX8WX2', 'Shen2024'),\n",
       " (8239, 'SH2J5LD2', 'Ruppert2009'),\n",
       " (8240, '5TEQWAX4', 'Yuan2021'),\n",
       " (8185, 'N7PJYTMR', 'Barlow1972'),\n",
       " (8257, '57RVFNFA', 'Monteiro2021'),\n",
       " (8244, 'CQIK4QVI', 'Shakerinava2024'),\n",
       " (8260, 'D8FEABCQ', 'Liu2022b'),\n",
       " (8261, '2RCZYDAS', 'Gupta2019'),\n",
       " (6585, 'CDL7X8G9', 'Frongillo'),\n",
       " (8269, 'QSJL3LKV', 'Wright2015'),\n",
       " (8272, 'E8IHVEV2', 'Ayer1955'),\n",
       " (8274, 'ME4C8ZU9', 'Best1990')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "citekeys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cfb2a50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = u\"\"\"\n",
    "    SELECT items.itemID, items.key, fields.fieldID, fields.fieldName, itemDataValues.value, citationkey.citationKey\n",
    "    FROM items, itemData, fields, itemDataValues, CitationKey\n",
    "    where \n",
    "        items.itemID = itemData.itemID\n",
    "        and items.itemID = CitationKey.itemID\n",
    "        and itemData.fieldID = fields.fieldID\n",
    "        and itemData.valueID = itemDataValues.valueID\n",
    "        and fields.fieldID in (1,2)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1ba96b39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2,\n",
       "  'KHW2498F',\n",
       "  1,\n",
       "  'title',\n",
       "  'The role of extrusions and intrusions in fatigue crack initiation',\n",
       "  'Polak2017'),\n",
       " (4,\n",
       "  'QZ4IPAQV',\n",
       "  1,\n",
       "  'title',\n",
       "  'Experimental evidence and physical models of fatigue crack initiation',\n",
       "  'Polak2016'),\n",
       " (6,\n",
       "  '6DGRHLYU',\n",
       "  1,\n",
       "  'title',\n",
       "  'The physics of fatigue crack initiation',\n",
       "  'Sangid2013'),\n",
       " (8,\n",
       "  'YGGUDQCL',\n",
       "  1,\n",
       "  'title',\n",
       "  'On the role of point defects in fatigue crack initiation',\n",
       "  'Polak1987'),\n",
       " (10,\n",
       "  '4MFHUNBV',\n",
       "  1,\n",
       "  'title',\n",
       "  'Fatigue crack initiation - The role of point defects',\n",
       "  'Polak2014'),\n",
       " (12,\n",
       "  'RK9FV68J',\n",
       "  1,\n",
       "  'title',\n",
       "  'A survey of methods for time series change point detection',\n",
       "  'Aminikhanghahi2017'),\n",
       " (16,\n",
       "  'XD2QDF6R',\n",
       "  1,\n",
       "  'title',\n",
       "  'Selective review of offline change point detection methods',\n",
       "  'Truong2018'),\n",
       " (18,\n",
       "  'WQ2CIXZK',\n",
       "  1,\n",
       "  'title',\n",
       "  'Recent advances on the numerical modelling of turbulent flows',\n",
       "  'Argyropoulos2015'),\n",
       " (20,\n",
       "  '84HAVXWN',\n",
       "  1,\n",
       "  'title',\n",
       "  'How to read a research paper.',\n",
       "  'Mitzenmacher1988'),\n",
       " (22,\n",
       "  'NYN9JAFT',\n",
       "  1,\n",
       "  'title',\n",
       "  'A non-singular continuum theory of dislocations',\n",
       "  'Cai2006'),\n",
       " (24,\n",
       "  'XGN74PFB',\n",
       "  1,\n",
       "  'title',\n",
       "  'New developments in Contact Problems',\n",
       "  'PeterWriggers2012'),\n",
       " (26,\n",
       "  'JRFNKVVT',\n",
       "  1,\n",
       "  'title',\n",
       "  'Reinforcement Learning: An Introduction',\n",
       "  'Andrew1998'),\n",
       " (28,\n",
       "  'AH3PKCWJ',\n",
       "  1,\n",
       "  'title',\n",
       "  'Research directions in computational mechanics, A Report of the United States National Committee on Theoretical and Applied Mechanics',\n",
       "  'Press2000'),\n",
       " (30,\n",
       "  'LYZ2SXKL',\n",
       "  1,\n",
       "  'title',\n",
       "  'Bayesian inverse problems for functions and applications to fluid mechanics',\n",
       "  'Cotter2009'),\n",
       " (31,\n",
       "  'T57E9LWP',\n",
       "  1,\n",
       "  'title',\n",
       "  'Adaptivity in Bayesian Inverse Finite Element Problems: Learning and Simultaneous Control of Discretisation and Sampling Errors',\n",
       "  'Kerfriden2019'),\n",
       " (32,\n",
       "  '37AE3RYY',\n",
       "  1,\n",
       "  'title',\n",
       "  'Approximation of bayesian inverse problems for PDES',\n",
       "  'Cotter2010'),\n",
       " (33,\n",
       "  'Z3P3I79V',\n",
       "  1,\n",
       "  'title',\n",
       "  'Inverse problems: A Bayesian perspective',\n",
       "  'Stuart2010'),\n",
       " (35,\n",
       "  'P6YBSNEN',\n",
       "  1,\n",
       "  'title',\n",
       "  'Atomic Pair Distribution Function Analysis',\n",
       "  'Page2007'),\n",
       " (37, 'MHWV4VWA', 1, 'title', 'Can one hear the shape of a Drum?', 'Kac1966'),\n",
       " (41,\n",
       "  '8T89CPZT',\n",
       "  1,\n",
       "  'title',\n",
       "  'Rock destruction effect on the stability of a drilling structure',\n",
       "  'Challamel2000'),\n",
       " (43,\n",
       "  'X5NTIP8Q',\n",
       "  1,\n",
       "  'title',\n",
       "  'The effects of nuclear radiation on materials',\n",
       "  'Flanagan1959'),\n",
       " (45,\n",
       "  'QT7I59WW',\n",
       "  1,\n",
       "  'title',\n",
       "  'Definitions and examples of inverse and ill-posed problems',\n",
       "  'Kabanikhin2008'),\n",
       " (47,\n",
       "  '8JHH63I2',\n",
       "  1,\n",
       "  'title',\n",
       "  'The rise of the X-ray atomic pair distribution function method: A series of fortunate events',\n",
       "  'Billinge2019'),\n",
       " (49, 'CDY9R2U2', 1, 'title', 'Functional Analysis', 'Palmieri2020'),\n",
       " (56, 'JP828P7I', 1, 'title', 'Dynamic Auctions', 'Vincent1990'),\n",
       " (58, 'XVP6ESUA', 1, 'title', 'The speed of evolution', 'Vishnoi2015'),\n",
       " (61,\n",
       "  'ZV3MJCRK',\n",
       "  1,\n",
       "  'title',\n",
       "  'A finite population model of molecular evolution: Theory and computation',\n",
       "  'Dixit2012'),\n",
       " (91, 'KMHD79T9', 1, 'title', 'Bidding Dynamics in Auctions', 'Hopenhayn2016'),\n",
       " (98,\n",
       "  'KVHB88B3',\n",
       "  1,\n",
       "  'title',\n",
       "  'Games and Economic Behavior Budget-constrained sequential auctions with incomplete information',\n",
       "  'Pitchik2009'),\n",
       " (102, 'TM77ICP7', 1, 'title', 'Draft Auctions', 'Nov2018'),\n",
       " (105,\n",
       "  '2UD7Y8NR',\n",
       "  1,\n",
       "  'title',\n",
       "  'Statistical Inference in Inverse Problems',\n",
       "  'Xun2012'),\n",
       " (107,\n",
       "  '4J8VQFF3',\n",
       "  1,\n",
       "  'title',\n",
       "  'Mixing time of Markov chains, dynamical systems and evolution',\n",
       "  'Panageas2016a'),\n",
       " (133,\n",
       "  'A36NLYGH',\n",
       "  1,\n",
       "  'title',\n",
       "  'Inverse Problems and Applications',\n",
       "  'Beilina1390'),\n",
       " (135,\n",
       "  '6XJXGAJK',\n",
       "  1,\n",
       "  'title',\n",
       "  'The Calculus Of Variations And Functional Analysis',\n",
       "  'Lebedev2003'),\n",
       " (139,\n",
       "  'TLRSTBP4',\n",
       "  1,\n",
       "  'title',\n",
       "  'Making evolution rigorous: The error threshold',\n",
       "  'Vishnoi2013'),\n",
       " (141,\n",
       "  'V7IPA6PF',\n",
       "  1,\n",
       "  'title',\n",
       "  'Auctions with interdependent valuations: theoretical and empirical analysis, in particular of internet auctions',\n",
       "  'Schindler2003'),\n",
       " (143,\n",
       "  'RJQX4CAD',\n",
       "  1,\n",
       "  'title',\n",
       "  \"Molecular Quasl-Speciest The formulation of a tractable chemical model based on Darwin's principle may be understood in several steps: 1. The major constituents of the system have to be inherently self-reproductive. Only two classes of molecules are prese\",\n",
       "  'Eigen1988'),\n",
       " (145,\n",
       "  'W34DX4WH',\n",
       "  1,\n",
       "  'title',\n",
       "  'Evolutionary dynamics in finite populations mix rapidly',\n",
       "  'Panageas2016'),\n",
       " (147,\n",
       "  'RRJJLKRQ',\n",
       "  1,\n",
       "  'title',\n",
       "  'The markov chain monte carlo revolution',\n",
       "  'Diaconis2009'),\n",
       " (149,\n",
       "  '8GHW6KVJ',\n",
       "  1,\n",
       "  'title',\n",
       "  'Handbook of uncertainty quantification',\n",
       "  'Ghanem2017'),\n",
       " (255, 'Q7YDEKGJ', 1, 'title', 'Functional analysis', 'Yosida1967'),\n",
       " (270,\n",
       "  'HJ2GJUKM',\n",
       "  1,\n",
       "  'title',\n",
       "  'Uncertainty Quantification and Bayesian Inversion',\n",
       "  'Stuart2017'),\n",
       " (273,\n",
       "  '7I2IXUME',\n",
       "  1,\n",
       "  'title',\n",
       "  'Introduction to banach spaces: Analysis and probability',\n",
       "  'Li2017'),\n",
       " (275,\n",
       "  '7CEDI6HD',\n",
       "  1,\n",
       "  'title',\n",
       "  'The Lax-Milgram Theorem. A detailed proof to be formalized in Coq',\n",
       "  'Clement2016'),\n",
       " (277,\n",
       "  'C85XBEGL',\n",
       "  1,\n",
       "  'title',\n",
       "  'Optimal feedback control, linear first-order PDE systems, and obstacle problems',\n",
       "  'Pedregal2017'),\n",
       " (279, 'FQAEDEPB', 1, 'title', 'A TEX Mathematical Symbols', 'Kumar2020'),\n",
       " (281, 'ZWI5BE2X', 1, 'title', 'Real Analysis', 'Royden2010'),\n",
       " (292,\n",
       "  'WRHMBIR4',\n",
       "  1,\n",
       "  'title',\n",
       "  'Modern regularization methods for inverse problems',\n",
       "  'Benning2018'),\n",
       " (295,\n",
       "  '89QEPBBX',\n",
       "  1,\n",
       "  'title',\n",
       "  'Linear convergence of iterative soft-thresholding',\n",
       "  'Bredies2008'),\n",
       " (297,\n",
       "  'WY8SJYFL',\n",
       "  1,\n",
       "  'title',\n",
       "  'Multiplier and gradient methods',\n",
       "  'Hestenes1969'),\n",
       " (301,\n",
       "  'GIUQPQU6',\n",
       "  1,\n",
       "  'title',\n",
       "  'A dual approach to solving nonlinear programming problems by unconstrained optimization',\n",
       "  'Rockafellar1973'),\n",
       " (302,\n",
       "  'CYTFSGT7',\n",
       "  1,\n",
       "  'title',\n",
       "  'Augmented Lagrangian Methods: Applications to the Numerical Solution of Boundary-Value Problems',\n",
       "  'Fortin2000'),\n",
       " (304,\n",
       "  'JQV64ZJ9',\n",
       "  1,\n",
       "  'title',\n",
       "  'Iterative Shrinkage / Thresholding Algorithms : Some History and Recent Development',\n",
       "  'Figueiredo2009'),\n",
       " (306,\n",
       "  '5SW9HXKK',\n",
       "  1,\n",
       "  'title',\n",
       "  'The Crandall-Liggett generation theorem',\n",
       "  'Rulla2015'),\n",
       " (308,\n",
       "  'EEVMXG2E',\n",
       "  1,\n",
       "  'title',\n",
       "  'Learning theory of randomized Kaczmarz algorithm',\n",
       "  'Lin2015'),\n",
       " (310,\n",
       "  'ZSMZTCTJ',\n",
       "  1,\n",
       "  'title',\n",
       "  'Open problem: Efficient online sparse regression',\n",
       "  'Kale2014'),\n",
       " (312,\n",
       "  '88HI884G',\n",
       "  1,\n",
       "  'title',\n",
       "  'On the Exponential Convergence of the Kaczmarz Algorithm',\n",
       "  'Dai2015'),\n",
       " (322,\n",
       "  'WVB3X3QG',\n",
       "  1,\n",
       "  'title',\n",
       "  'Successive projections on hyperplanes',\n",
       "  'Aharoni1984'),\n",
       " (325,\n",
       "  'G9459S3D',\n",
       "  1,\n",
       "  'title',\n",
       "  'Optimal k-thresholding algorithms for sparse optimization problems',\n",
       "  'Zhao2020'),\n",
       " (327,\n",
       "  '5R635KPF',\n",
       "  1,\n",
       "  'title',\n",
       "  'Randomized projection methods for linear systems with arbitrarily large sparse corruptions',\n",
       "  'Haddock2019a'),\n",
       " (329,\n",
       "  'YBSBB3MW',\n",
       "  1,\n",
       "  'title',\n",
       "  'Stochastic reformulations of linear systems: Algorithms and convergence theory',\n",
       "  'Richtarik2017'),\n",
       " (331,\n",
       "  '9CXPLTPC',\n",
       "  1,\n",
       "  'title',\n",
       "  'Rows versus Columns: Randomized Kaczmarz or Gauss--Seidel for Ridge Regression',\n",
       "  'Hefny2017'),\n",
       " (333,\n",
       "  'Y3XFX8EP',\n",
       "  1,\n",
       "  'title',\n",
       "  'Book Review: A mathematical introduction to compressive sensing',\n",
       "  'Tropp2016'),\n",
       " (335,\n",
       "  'DIQ7LN5L',\n",
       "  1,\n",
       "  'title',\n",
       "  'A Sampling Kaczmarz--Motzkin Algorithm for Linear Feasibility',\n",
       "  'DeLoera2017'),\n",
       " (337,\n",
       "  '79HTE7YU',\n",
       "  1,\n",
       "  'title',\n",
       "  'A smoothing method for sparse optimization over convex sets',\n",
       "  'Haddou2020'),\n",
       " (339,\n",
       "  'TX9IQDAE',\n",
       "  1,\n",
       "  'title',\n",
       "  'On Motzkin’s method for inconsistent linear systems',\n",
       "  'Haddock2019'),\n",
       " (341,\n",
       "  'DH56LMMZ',\n",
       "  1,\n",
       "  'title',\n",
       "  'Enhancing sparsity by reweightedℓ1 minimization',\n",
       "  'Candes2008a'),\n",
       " (343,\n",
       "  '8M6PSFR3',\n",
       "  1,\n",
       "  'title',\n",
       "  'Novel min-max reformulations of linear inverse problems',\n",
       "  'Sheriff2020'),\n",
       " (345,\n",
       "  'T6ALGK5Z',\n",
       "  1,\n",
       "  'title',\n",
       "  'The Kaczmarz algorithm, row action methods,                    and statistical learning algorithms',\n",
       "  'Chen2018'),\n",
       " (347,\n",
       "  'LVTDCDNJ',\n",
       "  1,\n",
       "  'title',\n",
       "  'A randomized kaczmarz algorithm with exponential convergence',\n",
       "  'Strohmer2009'),\n",
       " (353,\n",
       "  'IME9ZZQJ',\n",
       "  1,\n",
       "  'title',\n",
       "  'Faster Randomized Block Kaczmarz Algorithms ∗',\n",
       "  'Necoara2019'),\n",
       " (359,\n",
       "  'VPRN5GHF',\n",
       "  1,\n",
       "  'title',\n",
       "  'The randomized Kaczmarz method with mismatched adjoint',\n",
       "  'Lorenz2018'),\n",
       " (361,\n",
       "  'BEYVQHE4',\n",
       "  1,\n",
       "  'title',\n",
       "  'Atomic decomposition by basis pursuit',\n",
       "  'Chen2001'),\n",
       " (363,\n",
       "  'VUTD2L26',\n",
       "  1,\n",
       "  'title',\n",
       "  'The linearized bregman method via split feasibility problems: Analysis and generalizations',\n",
       "  'Lorenz2014'),\n",
       " (365,\n",
       "  'RF983IJ3',\n",
       "  1,\n",
       "  'title',\n",
       "  'Sparse Recovery beyond Compressed Sensing: Separable Nonlinear Inverse Problems',\n",
       "  'Bernstein2020'),\n",
       " (367,\n",
       "  '5EDIRAG2',\n",
       "  1,\n",
       "  'title',\n",
       "  'An Overview on Algorithms for Sparse Recovery',\n",
       "  'Fornasier2015'),\n",
       " (369,\n",
       "  'PSU4NWDX',\n",
       "  1,\n",
       "  'title',\n",
       "  'Maximum Hands-Off Control: A Paradigm of Control Effort Minimization',\n",
       "  'Nagahara2016'),\n",
       " (371,\n",
       "  'Z79VBMX2',\n",
       "  1,\n",
       "  'title',\n",
       "  'Sparse approximate solutions to linear systems',\n",
       "  'Natarajan1995'),\n",
       " (374,\n",
       "  'C74YT8SQ',\n",
       "  1,\n",
       "  'title',\n",
       "  'Almost Sure Convergence of the Kaczmarz Algorithm with Random Measurements',\n",
       "  'Chen2012'),\n",
       " (376,\n",
       "  '4T9KPRM5',\n",
       "  1,\n",
       "  'title',\n",
       "  'Row-action methods for compressed sensing',\n",
       "  'Sra2006'),\n",
       " (378,\n",
       "  'SYH8KZNE',\n",
       "  1,\n",
       "  'title',\n",
       "  'Solving basis pursuit: Heuristic optimality check and solver comparison',\n",
       "  'Lorenz2015'),\n",
       " (380,\n",
       "  'RLMUI567',\n",
       "  1,\n",
       "  'title',\n",
       "  'Approximate Solution for Systems of Linear Equations',\n",
       "  'Kaczmarz1937'),\n",
       " (386,\n",
       "  'VGZGUCNJ',\n",
       "  1,\n",
       "  'title',\n",
       "  'Accelerating the distributed Kaczmarz algorithm by strong over-relaxation',\n",
       "  'Borgard2020'),\n",
       " (388,\n",
       "  'D4MI8XVT',\n",
       "  1,\n",
       "  'title',\n",
       "  'Projection method for solving a singular system of linear equations and its applications',\n",
       "  'Tanabe1971'),\n",
       " (405, 'WDLVFYRL', 1, 'title', 'Natterer', 'Natterer2001a'),\n",
       " (407,\n",
       "  'MMP9VPRQ',\n",
       "  1,\n",
       "  'title',\n",
       "  'Enhancement of the Kaczmarz algorithm with projection adjustment',\n",
       "  'Lin2020'),\n",
       " (409,\n",
       "  'EYTHARPP',\n",
       "  1,\n",
       "  'title',\n",
       "  'A convex integer programming approach for optimal sparse PCA',\n",
       "  'Dey2018'),\n",
       " (411,\n",
       "  'XE8H3Y3H',\n",
       "  1,\n",
       "  'title',\n",
       "  'Sparse regression at scale: Branch-and-bound rooted in first-order optimization',\n",
       "  'Hazimeh2020'),\n",
       " (413,\n",
       "  'TWW88JF8',\n",
       "  1,\n",
       "  'title',\n",
       "  'The method of projections for finding the common point of convex sets',\n",
       "  'Gubin1967'),\n",
       " (415,\n",
       "  'AL9X9BHE',\n",
       "  1,\n",
       "  'title',\n",
       "  'Block-iterative methods for consistent and inconsistent linear equations',\n",
       "  'Elfving1980'),\n",
       " (417,\n",
       "  'MT6UGWKU',\n",
       "  1,\n",
       "  'title',\n",
       "  'Row-Action Methods for Huge and Sparse Systems and Their Applications',\n",
       "  'Censor1981'),\n",
       " (419,\n",
       "  'A6NAGGAQ',\n",
       "  1,\n",
       "  'title',\n",
       "  'Projection methods for solving sparse linear systems',\n",
       "  'Tewarson1969'),\n",
       " (423,\n",
       "  'PF5E9JAI',\n",
       "  1,\n",
       "  'title',\n",
       "  \"Strong Underrelaxation in Kaczmarz ' s Method for Inconsistent Systems *\",\n",
       "  'Censor1983'),\n",
       " (425,\n",
       "  'QMPN8RRB',\n",
       "  1,\n",
       "  'title',\n",
       "  'The Mathematics of Computed Tomography',\n",
       "  'Natterer2001'),\n",
       " (427,\n",
       "  '34J9TKNK',\n",
       "  1,\n",
       "  'title',\n",
       "  'A sparse Kaczmarz solver and a linearized Bregman method for online compressed sensing arXiv : 1403 . 7543v1 [ math . OC ] 28 Mar 2014',\n",
       "  'Lorenz2014a'),\n",
       " (429,\n",
       "  'MF96NTJF',\n",
       "  1,\n",
       "  'title',\n",
       "  'TRex: A Tomography Reconstruction Proximal Framework for Robust Sparse View X-Ray Applications',\n",
       "  'Aly2016'),\n",
       " (431,\n",
       "  'TX46X33P',\n",
       "  1,\n",
       "  'title',\n",
       "  'On greedy randomized kaczmarz method for solving large sparse linear systems',\n",
       "  'Bai2018'),\n",
       " (432,\n",
       "  'VSS69KW9',\n",
       "  1,\n",
       "  'title',\n",
       "  'On relaxed greedy randomized Kaczmarz methods for solving large sparse linear systems',\n",
       "  'Bai2018a'),\n",
       " (433,\n",
       "  'HHLDD6VN',\n",
       "  1,\n",
       "  'title',\n",
       "  'On partially randomized extended Kaczmarz method for solving large sparse overdetermined inconsistent linear systems',\n",
       "  'Bai2019'),\n",
       " (435,\n",
       "  'VZRC3QKE',\n",
       "  1,\n",
       "  'title',\n",
       "  'Component averaging: An efficient iterative parallel algorithm for large and sparse unstructured problems',\n",
       "  'Censor2001a'),\n",
       " (436,\n",
       "  '95D9N8YW',\n",
       "  1,\n",
       "  'title',\n",
       "  'BICAV: A block-iterative parallel algorithm for sparse systems with pixel-related weighting',\n",
       "  'Censor2001'),\n",
       " (437,\n",
       "  '7CZVWJYY',\n",
       "  1,\n",
       "  'title',\n",
       "  'Beyond Moore-Penrose: Sparse pseudoinverse',\n",
       "  'Dokmanic2013'),\n",
       " (438,\n",
       "  '7NVZXNFN',\n",
       "  1,\n",
       "  'title',\n",
       "  'Sparse Randomized Kaczmarz for Support Recovery of Jointly Sparse Corrupted Multiple Measurement Vectors',\n",
       "  'Durgin2019'),\n",
       " (439,\n",
       "  'QB74Q2ZG',\n",
       "  1,\n",
       "  'title',\n",
       "  'A Wavelet Based Sparse Row-Action Method for Image Reconstruction in Magnetic Particle Imaging',\n",
       "  'Lieb2020'),\n",
       " (441,\n",
       "  'LDBE7W2Q',\n",
       "  1,\n",
       "  'title',\n",
       "  'Microkicking for fast convergence of sparse kaczmarz and sparse LMS',\n",
       "  'Lunglmayr2018'),\n",
       " (446,\n",
       "  'Z9S8PHGD',\n",
       "  1,\n",
       "  'title',\n",
       "  'Randomized iterative hard thresholding for sparse approximations',\n",
       "  'Crandall2014'),\n",
       " (448,\n",
       "  'SUWRC5Y9',\n",
       "  1,\n",
       "  'title',\n",
       "  'Randomized Kaczmarz with averaging',\n",
       "  'Moorman2021'),\n",
       " (450,\n",
       "  'HQUMWS5X',\n",
       "  1,\n",
       "  'title',\n",
       "  'The Averaged Kaczmarz Iteration for Solving Inverse Problems',\n",
       "  'Li2018'),\n",
       " (452,\n",
       "  'GI6925RD',\n",
       "  1,\n",
       "  'title',\n",
       "  'Fast proximal gradient methods',\n",
       "  'Vandenberghe2013'),\n",
       " (454,\n",
       "  'YEPXJDM3',\n",
       "  1,\n",
       "  'title',\n",
       "  'Inverse problems and regularization - An Introduction',\n",
       "  'StefanKidermann'),\n",
       " (456,\n",
       "  'MLJV6IUI',\n",
       "  1,\n",
       "  'title',\n",
       "  'Inexact accelerated high-order proximal-point methods',\n",
       "  'Nesterov2021'),\n",
       " (458,\n",
       "  'VSMIFTFG',\n",
       "  1,\n",
       "  'title',\n",
       "  'Gradient methods for minimizing composite functions',\n",
       "  'Nesterov2013'),\n",
       " (460,\n",
       "  'J3D4KXAP',\n",
       "  1,\n",
       "  'title',\n",
       "  'Gradient methods for minimizing composite objective function',\n",
       "  'Nesterov2007'),\n",
       " (462, 'QM8QLV4W', 1, 'title', 'Inverse problems', 'HanneKekkonen2019'),\n",
       " (465,\n",
       "  'WETI5VIH',\n",
       "  1,\n",
       "  'title',\n",
       "  'A Note On Convergence Rate of Randomized Kaczmarz Method',\n",
       "  'Guan2020'),\n",
       " (467,\n",
       "  'VNIYDAD9',\n",
       "  1,\n",
       "  'title',\n",
       "  'Remarks on Kaczmarz algorithm for solving',\n",
       "  'Huang2020'),\n",
       " (469,\n",
       "  'FUT5TMMB',\n",
       "  1,\n",
       "  'title',\n",
       "  'Convergence rates for greedy Kaczmarz algorithms, and faster randomized Kaczmarz rules using the orthogonality graph',\n",
       "  'Nutini2016'),\n",
       " (471,\n",
       "  '6E5SB8X8',\n",
       "  1,\n",
       "  'title',\n",
       "  'Convergence analysis for Kaczmarz-type methods in a Hilbert space framework',\n",
       "  'Oswald2015'),\n",
       " (473,\n",
       "  'ENVTBAC7',\n",
       "  1,\n",
       "  'title',\n",
       "  'Convergence rates for Kaczmarz-type algorithms',\n",
       "  'Popa2018'),\n",
       " (477,\n",
       "  'RVDFLBZR',\n",
       "  1,\n",
       "  'title',\n",
       "  'A mathematical introduction to compressive sensing',\n",
       "  'Foucart2013'),\n",
       " (479,\n",
       "  'Z5LR3UYK',\n",
       "  1,\n",
       "  'title',\n",
       "  'The Method of Orthogonal Projections',\n",
       "  'Rektorys1977'),\n",
       " (483,\n",
       "  'VMURAD7R',\n",
       "  1,\n",
       "  'title',\n",
       "  'Lectures on Convex Optimization',\n",
       "  'Nesterov2018'),\n",
       " (485,\n",
       "  '5FHLXDFU',\n",
       "  1,\n",
       "  'title',\n",
       "  'Numerical Analysis Second Edition',\n",
       "  'WalterGautschi2012'),\n",
       " (487,\n",
       "  '9FRTCCVW',\n",
       "  1,\n",
       "  'title',\n",
       "  'The restricted isometry property and its implications for compressed sensing',\n",
       "  'Candes2008'),\n",
       " (491,\n",
       "  'E3I2UYZD',\n",
       "  1,\n",
       "  'title',\n",
       "  'Signal recovery by proximal forward-backward splitting',\n",
       "  'Combettes2005'),\n",
       " (493,\n",
       "  'BNY6DVKZ',\n",
       "  1,\n",
       "  'title',\n",
       "  'On sparse optimal control for general linear systems',\n",
       "  'Ikeda2019'),\n",
       " (495,\n",
       "  'HG8KKJXW',\n",
       "  1,\n",
       "  'title',\n",
       "  'Q-learning for Markov decision processes with a satisfiability criterion',\n",
       "  'Shah2018a'),\n",
       " (499,\n",
       "  '7FKHG94Q',\n",
       "  1,\n",
       "  'title',\n",
       "  'Multicriteria decision methods: An attempt to evaluate and unify',\n",
       "  'Cho2003'),\n",
       " (501,\n",
       "  'IFISIJDE',\n",
       "  1,\n",
       "  'title',\n",
       "  'A bibliometric-based survey on AHP and TOPSIS techniques',\n",
       "  'Zyoud2017'),\n",
       " (503,\n",
       "  'KCGTZC8X',\n",
       "  1,\n",
       "  'title',\n",
       "  'INFORMS TutORials in Operations Research Multiple Criteria Decision Making : Foundations and Some Approaches',\n",
       "  'Sciences2014'),\n",
       " (505,\n",
       "  'TFA9C9J3',\n",
       "  1,\n",
       "  'title',\n",
       "  'Identifying influential nodes in complex networks based on AHP',\n",
       "  'Bian2017'),\n",
       " (507,\n",
       "  '7GYZYRFR',\n",
       "  1,\n",
       "  'title',\n",
       "  'Dual cone approach to convex-cone dominance in multiple criteria decision making',\n",
       "  'Korhonen2016'),\n",
       " (509,\n",
       "  'E7PULVTQ',\n",
       "  1,\n",
       "  'title',\n",
       "  'The modern science of multicriteria decision making and its practical applications: The AHP/ANP approach',\n",
       "  'Saaty2013'),\n",
       " (511,\n",
       "  'SRTWUGVT',\n",
       "  1,\n",
       "  'title',\n",
       "  'A weighted randomized Kaczmarz method for solving linear systems',\n",
       "  'Steinerberger2021'),\n",
       " (513,\n",
       "  'V67NRB5K',\n",
       "  1,\n",
       "  'title',\n",
       "  'Decision-making with the AHP: Why is the principal eigenvector necessary',\n",
       "  'Saaty2003'),\n",
       " (515,\n",
       "  '8SLY8AR8',\n",
       "  1,\n",
       "  'title',\n",
       "  'Accelerated stochastic power iteration',\n",
       "  'Xu2018'),\n",
       " (517,\n",
       "  'SJSRIGG4',\n",
       "  1,\n",
       "  'title',\n",
       "  'A critical analysis of the eigenvalue method used to derive priorities in AHP',\n",
       "  'BanaECosta2008'),\n",
       " (519,\n",
       "  'I323E2P2',\n",
       "  1,\n",
       "  'title',\n",
       "  'The state of the art development of AHP (1979–2017): A literature review with a social network analysis',\n",
       "  'Emrouznejad2017'),\n",
       " (521,\n",
       "  '82ZMDNIX',\n",
       "  1,\n",
       "  'title',\n",
       "  'An approach to avoiding rank reversal in AHP',\n",
       "  'Wang2006'),\n",
       " (523,\n",
       "  '6KLGI39Z',\n",
       "  1,\n",
       "  'title',\n",
       "  'Another potential shortcoming of AHP',\n",
       "  'Perez2006'),\n",
       " (525,\n",
       "  'HM8M7WRJ',\n",
       "  1,\n",
       "  'title',\n",
       "  'A novel weight neighborhood centrality algorithm for identifying influential spreaders in complex networks',\n",
       "  'Wang2017'),\n",
       " (527,\n",
       "  'P8SC88WX',\n",
       "  1,\n",
       "  'title',\n",
       "  'Node Importance Ranking in Complex Networks Based on Multicriteria Decision Making',\n",
       "  'Yang2019'),\n",
       " (529,\n",
       "  'K3G2NVWK',\n",
       "  1,\n",
       "  'title',\n",
       "  'Systemic decision making in AHP: a Bayesian approach',\n",
       "  'Moreno-jimenez2016'),\n",
       " (531,\n",
       "  'VFP28VF9',\n",
       "  1,\n",
       "  'title',\n",
       "  'New variants of the POCS method using affine subspaces of finite codimension with applications to irregular sampling',\n",
       "  'Feichtinger1992'),\n",
       " (534,\n",
       "  'W4QW6JRQ',\n",
       "  1,\n",
       "  'title',\n",
       "  'Iterative procedures related to relaxation methods for eigenvalue problems',\n",
       "  'Crandall1951'),\n",
       " (538,\n",
       "  'HLCYPHXQ',\n",
       "  1,\n",
       "  'title',\n",
       "  'Randomized kaczmarz converges along small singular vectors',\n",
       "  'Steinerberger2021a'),\n",
       " (544,\n",
       "  'BXLPZVV6',\n",
       "  1,\n",
       "  'title',\n",
       "  'Replicator Dynamics in Combinatorial Optimization',\n",
       "  'Pelillo2012'),\n",
       " (546,\n",
       "  'HZTU3C3U',\n",
       "  1,\n",
       "  'title',\n",
       "  'Preasymptotic convergence of randomized Kaczmarz method',\n",
       "  'Jiao2017'),\n",
       " (548,\n",
       "  'TPFVSV5G',\n",
       "  1,\n",
       "  'title',\n",
       "  'On the regularizing property of stochastic gradient descent',\n",
       "  'Jin2019'),\n",
       " (550,\n",
       "  '7FCS6PNW',\n",
       "  1,\n",
       "  'title',\n",
       "  'Simple, robust and optimal ranking from pairwise comparisons',\n",
       "  'Shah2018'),\n",
       " (556,\n",
       "  'ZMN6RFF5',\n",
       "  1,\n",
       "  'title',\n",
       "  'Approximate ranking from pairwise comparisons',\n",
       "  'Heckel2018'),\n",
       " (558,\n",
       "  'UEQL35AD',\n",
       "  1,\n",
       "  'title',\n",
       "  'Springer Optimization and Its Applications 123 Non-Convex Multi-Objective Optimization',\n",
       "  'Pardalos2017'),\n",
       " (560,\n",
       "  '8UQYDKBD',\n",
       "  1,\n",
       "  'title',\n",
       "  'Multi-Objective Optimization, Evolutionary to Hybrid Framework',\n",
       "  'Mandal2018'),\n",
       " (562,\n",
       "  'JX3BNSI3',\n",
       "  1,\n",
       "  'title',\n",
       "  'Mathematics of Multi Objective Optimization',\n",
       "  'Centre1985'),\n",
       " (564,\n",
       "  'FQV6EMP5',\n",
       "  1,\n",
       "  'title',\n",
       "  'A Simple Algorithm for the Linear Bilevel Programming Problem',\n",
       "  'Dempe1987'),\n",
       " (566,\n",
       "  'U7EYCXPJ',\n",
       "  1,\n",
       "  'title',\n",
       "  'The Concave-Convex Procedure ( CCCP )',\n",
       "  'Yuille2001'),\n",
       " (568, '57F2NNHD', 1, 'title', 'Legendre Transforms', 'Debnath2020'),\n",
       " (570,\n",
       "  'M5ZNE2YE',\n",
       "  1,\n",
       "  'title',\n",
       "  \"A proof of convergence of the concave-convex procedure using Zangwill's theory\",\n",
       "  'Sriperumbudur2012'),\n",
       " (572,\n",
       "  '25ZYRQ7W',\n",
       "  1,\n",
       "  'title',\n",
       "  'Disciplined Convex-Concave Programming',\n",
       "  'Shen2016'),\n",
       " (574,\n",
       "  'UZMJ8VMK',\n",
       "  1,\n",
       "  'title',\n",
       "  'DC decomposition of nonconvex polynomials with algebraic techniques',\n",
       "  'Ahmadi2018'),\n",
       " (576,\n",
       "  'W5LFHVT6',\n",
       "  1,\n",
       "  'title',\n",
       "  'On the convergence of the concave-convex procedure',\n",
       "  'Sriperumbudur2009'),\n",
       " (578,\n",
       "  'LUQX94JI',\n",
       "  1,\n",
       "  'title',\n",
       "  'A discrete dynamical system and its applications',\n",
       "  'Carrasco-gutierrez2019'),\n",
       " (580,\n",
       "  'M7S7S55S',\n",
       "  1,\n",
       "  'title',\n",
       "  'Variations and extension of the convex–concave procedure',\n",
       "  'Lipp2016'),\n",
       " (582,\n",
       "  'CRLGH2XJ',\n",
       "  1,\n",
       "  'title',\n",
       "  'Algebraic transformations of objective functions',\n",
       "  'Mjolsness1990'),\n",
       " (584,\n",
       "  'R6HPJ6C5',\n",
       "  1,\n",
       "  'title',\n",
       "  'A Novel Optimizing Network Architecture with Applications',\n",
       "  'Rangarajan1996'),\n",
       " (586, '6KGNZXBY', 1, 'title', 'Bilevel Integer Programming', 'Ralphs2009'),\n",
       " (588,\n",
       "  'F7I8V8EZ',\n",
       "  1,\n",
       "  'title',\n",
       "  'Linear bi-level programming problems—a review',\n",
       "  'WEn1991'),\n",
       " (590,\n",
       "  'RPU6LD4M',\n",
       "  1,\n",
       "  'title',\n",
       "  'Operational Research Society is collaborating with JSTOR to digitize, preserve, and extend access to Journal of the Operational Research Society. ® www.jstor.org',\n",
       "  'Yeh1996'),\n",
       " (592, 'EAHRA8M8', 1, 'title', 'Bilevel programming: A survey', 'Colson2005'),\n",
       " (594,\n",
       "  'DM8QYP53',\n",
       "  1,\n",
       "  'title',\n",
       "  'Computational Difficulties of Bilevel Linear Programming',\n",
       "  'Ben-ayed1990'),\n",
       " (596, 'SD7778N4', 1, 'title', 'Bilevel Optimization', 'Dempe2020'),\n",
       " (603,\n",
       "  '253SZQ85',\n",
       "  1,\n",
       "  'title',\n",
       "  'Multicriteria approach to bilevel optimization',\n",
       "  'Fliege2006'),\n",
       " (606,\n",
       "  'ZF6XYRPG',\n",
       "  1,\n",
       "  'title',\n",
       "  'An efficient point algorithm for a linear two-stage optimization problem',\n",
       "  'Bard1983'),\n",
       " (608,\n",
       "  'XSW4NMKH',\n",
       "  1,\n",
       "  'title',\n",
       "  'The computational complexity of multi-level linear programs',\n",
       "  'Blair1992'),\n",
       " (610,\n",
       "  'ET3MS7NC',\n",
       "  1,\n",
       "  'title',\n",
       "  'Bilevel programming : Theoretical and algorithmic approaches',\n",
       "  'Gale2016'),\n",
       " (612,\n",
       "  '5U45VJFY',\n",
       "  1,\n",
       "  'title',\n",
       "  'A linear bilevel programming algorithm: A comment',\n",
       "  'Candler1988'),\n",
       " (615,\n",
       "  'XICLQE69',\n",
       "  1,\n",
       "  'title',\n",
       "  'A note on a linear bilevel programming algorithm based on bicriteria programming',\n",
       "  'Wen1989'),\n",
       " (618,\n",
       "  'TM8EZDRS',\n",
       "  1,\n",
       "  'title',\n",
       "  'A Linear bilevel programming algorithm based on bicriteria programming',\n",
       "  'unlu1987'),\n",
       " (622,\n",
       "  'XGAGTYCV',\n",
       "  1,\n",
       "  'title',\n",
       "  'Bilinear Programming: an Exact Algorithm*',\n",
       "  'Gallo1977'),\n",
       " (628,\n",
       "  'N4F8UE57',\n",
       "  1,\n",
       "  'title',\n",
       "  'Adaptive algorithms for first principal eigenvector computation',\n",
       "  'Chatterjee2005'),\n",
       " (630,\n",
       "  '3RZCATAN',\n",
       "  1,\n",
       "  'title',\n",
       "  'Accelerated multiscale mechanics modeling in a deep learning framework',\n",
       "  'Gupta2022'),\n",
       " (633,\n",
       "  'F86H99HR',\n",
       "  1,\n",
       "  'title',\n",
       "  'A Review on Bilevel Optimization: From Classical to Evolutionary Approaches and Applications',\n",
       "  'Sinha2018'),\n",
       " (635,\n",
       "  'LBNYQCYI',\n",
       "  1,\n",
       "  'title',\n",
       "  '∈-Subgradient Algorithms for Bilevel Convex Optimization',\n",
       "  'Helou2017'),\n",
       " (637,\n",
       "  'YPVY67LG',\n",
       "  1,\n",
       "  'title',\n",
       "  'Efficient Gradient Approximation Method for Constrained Bilevel Optimization',\n",
       "  'Xu2023'),\n",
       " (639,\n",
       "  'RUGZRURD',\n",
       "  1,\n",
       "  'title',\n",
       "  'A Near-Optimal Algorithm for Bilevel Empirical Risk Minimization',\n",
       "  'Dagreou2023'),\n",
       " (641,\n",
       "  '6EJKG24B',\n",
       "  1,\n",
       "  'title',\n",
       "  'Bilevel Inverse Problems in Neuromorphic Imaging',\n",
       "  'Antil2023'),\n",
       " (643,\n",
       "  'WWPIMYFJ',\n",
       "  1,\n",
       "  'title',\n",
       "  'Implicit Bilevel Optimization: Differentiating through Bilevel Optimization Programming',\n",
       "  'Alesiani2023'),\n",
       " (645,\n",
       "  'K3KMVWJU',\n",
       "  1,\n",
       "  'title',\n",
       "  'A novel approach for bilevel programs based on Wolfe duality',\n",
       "  'Li2023a'),\n",
       " (647,\n",
       "  'FA7WG6GX',\n",
       "  1,\n",
       "  'title',\n",
       "  'Model Agnostic Sample Reweighting for Out-of-Distribution Learning',\n",
       "  'Zhou2022'),\n",
       " (649,\n",
       "  'XDKFE4C6',\n",
       "  1,\n",
       "  'title',\n",
       "  'Network Utility Maximization with Unknown Utility Functions: A Distributed, Data-Driven Bilevel Optimization Approach',\n",
       "  'Ji2023'),\n",
       " (651,\n",
       "  'ADGV57P4',\n",
       "  1,\n",
       "  'title',\n",
       "  'The Impact of Dedicated Lanes on On-Demand Multimodal Transit Systems',\n",
       "  'Lu2023'),\n",
       " (653,\n",
       "  'X9GETDTD',\n",
       "  1,\n",
       "  'title',\n",
       "  'On Penalty-based Bilevel Gradient Descent Method',\n",
       "  'Shen2023'),\n",
       " (655,\n",
       "  'YIBEDJ89',\n",
       "  1,\n",
       "  'title',\n",
       "  'Communication-Efficient Federated Bilevel Optimization with Local and Global Lower Level Problems',\n",
       "  'Li2023'),\n",
       " (657,\n",
       "  'YJRCTNBF',\n",
       "  1,\n",
       "  'title',\n",
       "  'Achieving Linear Speedup in Non-IID Federated Bilevel Learning',\n",
       "  'Huang2023'),\n",
       " (659,\n",
       "  'MB3PV964',\n",
       "  1,\n",
       "  'title',\n",
       "  'Inverse demand tracking in transportation networks',\n",
       "  'Mehlitz2022'),\n",
       " (661,\n",
       "  'UL4U7BAC',\n",
       "  1,\n",
       "  'title',\n",
       "  'Methodology and first-order algorithms for solving nonsmooth and non-strongly convex bilevel optimization problems',\n",
       "  'Doron2022'),\n",
       " (663,\n",
       "  'FQB6LZJ4',\n",
       "  1,\n",
       "  'title',\n",
       "  'Single-Level Differentiable Contact Simulation',\n",
       "  'Cleach2022'),\n",
       " (665,\n",
       "  'DXKQUNMN',\n",
       "  1,\n",
       "  'title',\n",
       "  'Input-to-State Stability of a Bilevel Proximal Gradient Descent Algorithm',\n",
       "  'Cunis2022'),\n",
       " (667,\n",
       "  'CFXI8JAW',\n",
       "  1,\n",
       "  'title',\n",
       "  'Quantifying the structural stability of simplicial homology',\n",
       "  'Guglielmi2023'),\n",
       " (669,\n",
       "  'BMZEGTY7',\n",
       "  1,\n",
       "  'title',\n",
       "  'First-order penalty methods for bilevel optimization',\n",
       "  'Lu2023a'),\n",
       " (671,\n",
       "  'GDC37DJN',\n",
       "  1,\n",
       "  'title',\n",
       "  'On Bilevel Optimization without Lower-level Strong Convexity',\n",
       "  'Chen2023'),\n",
       " (673,\n",
       "  'KAE6K34D',\n",
       "  1,\n",
       "  'title',\n",
       "  'Saliency-Augmented Memory Completion for Continual Learning',\n",
       "  'Bai2022'),\n",
       " (675,\n",
       "  'CESDTW8Q',\n",
       "  1,\n",
       "  'title',\n",
       "  'Generalized Frank-Wolfe Algorithm for Bilevel Optimization',\n",
       "  'Jiang2022'),\n",
       " (677,\n",
       "  'IQ825D6K',\n",
       "  1,\n",
       "  'title',\n",
       "  'A Single-Timescale Analysis For Stochastic Approximation With Multiple Coupled Sequences',\n",
       "  'Shen2022'),\n",
       " (679,\n",
       "  '5367LCVU',\n",
       "  1,\n",
       "  'title',\n",
       "  'Decentralized Gossip-Based Stochastic Bilevel Optimization over Communication Networks',\n",
       "  'Yang2022'),\n",
       " (681,\n",
       "  '5Q6BWBWV',\n",
       "  1,\n",
       "  'title',\n",
       "  'Optimising Electric Vehicle Charging Station Placement using Advanced Discrete Choice Models',\n",
       "  'Lamontagne2022'),\n",
       " (683,\n",
       "  'NK5ANRRN',\n",
       "  1,\n",
       "  'title',\n",
       "  'Simultaneous Spatial and Temporal Assignment for Fast UAV Trajectory Optimization using Bilevel Optimization',\n",
       "  'Chen2022a'),\n",
       " (685,\n",
       "  'S68TSLNF',\n",
       "  1,\n",
       "  'title',\n",
       "  'Decentralized Bilevel Optimization',\n",
       "  'Chen2022b'),\n",
       " (687,\n",
       "  'DT386PQ6',\n",
       "  1,\n",
       "  'title',\n",
       "  'Two-Timescale Stochastic Approximation for Bilevel Optimisation Problems in Continuous-Time Models',\n",
       "  'Sharrock2022'),\n",
       " (689,\n",
       "  'SXLZI77J',\n",
       "  1,\n",
       "  'title',\n",
       "  'Value Function Based Difference-of-Convex Algorithm for Bilevel Hyperparameter Selection Problems',\n",
       "  'Gao2022a'),\n",
       " (691,\n",
       "  'AGQTPT2D',\n",
       "  1,\n",
       "  'title',\n",
       "  'An Improved Unconstrained Approach for Bilevel Optimization',\n",
       "  'Hu2022'),\n",
       " (693,\n",
       "  'XQ492XTK',\n",
       "  1,\n",
       "  'title',\n",
       "  'On Stability and Generalization of Bilevel Optimization Problem',\n",
       "  'Ding2022'),\n",
       " (695,\n",
       "  '6TXXEUBF',\n",
       "  1,\n",
       "  'title',\n",
       "  'BOME! Bilevel Optimization Made Easy: A Simple First-Order Approach',\n",
       "  'Liu2022'),\n",
       " (697,\n",
       "  '87SPSVBW',\n",
       "  1,\n",
       "  'title',\n",
       "  'Optimal Retail Tariff Design with Prosumers: Pursuing Equity at the Expenses of Economic Efficiencies?',\n",
       "  'Chen2022c'),\n",
       " (699,\n",
       "  '3QN8LT7X',\n",
       "  1,\n",
       "  'title',\n",
       "  'Stochastic Bilevel Distributed Optimization over a Network',\n",
       "  'Gao2022'),\n",
       " (701,\n",
       "  '7WAJK947',\n",
       "  1,\n",
       "  'title',\n",
       "  'Online Bilevel Optimization: Regret Analysis of Online Alternating Gradient Methods',\n",
       "  'Tarzanagh2022'),\n",
       " (703,\n",
       "  'HINCII67',\n",
       "  1,\n",
       "  'title',\n",
       "  'Inverse Matrix Games With Unique Quantal Response Equilibrium',\n",
       "  'Yu2023'),\n",
       " (705,\n",
       "  'TL4JSHM6',\n",
       "  1,\n",
       "  'title',\n",
       "  'A Primal-Dual Approach to Bilevel Optimization with Multiple Inner Minima',\n",
       "  'Sow2022'),\n",
       " (707,\n",
       "  'U7VSMGYS',\n",
       "  1,\n",
       "  'title',\n",
       "  'Learning Sparsity-Promoting Regularizers using Bilevel Optimization',\n",
       "  'Ghosh2022'),\n",
       " (709,\n",
       "  'EST4982Y',\n",
       "  1,\n",
       "  'title',\n",
       "  'Robust Pivoting: Exploiting Frictional Stability Using Bilevel Optimization',\n",
       "  'Shirai2022'),\n",
       " (711,\n",
       "  'PIINZZSZ',\n",
       "  1,\n",
       "  'title',\n",
       "  'A Learnable Variational Model for\\xa0Joint Multimodal MRI Reconstruction and\\xa0Synthesis',\n",
       "  'Bian2022'),\n",
       " (713,\n",
       "  'T9IPBG26',\n",
       "  1,\n",
       "  'title',\n",
       "  'First-Order Bilevel Topology Optimization for Fast Mechanical Design',\n",
       "  'Pan2022'),\n",
       " (715,\n",
       "  'YS9ZQ4HQ',\n",
       "  1,\n",
       "  'title',\n",
       "  'Bilevel Online Deep Learning in Non-stationary Environment',\n",
       "  'Han2021'),\n",
       " (717,\n",
       "  '8GI6HS77',\n",
       "  1,\n",
       "  'title',\n",
       "  'Bilevel optimization with single-step inner methods',\n",
       "  'Suonpera2022'),\n",
       " (719,\n",
       "  '72DJH6VL',\n",
       "  1,\n",
       "  'title',\n",
       "  'A framework for bilevel optimization that enables stochastic and global variance reduction algorithms',\n",
       "  'Dagreou2022'),\n",
       " (721,\n",
       "  'V6KYW76N',\n",
       "  1,\n",
       "  'title',\n",
       "  'Set-based Meta-Interpolation for Few-Task Meta-Learning',\n",
       "  'Lee2022'),\n",
       " (723,\n",
       "  'GFD27SSZ',\n",
       "  1,\n",
       "  'title',\n",
       "  'Efficiently Escaping Saddle Points in Bilevel Optimization',\n",
       "  'Huang2022'),\n",
       " (725,\n",
       "  'ZJYHTBDY',\n",
       "  1,\n",
       "  'title',\n",
       "  'Towards Extremely Fast Bilevel Optimization with Self-governed Convergence Guarantees',\n",
       "  'Liu2022a'),\n",
       " (727,\n",
       "  'E557DDDB',\n",
       "  1,\n",
       "  'title',\n",
       "  'Local Stochastic Bilevel Optimization with Momentum-Based Variance Reduction',\n",
       "  'Li2022'),\n",
       " (729,\n",
       "  'MCJ7CNMD',\n",
       "  1,\n",
       "  'title',\n",
       "  'Duality theory for optimistic bilevel optimization',\n",
       "  'En-naciri2022'),\n",
       " (731,\n",
       "  'DCNALP8B',\n",
       "  1,\n",
       "  'title',\n",
       "  'Finding global solutions of some inverse optimal control problems using penalization and semismooth Newton methods',\n",
       "  'Friedemann2022'),\n",
       " (733,\n",
       "  'ZTE5RHYL',\n",
       "  1,\n",
       "  'title',\n",
       "  'BilevelJuMP.jl: Modeling and Solving Bilevel Optimization in Julia',\n",
       "  'Garcia2022'),\n",
       " (735,\n",
       "  'IZJ6SK2J',\n",
       "  1,\n",
       "  'title',\n",
       "  'Second-Order Sensitivity Analysis for Bilevel Optimization',\n",
       "  'Dyro2022'),\n",
       " (737,\n",
       "  'FTX5EYAJ',\n",
       "  1,\n",
       "  'title',\n",
       "  'Beyond Backpropagation: Bilevel Optimization Through Implicit Differentiation and Equilibrium Propagation',\n",
       "  'Zucchet2022'),\n",
       " (739,\n",
       "  'U4FTS88Y',\n",
       "  1,\n",
       "  'title',\n",
       "  'BiAdam: Fast Adaptive Bilevel Optimization Methods',\n",
       "  'Huang2021'),\n",
       " (741,\n",
       "  'BWLLQG63',\n",
       "  1,\n",
       "  'title',\n",
       "  'Tighter Analysis of Alternating Stochastic Gradient Method for Stochastic Nested Problems',\n",
       "  'Chen2021'),\n",
       " (743,\n",
       "  'ZHVKVS69',\n",
       "  1,\n",
       "  'title',\n",
       "  'Enhanced Bilevel Optimization via Bregman Distance',\n",
       "  'Huang2022a'),\n",
       " (746,\n",
       "  '5K3AE3BT',\n",
       "  1,\n",
       "  'title',\n",
       "  'A Gradient Method for Multilevel Optimization',\n",
       "  'Sato2021'),\n",
       " (748,\n",
       "  'KYW7LMT4',\n",
       "  1,\n",
       "  'title',\n",
       "  'Smooth Bilevel Programming for Sparse Regularization',\n",
       "  'Poon2021'),\n",
       " (750,\n",
       "  '2M38SR4F',\n",
       "  1,\n",
       "  'title',\n",
       "  'A Catalog of Formulations for the Network Pricing Problem',\n",
       "  'Bui2022'),\n",
       " (752,\n",
       "  'JLKZL69I',\n",
       "  1,\n",
       "  'title',\n",
       "  'Stability and Generalization of Bilevel Programming in Hyperparameter Optimization',\n",
       "  'Bao2021'),\n",
       " (754,\n",
       "  'RCTEHWMF',\n",
       "  1,\n",
       "  'title',\n",
       "  'Sign-MAML: Efficient Model-Agnostic Meta-Learning by SignSGD',\n",
       "  'Fan2021'),\n",
       " (756,\n",
       "  'EFARBJCI',\n",
       "  1,\n",
       "  'title',\n",
       "  'Bilevel Methods for Image Reconstruction',\n",
       "  'Crockett2022'),\n",
       " (760,\n",
       "  'DPGI86W5',\n",
       "  1,\n",
       "  'title',\n",
       "  'Credible Interdiction for Transmission Systems',\n",
       "  'Sundar2021'),\n",
       " (762,\n",
       "  'XCLF7C4H',\n",
       "  1,\n",
       "  'title',\n",
       "  'Generic Property of the Partial Calmness Condition for Bilevel Programming Problems',\n",
       "  'Ke2022'),\n",
       " (764,\n",
       "  'I5CUKMSD',\n",
       "  1,\n",
       "  'title',\n",
       "  'Combined approach with second-order optimality conditions for bilevel programming problems',\n",
       "  'Ma2021'),\n",
       " (766,\n",
       "  '2G9MIUPZ',\n",
       "  1,\n",
       "  'title',\n",
       "  'Bilevel Optimization for Machine Learning: Algorithm Design and Convergence Analysis',\n",
       "  'Ji2021'),\n",
       " (768,\n",
       "  'E64CWC84',\n",
       "  1,\n",
       "  'title',\n",
       "  'The Stochastic Bilevel Continuous Knapsack Problem with Uncertain Follower’s Objective',\n",
       "  'Buchheim2022'),\n",
       " (770,\n",
       "  '4FGKJDIE',\n",
       "  1,\n",
       "  'title',\n",
       "  'Sufficient optimality conditions in bilevel programming',\n",
       "  'Mehlitz2021'),\n",
       " (772,\n",
       "  'JLYLC8QJ',\n",
       "  1,\n",
       "  'title',\n",
       "  'Bilevel Optimization for On-Demand Multimodal Transit Systems',\n",
       "  'Basciftci2020'),\n",
       " (774,\n",
       "  'WW7N4GQ6',\n",
       "  1,\n",
       "  'title',\n",
       "  'Gauss–Newton-type methods for bilevel optimization',\n",
       "  'Fliege2021'),\n",
       " (776,\n",
       "  'AJWYDT2V',\n",
       "  1,\n",
       "  'title',\n",
       "  'Directional Necessary Optimality Conditions for Bilevel Programs',\n",
       "  'Bai2022a'),\n",
       " (778,\n",
       "  '7M5M5AFA',\n",
       "  1,\n",
       "  'title',\n",
       "  'Influence maximization with deactivation in social networks',\n",
       "  'Taninmis2019'),\n",
       " (780,\n",
       "  'R4XU57TF',\n",
       "  1,\n",
       "  'title',\n",
       "  'Bilevel Optimal Control: Existence Results and Stationarity Conditions',\n",
       "  'Mehlitz2020'),\n",
       " (782,\n",
       "  'KX88B488',\n",
       "  1,\n",
       "  'title',\n",
       "  'Bilevel Optimization and Variational Analysis',\n",
       "  'Mordukhovich2020'),\n",
       " (784,\n",
       "  'MAVUFTDA',\n",
       "  1,\n",
       "  'title',\n",
       "  'Constraint Qualifications and Optimality Conditions in Bilevel Optimization',\n",
       "  'Ye2020'),\n",
       " (786,\n",
       "  'WTDSXS2A',\n",
       "  1,\n",
       "  'title',\n",
       "  'Learning to play sequential games versus unknown opponents',\n",
       "  'Sessa2020'),\n",
       " (788,\n",
       "  'GL3T9J78',\n",
       "  1,\n",
       "  'title',\n",
       "  'A Gradient-based Bilevel Optimization Approach for Tuning Hyperparameters in Machine Learning',\n",
       "  'Sinha2020'),\n",
       " (790,\n",
       "  'JBN9BIKY',\n",
       "  1,\n",
       "  'title',\n",
       "  'On implicit variables in optimization theory',\n",
       "  'Benko2021'),\n",
       " (792,\n",
       "  'BNVSNCSI',\n",
       "  1,\n",
       "  'title',\n",
       "  'Bilevel Distributed Optimization in Directed Networks',\n",
       "  'Yousefian2021'),\n",
       " (794,\n",
       "  'J3F734QF',\n",
       "  1,\n",
       "  'title',\n",
       "  'On the Global Optimality of Model-Agnostic Meta-Learning',\n",
       "  'Wang2020'),\n",
       " (796,\n",
       "  '2Z7RQCNR',\n",
       "  1,\n",
       "  'title',\n",
       "  'Dynamic knapsack optimization towards efficient multi-channel sequential advertising',\n",
       "  'Hao2020'),\n",
       " (798,\n",
       "  'LXJZFP6M',\n",
       "  1,\n",
       "  'title',\n",
       "  'Modeling and optimization trade-off in meta-learning',\n",
       "  'Gao2020'),\n",
       " (800,\n",
       "  '4QQ8SMFX',\n",
       "  1,\n",
       "  'title',\n",
       "  '?-STN: Efficient bilevel optimization for neural networks using structured response Jacobians',\n",
       "  'Bae2020'),\n",
       " (802, 'PCAWQ2X3', 1, 'title', 'Bilevel Continual Learning', 'Shaker2021'),\n",
       " (804,\n",
       "  'C7M2VSLM',\n",
       "  1,\n",
       "  'title',\n",
       "  'Optimization Under Rare Chance Constraints',\n",
       "  'Tong2022'),\n",
       " (806,\n",
       "  '8TDTCJW6',\n",
       "  1,\n",
       "  'title',\n",
       "  'Improved Bilevel Model: Fast and Optimal Algorithm with Theoretical Guarantee',\n",
       "  'Li2020'),\n",
       " (808,\n",
       "  'FCM6R2BF',\n",
       "  1,\n",
       "  'title',\n",
       "  'Controlling a CyberOctopus Soft Arm with Muscle-like Actuation',\n",
       "  'Chang2021'),\n",
       " (810,\n",
       "  'KW42Z4UD',\n",
       "  1,\n",
       "  'title',\n",
       "  'Bilevel Optimization: Convergence Analysis and Enhanced Design',\n",
       "  'Ji2020'),\n",
       " (812,\n",
       "  'V296MRRP',\n",
       "  1,\n",
       "  'title',\n",
       "  'Semi-supervised batch active learning via bilevel optimization',\n",
       "  'Borsos2021'),\n",
       " (814,\n",
       "  'SUTM5Q9U',\n",
       "  1,\n",
       "  'title',\n",
       "  'Sensitivity-conditioning: Beyond Singular Perturbation for Control Design on Multiple Time Scales',\n",
       "  'Picallo2022'),\n",
       " (816,\n",
       "  'IZG9TLMD',\n",
       "  1,\n",
       "  'title',\n",
       "  'Optimizing Hyperparameters in CNNs using Bilevel Programming in Time Series Data',\n",
       "  'Seth2021'),\n",
       " (818,\n",
       "  'QTV3M7T9',\n",
       "  1,\n",
       "  'title',\n",
       "  'Lower Bounds and Accelerated Algorithms for Bilevel Optimization',\n",
       "  'Ji2021a'),\n",
       " (822,\n",
       "  'T23ECC4V',\n",
       "  1,\n",
       "  'title',\n",
       "  'Interdicting restructuring networks with applications in illicit trafficking',\n",
       "  'Kosmas2022'),\n",
       " (824,\n",
       "  '74QDVUN8',\n",
       "  1,\n",
       "  'title',\n",
       "  'Convergence Properties of Stochastic Hypergradients',\n",
       "  'Grazzi2020'),\n",
       " (826,\n",
       "  'QLGS3FMX',\n",
       "  1,\n",
       "  'title',\n",
       "  'Convergence and Sample Complexity of SGD in GANs',\n",
       "  'Kontonis2020'),\n",
       " (828,\n",
       "  'WSU6GPVE',\n",
       "  1,\n",
       "  'title',\n",
       "  'FairBatch: Batch Selection for Model Fairness',\n",
       "  'Roh2020'),\n",
       " (830,\n",
       "  '2CW4LMYK',\n",
       "  1,\n",
       "  'title',\n",
       "  'A regularization method for ILL-posed bilevel optimization problems',\n",
       "  'Bergounioux2006'),\n",
       " (832,\n",
       "  'RZVEM7E9',\n",
       "  1,\n",
       "  'title',\n",
       "  'Interdiction of a Markovian Evader',\n",
       "  'Gutfraind2011'),\n",
       " (834,\n",
       "  'YYCFLDCQ',\n",
       "  1,\n",
       "  'title',\n",
       "  'Techniques for Gradient-Based Bilevel Optimization with Non-smooth Lower Level Problems',\n",
       "  'Ochs2016'),\n",
       " (836,\n",
       "  'VLEB98Z3',\n",
       "  1,\n",
       "  'title',\n",
       "  'Self-tuning networks: Bilevel optimization of hyperparameters using structured best-response functions',\n",
       "  'Mackay2019'),\n",
       " (838,\n",
       "  'USKLZNPB',\n",
       "  1,\n",
       "  'title',\n",
       "  'A Near-Optimal Algorithm for Stochastic Bilevel Optimization via Double-Momentum',\n",
       "  'Khanduri2021'),\n",
       " (840,\n",
       "  'IQUPQ2Y2',\n",
       "  1,\n",
       "  'title',\n",
       "  'BODAME: Bilevel Optimization for Defense Against Model Extraction',\n",
       "  'Mori2021'),\n",
       " (842,\n",
       "  'T5IYSD49',\n",
       "  1,\n",
       "  'title',\n",
       "  'Crowd Motion Paradigm Modeled by a Bilevel Sweeping Control Problem',\n",
       "  'Cao2022'),\n",
       " (844,\n",
       "  'JFMRWU7H',\n",
       "  1,\n",
       "  'title',\n",
       "  'Learning to Continuously Optimize Wireless Resource in a Dynamic Environment: A Bilevel Optimization Perspective',\n",
       "  'Sun2022'),\n",
       " (848,\n",
       "  'UQ3ZP9WV',\n",
       "  1,\n",
       "  'title',\n",
       "  'Bilevel optimization based on iterative approximation of multiple mappings',\n",
       "  'Sinha2020a'),\n",
       " (850,\n",
       "  'U4WGC3PK',\n",
       "  1,\n",
       "  'title',\n",
       "  'Duality Approach to Bilevel Programs with a Convex Lower Level',\n",
       "  'Ouattara2018'),\n",
       " (852,\n",
       "  'QD2ZTDNF',\n",
       "  1,\n",
       "  'title',\n",
       "  'Source reconstruction using a bilevel optimisation method with a smooth weighted distance function',\n",
       "  'Brannstrom2016'),\n",
       " (855,\n",
       "  'BIBXG6VL',\n",
       "  1,\n",
       "  'title',\n",
       "  'A Decomposition Based Approach for Solving a General Bilevel Linear Programming',\n",
       "  'Liu2016'),\n",
       " (857,\n",
       "  'TIMTYM6M',\n",
       "  1,\n",
       "  'title',\n",
       "  'A bilevel approach for parameter learning in inverse problems',\n",
       "  'Holler2018'),\n",
       " (859,\n",
       "  'R22I3PLQ',\n",
       "  1,\n",
       "  'title',\n",
       "  'Exact penalization and necessary optimality conditions for generalized bilevel programming problems',\n",
       "  'Ye1997'),\n",
       " (870,\n",
       "  'KX96NT9Y',\n",
       "  1,\n",
       "  'title',\n",
       "  'On solving simple bilevel programs with a nonconvex lower level program',\n",
       "  'Lin2014'),\n",
       " (872,\n",
       "  '8VNM7A2I',\n",
       "  1,\n",
       "  'title',\n",
       "  'Is bilevel programming a special case of a mathematical program with complementarity constraints?',\n",
       "  'Dempe2012'),\n",
       " (881,\n",
       "  '9Y76YMZI',\n",
       "  1,\n",
       "  'title',\n",
       "  'Lagrange multipliers and optimality',\n",
       "  'Rockafellar1993'),\n",
       " (883,\n",
       "  'WBVL9TSH',\n",
       "  1,\n",
       "  'title',\n",
       "  'A Variational Approach to Lagrange Multipliers',\n",
       "  'Borwein2016'),\n",
       " (885,\n",
       "  '23TVFPBQ',\n",
       "  1,\n",
       "  'title',\n",
       "  'Lagrange Multipliers Can Fail to Determine Extrema',\n",
       "  'Nunemacher2003'),\n",
       " (887,\n",
       "  'HYPCVZJM',\n",
       "  1,\n",
       "  'title',\n",
       "  'A short elementary proof of the Lagrange multiplier theorem',\n",
       "  'Brezhneva2012'),\n",
       " (889,\n",
       "  'UX3MGI8B',\n",
       "  1,\n",
       "  'title',\n",
       "  'Optimality Conditions in Convex Optimization: A Finite-Dimensional View',\n",
       "  'Dutta2011'),\n",
       " (891,\n",
       "  'ULQGKA8I',\n",
       "  1,\n",
       "  'title',\n",
       "  'Direct theorems in semi-infinite convex programming',\n",
       "  'Borwein1981'),\n",
       " (893, 'DYHMYXDI', 1, 'title', 'Proximal Algorithms', 'Parikh2014'),\n",
       " (896,\n",
       "  'GX8DJPYF',\n",
       "  1,\n",
       "  'title',\n",
       "  'Near-optimal solutions of convex semi-infinite programs via targeted sampling',\n",
       "  'Das2022'),\n",
       " (915,\n",
       "  'WRNQAA7T',\n",
       "  1,\n",
       "  'title',\n",
       "  'Inexact Derivative-Free Optimization for Bilevel Learning',\n",
       "  'Ehrhardt2021'),\n",
       " (928,\n",
       "  'J6UB5VSV',\n",
       "  1,\n",
       "  'title',\n",
       "  'Fatigue crack propagation—an analysis',\n",
       "  'Tomkins1968'),\n",
       " (930,\n",
       "  '88L5JPY3',\n",
       "  1,\n",
       "  'title',\n",
       "  'Learning theory of randomized sparse Kaczmarz method',\n",
       "  'Lei2018'),\n",
       " (932,\n",
       "  'CZL7RGSJ',\n",
       "  1,\n",
       "  'title',\n",
       "  'A dislocation model for fatigue crack initiation',\n",
       "  'Tanaka1981'),\n",
       " (935,\n",
       "  'R69KWBZB',\n",
       "  1,\n",
       "  'title',\n",
       "  'Generation of Semi-Groups of Nonlinear Transformations on General Banach Spaces',\n",
       "  'Crandall1971'),\n",
       " (937,\n",
       "  'N44RWBNR',\n",
       "  1,\n",
       "  'title',\n",
       "  'Maximum Hands-Off Control with Time-Space Sparsity',\n",
       "  'Ikeda2021'),\n",
       " (941,\n",
       "  'GVSL54Z6',\n",
       "  1,\n",
       "  'title',\n",
       "  'Dislocation-based fracture mechanics within nonlocal and gradient elasticity of bi-Helmholtz type - Part I: Antiplane analysis',\n",
       "  'Mousavi2016'),\n",
       " (947,\n",
       "  'VXNQ6MRC',\n",
       "  1,\n",
       "  'title',\n",
       "  'Extension of sparse randomized kaczmarz algorithm for multiple measurement vectors',\n",
       "  'Aggarwal2014'),\n",
       " (956,\n",
       "  'TV93ENB2',\n",
       "  1,\n",
       "  'title',\n",
       "  'Linear convergence of the randomized sparse Kaczmarz method',\n",
       "  'Schopfer2019'),\n",
       " (959,\n",
       "  'NDN3GLWN',\n",
       "  1,\n",
       "  'title',\n",
       "  'The rate of convergence for the method of alternating projections, II',\n",
       "  'Deutsch1997'),\n",
       " (961,\n",
       "  'H6ZP6LB5',\n",
       "  1,\n",
       "  'title',\n",
       "  'For most large underdetermined systems of linear equations the minimal ℓ 1-norm solution is also the sparsest solution',\n",
       "  'Donoho2006'),\n",
       " (963,\n",
       "  'GUWQGCPC',\n",
       "  1,\n",
       "  'title',\n",
       "  'A fast randomized Kaczmarz algorithm for sparse solutions of consistent linear systems',\n",
       "  'Mansour2013'),\n",
       " (965, 'U9D53TQ3', 1, 'title', 'Notes on Randomized Kaczmarz', 'Schmidt2015'),\n",
       " (967, '4KF9M5N8', 1, 'title', 'Bilevel linear programming', 'Ben-ayed1993'),\n",
       " (969,\n",
       "  'TLT6W8WR',\n",
       "  1,\n",
       "  'title',\n",
       "  'A differential equations approach to l 1-minimization with applications to array imaging',\n",
       "  'Moscoso2012'),\n",
       " (971,\n",
       "  'AEKQM9AP',\n",
       "  1,\n",
       "  'title',\n",
       "  'A kaczmarz algorithm for solving tree based distributed systems of equations',\n",
       "  'Hegde2019'),\n",
       " (972,\n",
       "  'XP2LVELR',\n",
       "  1,\n",
       "  'title',\n",
       "  'Fast orthogonal sparse approximation algorithms over local dictionaries',\n",
       "  'Mailhe2011'),\n",
       " (974,\n",
       "  '9VZSGX4V',\n",
       "  1,\n",
       "  'title',\n",
       "  'Randomized extended average block kaczmarz for solving least squares',\n",
       "  'Du2020'),\n",
       " (976,\n",
       "  'ACY2Z6BX',\n",
       "  1,\n",
       "  'title',\n",
       "  'A note on sparse solutions of sparse linear systems',\n",
       "  'Yuan2014'),\n",
       " (978,\n",
       "  'SYLW8VD7',\n",
       "  1,\n",
       "  'title',\n",
       "  'The distance between two convex sets',\n",
       "  'Dax2006'),\n",
       " (980,\n",
       "  'KNYR4QI4',\n",
       "  1,\n",
       "  'title',\n",
       "  'A Hamilton–Jacobi-based proximal operator',\n",
       "  'Osher2023'),\n",
       " (983,\n",
       "  '2PLBN8GG',\n",
       "  1,\n",
       "  'title',\n",
       "  'Moreau’s Proximal Mappings and Convexity in Hamilton-Jacobi Theory',\n",
       "  'Rockafellar2006'),\n",
       " (985,\n",
       "  'LMIT9F5F',\n",
       "  1,\n",
       "  'title',\n",
       "  'Efficient point algorithm for a linear two-stage optimization problem',\n",
       "  'Haurie1990'),\n",
       " (987,\n",
       "  'BXQFVNZG',\n",
       "  1,\n",
       "  'title',\n",
       "  'The Method of Alternating Projections',\n",
       "  'Deutsch2001'),\n",
       " (989,\n",
       "  'G83MTYHE',\n",
       "  1,\n",
       "  'title',\n",
       "  'Model-agnostic meta-learning for fast adaptation of deep networks',\n",
       "  'Finn2017'),\n",
       " (991,\n",
       "  'BQAQ6S6P',\n",
       "  1,\n",
       "  'title',\n",
       "  'Generic methods for optimization-based modeling',\n",
       "  'Domke2012'),\n",
       " (996,\n",
       "  'Y9MKWRDB',\n",
       "  1,\n",
       "  'title',\n",
       "  'Lectures on Stochastic Programming: Modeling and Theory, Third Edition',\n",
       "  'Shapiro2021'),\n",
       " (998, '2WMXG5V8', 1, 'title', 'Stochastic Approximation', 'Borkar2008'),\n",
       " (1000,\n",
       "  'AVTWNRQ4',\n",
       "  1,\n",
       "  'title',\n",
       "  'A Two-Timescale Stochastic Algorithm Framework for Bilevel Optimization: Complexity Analysis and Application to Actor-Critic',\n",
       "  'Hong2023'),\n",
       " (1002,\n",
       "  '32XPMGIN',\n",
       "  1,\n",
       "  'title',\n",
       "  'Provably Faster Algorithms for Bilevel Optimization',\n",
       "  'Yang2021'),\n",
       " (1004,\n",
       "  'LK4LNP5M',\n",
       "  1,\n",
       "  'title',\n",
       "  'Approximation Methods for Bilevel Programming',\n",
       "  'Ghadimi2018'),\n",
       " (1008, 'R2GG9C3C', 1, 'title', 'Nonlinear Programming', 'Bazaraa2006'),\n",
       " (1010,\n",
       "  'XLNV8YQT',\n",
       "  1,\n",
       "  'title',\n",
       "  'Introduction to Optimization',\n",
       "  'Padregal2003'),\n",
       " (1012,\n",
       "  'K3B6252W',\n",
       "  1,\n",
       "  'title',\n",
       "  'Linear Programming and Network Flows',\n",
       "  'Bazaraa2009'),\n",
       " (1014,\n",
       "  'HZ6RZ4WV',\n",
       "  1,\n",
       "  'title',\n",
       "  'Practical Methods of Optimization',\n",
       "  'Fletcher2000'),\n",
       " (1018,\n",
       "  '7YSSSS5Y',\n",
       "  1,\n",
       "  'title',\n",
       "  'Optimization—Theory and Practice',\n",
       "  'Forst2010'),\n",
       " (1020, 'V47UFD67', 1, 'title', 'Numerical optimization', 'Nocedal2006'),\n",
       " (1022,\n",
       "  'FZ2J6UW9',\n",
       "  1,\n",
       "  'title',\n",
       "  'An overview of bilevel optimization',\n",
       "  'Colson2007'),\n",
       " (1024,\n",
       "  '6RXCS6CX',\n",
       "  1,\n",
       "  'title',\n",
       "  'Stochastic approximation with two time scales',\n",
       "  'Borkar1997'),\n",
       " (1026,\n",
       "  'DSBFU4IC',\n",
       "  1,\n",
       "  'title',\n",
       "  'Online algorithms for estimating change rates of web pages',\n",
       "  'Avrachenkov2022'),\n",
       " (1029,\n",
       "  'HG63K2JX',\n",
       "  1,\n",
       "  'title',\n",
       "  'A Single-Timescale Method for Stochastic Bilevel Optimization',\n",
       "  'Chen2022'),\n",
       " (1031,\n",
       "  'NF9328RX',\n",
       "  1,\n",
       "  'title',\n",
       "  'Stochastic first- and zeroth-order methods for nonconvex stochastic programming',\n",
       "  'Ghadimi2013'),\n",
       " (320,\n",
       "  'ZIJP72GB',\n",
       "  1,\n",
       "  'title',\n",
       "  'A dualized Kaczmarz algorithm in Hilbert and Banach space',\n",
       "  'Aboud2019'),\n",
       " (14,\n",
       "  'KN3GNAXU',\n",
       "  1,\n",
       "  'title',\n",
       "  'Introduction to Dislocation mechanics',\n",
       "  'Acharya'),\n",
       " (356,\n",
       "  'BUHNFE55',\n",
       "  1,\n",
       "  'title',\n",
       "  'Bibliography on the Kaczmarz Method',\n",
       "  'Cegielski2020'),\n",
       " (1556,\n",
       "  'SX9HFZV6',\n",
       "  1,\n",
       "  'title',\n",
       "  'An Active learning algorithm for ranking from pairwise preferences with an almost optimal query complexity',\n",
       "  'Ailon2012'),\n",
       " (1574,\n",
       "  'AZSERNWI',\n",
       "  1,\n",
       "  'title',\n",
       "  'Aggregating inconsistent information: Ranking and clustering',\n",
       "  'Ailon2008'),\n",
       " (1610,\n",
       "  '5VAKT6FZ',\n",
       "  1,\n",
       "  'title',\n",
       "  'A low memory, highly concurrent multigrid algorithm',\n",
       "  'Adams2012'),\n",
       " (1648,\n",
       "  '2JZ9EPED',\n",
       "  1,\n",
       "  'title',\n",
       "  'Rayleigh Quotient Iteration and Some generalizations for Nonnormal Matrices',\n",
       "  'Parlett1974'),\n",
       " (1682, 'AZCSU4V9', 1, 'title', 'Nonlinear Systems', 'Khalil2014'),\n",
       " (1841,\n",
       "  '8ZG6NSPD',\n",
       "  1,\n",
       "  'title',\n",
       "  'Principles of mathematical analysis',\n",
       "  'Rudin2008'),\n",
       " (497, 'J3L29JHJ', 1, 'title', '8. Maximum Hands-off Control', 'Nagahara2020'),\n",
       " (1846, 'CQFE96PZ', 1, 'title', 'Convex optimization theory', 'Bertsekas2009'),\n",
       " (944,\n",
       "  'TD6I25LG',\n",
       "  1,\n",
       "  'title',\n",
       "  'Convex Optimization: Algorithms and Complexity',\n",
       "  'Bubeck2015'),\n",
       " (1859, 'R37HP73W', 1, 'title', 'Convex Optimization', 'Boyd2004'),\n",
       " (993,\n",
       "  'BGFRXZZU',\n",
       "  1,\n",
       "  'title',\n",
       "  'Nyström Method for Accurate and Scalable Implicit Differentiation',\n",
       "  'Hataya2023'),\n",
       " (489,\n",
       "  'TRNCG7SV',\n",
       "  1,\n",
       "  'title',\n",
       "  'Lyapunov Arguments in Optimization',\n",
       "  'Wilson2018'),\n",
       " (442,\n",
       "  'NW34PSF3',\n",
       "  1,\n",
       "  'title',\n",
       "  \"Split feasibility problem and it's solution algorithms\",\n",
       "  'Qu2018'),\n",
       " (879,\n",
       "  'B64TVTKP',\n",
       "  1,\n",
       "  'title',\n",
       "  'A proof of the method of Lagrange Multipliers',\n",
       "  'Iyer'),\n",
       " (624,\n",
       "  '4P8XUHCA',\n",
       "  1,\n",
       "  'title',\n",
       "  'Foundations of Bilevel Programming',\n",
       "  'Dempe2005'),\n",
       " (1892,\n",
       "  'G9GAP343',\n",
       "  1,\n",
       "  'title',\n",
       "  'Meta-Learning with Implicit Gradients',\n",
       "  'Rajeswaran2019'),\n",
       " (1904,\n",
       "  '4TCHH3DV',\n",
       "  1,\n",
       "  'title',\n",
       "  'Recasting gradient-based meta-learning as Hierarchical Bayes',\n",
       "  'Grant2018'),\n",
       " (758,\n",
       "  'IWCFKRWW',\n",
       "  1,\n",
       "  'title',\n",
       "  'Towards Gradient-based Bilevel Optimization with Non-convex Followers and Beyond',\n",
       "  'Liu2021'),\n",
       " (1930,\n",
       "  'NDAXQPTT',\n",
       "  1,\n",
       "  'title',\n",
       "  'Solving Linear Equations by Monte Carlo Simulation',\n",
       "  'okten2005'),\n",
       " (1931,\n",
       "  'WKEIB3P2',\n",
       "  1,\n",
       "  'title',\n",
       "  'Multiway Monte Carlo Method for Linear Systems',\n",
       "  'Wu2019'),\n",
       " (1932,\n",
       "  'SI7RCS5P',\n",
       "  1,\n",
       "  'title',\n",
       "  'A Probabilistic Linear Solver Based on a Multilevel Monte Carlo Method',\n",
       "  'Acebron2020'),\n",
       " (1933,\n",
       "  'PZQ39PPS',\n",
       "  1,\n",
       "  'title',\n",
       "  'A Note on the Inversion of Matrices by Random Walks',\n",
       "  'Wasow1952'),\n",
       " (1934,\n",
       "  'VUTZNP8L',\n",
       "  1,\n",
       "  'title',\n",
       "  'Matrix inversion by a Monte Carlo method',\n",
       "  'Forsythe1950'),\n",
       " (1937,\n",
       "  'M5QUNHC3',\n",
       "  1,\n",
       "  'title',\n",
       "  'Gaussian elimination is not optimal',\n",
       "  'Strassen1969'),\n",
       " (1617,\n",
       "  '4BFIEB8W',\n",
       "  1,\n",
       "  'title',\n",
       "  'Problems in Signal Processing and Inference on Graphs',\n",
       "  'Agaskar2015'),\n",
       " (1940, 'M8742NRY', 1, 'title', 'Nonlinear programming', 'Bertsekas2016'),\n",
       " (290, 'UN4UEVVP', 1, 'title', 'Partial differential equations', 'Evans2010'),\n",
       " (1951,\n",
       "  'EC8HECN7',\n",
       "  1,\n",
       "  'title',\n",
       "  'Stories about maxima and minima',\n",
       "  'Tihomirov1990'),\n",
       " (39,\n",
       "  'C24R4FL4',\n",
       "  1,\n",
       "  'title',\n",
       "  'Bayesian inference in Inverse problems',\n",
       "  'Mallick2011'),\n",
       " (1961,\n",
       "  'RGL5BVFW',\n",
       "  1,\n",
       "  'title',\n",
       "  'EE227C: Convex Optimization and Approximation',\n",
       "  'Hardt2018'),\n",
       " (1968,\n",
       "  'RNYA5SAG',\n",
       "  1,\n",
       "  'title',\n",
       "  'Elementary Convexity with Optimization',\n",
       "  'Borkar2023'),\n",
       " (1977,\n",
       "  'LGIPAX7M',\n",
       "  1,\n",
       "  'title',\n",
       "  'Convex and stochastic optimization',\n",
       "  'Bonnans2019'),\n",
       " (1980,\n",
       "  'IXUQD4AS',\n",
       "  1,\n",
       "  'title',\n",
       "  'Bounding duality gap for separable problems with linear constraints',\n",
       "  'Udell2016'),\n",
       " (1985,\n",
       "  'MVNAP3IH',\n",
       "  1,\n",
       "  'title',\n",
       "  'A New Proof of the Lyapunov Convexity Theorem',\n",
       "  'Tardella1990'),\n",
       " (1988,\n",
       "  'RPTLPDGS',\n",
       "  1,\n",
       "  'title',\n",
       "  'An Approximate Shapley-Folkman Theorem',\n",
       "  'Kerdreux2018'),\n",
       " (1989,\n",
       "  'UITHCF6S',\n",
       "  1,\n",
       "  'title',\n",
       "  'Estimates of the duality gap in nonconvex optimization',\n",
       "  'Aubin1976'),\n",
       " (1990,\n",
       "  'R8NYDN9B',\n",
       "  1,\n",
       "  'title',\n",
       "  'Measures of the non-convexity of sets and the Shapley-Folkman-Starr theorem',\n",
       "  'Cassels1975'),\n",
       " (1999,\n",
       "  'XCXC32JT',\n",
       "  1,\n",
       "  'title',\n",
       "  'Approximately convex average sums of unbounded sets',\n",
       "  'Khan1974'),\n",
       " (2001,\n",
       "  'J6T5QPNQ',\n",
       "  1,\n",
       "  'title',\n",
       "  'Nonconvex Duality in Multiobjective Optimization',\n",
       "  'DiGuglielmo1977'),\n",
       " (2002,\n",
       "  '68KXJH92',\n",
       "  1,\n",
       "  'title',\n",
       "  'Accelerating conditional gradient methods',\n",
       "  'Kerdreux2020'),\n",
       " (2008,\n",
       "  'QDVZRKW6',\n",
       "  1,\n",
       "  'title',\n",
       "  'On the monotonicity of Minkowski sums towards convexity',\n",
       "  'Fradelizi2017'),\n",
       " (2021, 'RPWK3AG3', 1, 'title', 'Shapley-Folkman theorem', 'Starr2008'),\n",
       " (2023, '2RCYD5AE', 1, 'title', 'Collected Writings', 'Kumar2023'),\n",
       " (2025, '6G2NIYY2', 1, 'title', 'Set-Valued Analysis', 'Aubin2009'),\n",
       " (2049,\n",
       "  '733KMJE5',\n",
       "  1,\n",
       "  'title',\n",
       "  'EE363 : Linear Dynamical Systems',\n",
       "  'Boyd2009'),\n",
       " (2050, 'UAMAF28K', 1, 'title', 'The Isoperimetric Problem', 'Blasjo2005'),\n",
       " (2052,\n",
       "  'TYT7VIK5',\n",
       "  1,\n",
       "  'title',\n",
       "  'EE 377 (STAT 311) - Information theory and Statistics',\n",
       "  'Duchi2021'),\n",
       " (2060,\n",
       "  'TIHSDGWC',\n",
       "  1,\n",
       "  'title',\n",
       "  'On elastic solids with limiting small strain: modelling and analysis',\n",
       "  'Bulicek2014'),\n",
       " (2062,\n",
       "  'MJBT3QGE',\n",
       "  1,\n",
       "  'title',\n",
       "  'Flows on networks: recent results and perspectives',\n",
       "  'Bressan2014'),\n",
       " (2064, '7YBJL2W4', 1, 'title', 'Games on graphs', 'Allen2014'),\n",
       " (2066,\n",
       "  'NIS9MWY6',\n",
       "  1,\n",
       "  'title',\n",
       "  'On the convergence of stochastic bi-level gradient methods',\n",
       "  'Couellan2016'),\n",
       " (2068,\n",
       "  'H4SILD9W',\n",
       "  1,\n",
       "  'title',\n",
       "  'Projection-Free Stochastic Bi-Level Optimization',\n",
       "  'Akhtar2022'),\n",
       " (475,\n",
       "  '8P3KFZIB',\n",
       "  1,\n",
       "  'title',\n",
       "  'A Fast Iterative Shrinkage-Thresholding Algorithm for Linear Inverse Problems',\n",
       "  'Beck2009'),\n",
       " (2071,\n",
       "  'CA5RTJC3',\n",
       "  1,\n",
       "  'title',\n",
       "  'Recent Theoretical Advances in Non-Convex Optimization',\n",
       "  'Danilova2022'),\n",
       " (2074, 'HHSUQXD6', 1, 'title', 'Bregman Algorithms', 'Bush2011'),\n",
       " (2076, 'GLKPGHK9', 1, 'title', 'Random matrix theory', 'Edelman2005'),\n",
       " (2077,\n",
       "  '89X4LYKI',\n",
       "  1,\n",
       "  'title',\n",
       "  'Introduction to Random Matrices - Theory and Practice',\n",
       "  'Livan2018'),\n",
       " (2081,\n",
       "  'NAZ52QSA',\n",
       "  1,\n",
       "  'title',\n",
       "  'An Introduction to Control Theory',\n",
       "  'Quadrat2006'),\n",
       " (2083,\n",
       "  'GXRUTHIA',\n",
       "  1,\n",
       "  'title',\n",
       "  'An introduction to the lattice approach to stabilization problems',\n",
       "  'Quadrat2006a'),\n",
       " (2085,\n",
       "  'Y5ZRIZFC',\n",
       "  1,\n",
       "  'title',\n",
       "  'Linear control theory: An effective algebraic analysis approach',\n",
       "  'Quadrat2003'),\n",
       " (2088,\n",
       "  'RSZDH8XB',\n",
       "  1,\n",
       "  'title',\n",
       "  'An introduction to internal stabilization of linear inﬁnite-dimensional systems',\n",
       "  'Quadrat2003a'),\n",
       " (2097,\n",
       "  'AM5T36AN',\n",
       "  1,\n",
       "  'title',\n",
       "  'Some Fundamental Control Theory I: Controllability, Observability, and Duality',\n",
       "  'Terrell1999'),\n",
       " (2092,\n",
       "  'SSG4LNHE',\n",
       "  1,\n",
       "  'title',\n",
       "  'Some Fundamental Control Theory II: Feedback Linearization of single input nonlinear system',\n",
       "  'Terrell2018'),\n",
       " (2106,\n",
       "  'UVDSIKLJ',\n",
       "  1,\n",
       "  'title',\n",
       "  'Provable Alternating Minimization for Non-Convex Learning Problems',\n",
       "  'Netrapalli2014'),\n",
       " (2102,\n",
       "  'W53XHJFE',\n",
       "  1,\n",
       "  'title',\n",
       "  'Mathematical methods for Optimization - Dynamic Optimization',\n",
       "  'Evans2021a'),\n",
       " (2100,\n",
       "  '4AI3NRNX',\n",
       "  1,\n",
       "  'title',\n",
       "  'Mathematical Methods for Optimization Finite Dimensional Optimization',\n",
       "  'Evans2021'),\n",
       " (2091,\n",
       "  'QWUDCTW7',\n",
       "  1,\n",
       "  'title',\n",
       "  'A Mathematical Approach to Classical Control',\n",
       "  'Lewis2016'),\n",
       " (314,\n",
       "  'UIKWGIQ8',\n",
       "  1,\n",
       "  'title',\n",
       "  'Boundedness for the iterates of an iterative projection algorithms',\n",
       "  'Terra2012'),\n",
       " (2113,\n",
       "  'NUIVC8XC',\n",
       "  1,\n",
       "  'title',\n",
       "  'Do projections stay close together?',\n",
       "  'Kirchheim2009'),\n",
       " (2116, 'CUN9HMSZ', 1, 'title', 'On products of projections', 'Meshulam1996'),\n",
       " (2139,\n",
       "  'MYG5HYEB',\n",
       "  1,\n",
       "  'title',\n",
       "  'Sequential Difference-of-Convex Programming',\n",
       "  'DeOliveira2020'),\n",
       " (2141,\n",
       "  'L84AZXSF',\n",
       "  1,\n",
       "  'title',\n",
       "  'Semidefinite Programming vs. LP Relaxations for Polynomial Programming',\n",
       "  'Lasserre2002'),\n",
       " (2142,\n",
       "  'LFHGKH5R',\n",
       "  1,\n",
       "  'title',\n",
       "  'Collected Lectures on Difference of Convex Programming',\n",
       "  'zotero-2142'),\n",
       " (2147,\n",
       "  'NDDCVQTC',\n",
       "  1,\n",
       "  'title',\n",
       "  'Nonsmooth DC optimization: recent developments',\n",
       "  'zotero-2147'),\n",
       " (2145,\n",
       "  'NEVPSSTE',\n",
       "  1,\n",
       "  'title',\n",
       "  'Smooth approximations of D.C. functions',\n",
       "  'Polyakova'),\n",
       " (2159,\n",
       "  'QHYBBJVD',\n",
       "  1,\n",
       "  'title',\n",
       "  'Writing Systems and Networking Articles',\n",
       "  'zotero-2159'),\n",
       " (2162,\n",
       "  'SYGSGKBS',\n",
       "  1,\n",
       "  'title',\n",
       "  \"Whitesides' Group: Writing a Paper\",\n",
       "  'Whitesides2004'),\n",
       " (2164,\n",
       "  'C25WS4RA',\n",
       "  1,\n",
       "  'title',\n",
       "  'Writing reviews for systems conferences',\n",
       "  'Roscoe'),\n",
       " (2166, 'BGW8YZT4', 1, 'title', 'How to Read Paper', 'keshav'),\n",
       " (2199,\n",
       "  'ZGBMABRG',\n",
       "  1,\n",
       "  'title',\n",
       "  'The Approximate Duality Gap Technique: A Unified Theory of First-Order Methods',\n",
       "  'Diakonikolas2019'),\n",
       " (2206,\n",
       "  'IBGVHNXB',\n",
       "  1,\n",
       "  'title',\n",
       "  'Approximation of Convex Envelope Using Reinforcement Learning',\n",
       "  'Borkar2023a'),\n",
       " (2213,\n",
       "  '9R87NLTA',\n",
       "  1,\n",
       "  'title',\n",
       "  'The Brunn-Minkowski inequality',\n",
       "  'Gardner2002'),\n",
       " (2219,\n",
       "  'QAXRATHA',\n",
       "  1,\n",
       "  'title',\n",
       "  'Minkowski sum boundary surfaces of 3D-objects',\n",
       "  'Peternell2007'),\n",
       " (2223,\n",
       "  'B3M9YCEK',\n",
       "  1,\n",
       "  'title',\n",
       "  'A monotonic convolution for Minkowski sums',\n",
       "  'Milenkovic2007'),\n",
       " (2224,\n",
       "  '3HUSHI9S',\n",
       "  1,\n",
       "  'title',\n",
       "  'Computing convolutions by reciprocal search',\n",
       "  'Guibas1986'),\n",
       " (2225,\n",
       "  '3N8SVK7A',\n",
       "  1,\n",
       "  'title',\n",
       "  'Computing convolutions and Minkowski sums via support functions',\n",
       "  'sir2006'),\n",
       " (2245,\n",
       "  'XMRSNPK4',\n",
       "  1,\n",
       "  'title',\n",
       "  'The convex envelope is the solution of a nonlinear obstacle problem',\n",
       "  'Oberman2007'),\n",
       " (2246,\n",
       "  'QYXVKXWR',\n",
       "  1,\n",
       "  'title',\n",
       "  'Computing the Level Set Convex Hull',\n",
       "  'Abbasi2018'),\n",
       " (2247,\n",
       "  'V3I97W65',\n",
       "  1,\n",
       "  'title',\n",
       "  'Exponential convergence for a convexifying equation',\n",
       "  'Carlier2012'),\n",
       " (2248,\n",
       "  'TAC5CCVG',\n",
       "  1,\n",
       "  'title',\n",
       "  'Two-scale methods for convex envelopes',\n",
       "  'Li2021'),\n",
       " (2250,\n",
       "  'Y2ZTWYCK',\n",
       "  1,\n",
       "  'title',\n",
       "  'A Partial Differential Equation for the Rank One Convex Envelope',\n",
       "  'Oberman2017'),\n",
       " (2251,\n",
       "  'ZI55TRFC',\n",
       "  1,\n",
       "  'title',\n",
       "  'The Dirichlet problem for the convex envelope',\n",
       "  'Oberman2011'),\n",
       " (2252,\n",
       "  'ZKNC9EJH',\n",
       "  1,\n",
       "  'title',\n",
       "  'A method to convexify functions via curve evolution',\n",
       "  'Vese1999'),\n",
       " (2249,\n",
       "  'TKXBP9NY',\n",
       "  1,\n",
       "  'title',\n",
       "  'Computing the Convex Envelop using a nonlinear partial differential equation',\n",
       "  'Oberman2008'),\n",
       " (2263,\n",
       "  'P3Q77HTP',\n",
       "  1,\n",
       "  'title',\n",
       "  'On the smoothness of convex envelopes',\n",
       "  'Griewank1990'),\n",
       " (2211,\n",
       "  'YPF8RK89',\n",
       "  1,\n",
       "  'title',\n",
       "  'Convex Bodies: The Brunn–Minkowski Theory',\n",
       "  'Schneider2014'),\n",
       " (2265,\n",
       "  'SZFQTK9W',\n",
       "  1,\n",
       "  'title',\n",
       "  'Approximated Convex Envelope of a Function',\n",
       "  'Brighi1994'),\n",
       " (2267,\n",
       "  'RQLTTV5T',\n",
       "  1,\n",
       "  'title',\n",
       "  'A fast computational algorithm for the Legendre-Fenchel transform',\n",
       "  'Lucet1996'),\n",
       " (2271,\n",
       "  '94Y24AA9',\n",
       "  1,\n",
       "  'title',\n",
       "  'The Legendre–Fenchel Transform and the Convex Hull of a Function: Fast Computational Algorithms, Second-Order Smoothness and Analysis',\n",
       "  'Lucet1997'),\n",
       " (2274,\n",
       "  '6WN9AGEY',\n",
       "  1,\n",
       "  'title',\n",
       "  'Legendre-Fenchel transforms in a nutshell',\n",
       "  'Touchette'),\n",
       " (2280,\n",
       "  'T3XYVZIP',\n",
       "  1,\n",
       "  'title',\n",
       "  'An Introduction to Laplace Transforms and Fourier Series',\n",
       "  'Dyke2014'),\n",
       " (2284,\n",
       "  'BPS4WRHB',\n",
       "  1,\n",
       "  'title',\n",
       "  'On the Global Convergence of Gradient Descent for Over-parameterized Models using Optimal Transport',\n",
       "  'Chizat'),\n",
       " (2289,\n",
       "  'EB7CBIL2',\n",
       "  1,\n",
       "  'title',\n",
       "  'Sampling can be faster than optimization',\n",
       "  'Ma2019'),\n",
       " (2290,\n",
       "  '9EY452RP',\n",
       "  1,\n",
       "  'title',\n",
       "  'Mean-field limits and beyond: Large deviations for singular interacting diffusions and variational convergence for population dynamics',\n",
       "  'Hoeksema2023'),\n",
       " (2293,\n",
       "  'ZHUPKXN2',\n",
       "  1,\n",
       "  'title',\n",
       "  'Implicit bias of gradient descent for wide two-layer neural networks trained with the logistic loss',\n",
       "  'Chizat2020'),\n",
       " (2297,\n",
       "  'DL84AV32',\n",
       "  1,\n",
       "  'title',\n",
       "  'Controlled diffusion processes',\n",
       "  'Borkar2005'),\n",
       " (2305,\n",
       "  'TV7EF8NR',\n",
       "  1,\n",
       "  'title',\n",
       "  'Business Dynamics: Systems Thinking and Modeling for a Complex World',\n",
       "  'Sterman2000'),\n",
       " (2315,\n",
       "  'HC48X7LX',\n",
       "  1,\n",
       "  'title',\n",
       "  'Microelectronics: circuit analysis and design',\n",
       "  'Neamen2010'),\n",
       " (2317,\n",
       "  'V7HQ9MXB',\n",
       "  1,\n",
       "  'title',\n",
       "  'Fox and McDonald′s Introduction to Fluid Mechanics',\n",
       "  'Pritchard2010'),\n",
       " (2326, '3ADSLNVF', 1, 'title', 'A Course in Probability Theory', 'Chung2000'),\n",
       " (2323,\n",
       "  'G5RGAS9N',\n",
       "  1,\n",
       "  'title',\n",
       "  'Symmetries, Flat Minima and The Conserved quantities of Gradient Flow',\n",
       "  'Zhao2023'),\n",
       " (2329,\n",
       "  'DPXUVVDG',\n",
       "  1,\n",
       "  'title',\n",
       "  'Deep Equals Shallow for ReLU Networks in Kernel Regimes',\n",
       "  'Bietti2021'),\n",
       " (2332,\n",
       "  '53RJAB6B',\n",
       "  1,\n",
       "  'title',\n",
       "  'Any Deep ReLU Network is Shallow',\n",
       "  'Villani2023'),\n",
       " (2104,\n",
       "  'L6GK6MFG',\n",
       "  1,\n",
       "  'title',\n",
       "  'An Introduction to Mathematical Optimal Control Theory',\n",
       "  'Evans'),\n",
       " (2388, 'TAGXADRL', 1, 'title', 'Beginning Functional Analysis', 'Saxe2001'),\n",
       " (2401,\n",
       "  'EXSNRXTC',\n",
       "  1,\n",
       "  'title',\n",
       "  'Elements of the Theory of Functions and Functional Analysis, Vol. I: Metric and Normed Spaces',\n",
       "  'Kolmogorov1954'),\n",
       " (2396,\n",
       "  '59EDJEQK',\n",
       "  1,\n",
       "  'title',\n",
       "  'Elements of the Theory of Functions and Functional Analysis, Vol. II: Measure, The Lebesgue Integral, Hilbert Space',\n",
       "  'Kolmogorov1960'),\n",
       " (2405,\n",
       "  '87MZZDKG',\n",
       "  1,\n",
       "  'title',\n",
       "  'Introductory Functional Analysis with Applications',\n",
       "  'Kreyszig1989'),\n",
       " (2409,\n",
       "  'LHNL6N4K',\n",
       "  1,\n",
       "  'title',\n",
       "  'Functional Analysis, Sobolev Spaces and Partial Differential Equations',\n",
       "  'Brezis2011'),\n",
       " (2415,\n",
       "  'U8DG8YD2',\n",
       "  1,\n",
       "  'title',\n",
       "  'Real analysis: modern techniques and their applications',\n",
       "  'Folland1999'),\n",
       " (2420,\n",
       "  'JHARL7CQ',\n",
       "  1,\n",
       "  'title',\n",
       "  'Integration and probability',\n",
       "  'Malliavin1995'),\n",
       " (2423, 'TDBC275P', 1, 'title', 'Probability and measure', 'Billingsley1995'),\n",
       " (2424, '8KUMGKB2', 1, 'title', 'Random Processes for Engineers', 'Hajek2015'),\n",
       " (2425,\n",
       "  '7VJIRWUM',\n",
       "  1,\n",
       "  'title',\n",
       "  'Probability and random processes',\n",
       "  'Grimmett2007'),\n",
       " (2426,\n",
       "  'IWI8WTJT',\n",
       "  1,\n",
       "  'title',\n",
       "  'Discrete Event  Stochastic Process',\n",
       "  'Kumar2012'),\n",
       " (2462, '9Q4PUE3N', 1, 'title', 'Introduction to Optimal Transport', 'Thorpe'),\n",
       " (2463,\n",
       "  'E2XJGMA3',\n",
       "  1,\n",
       "  'title',\n",
       "  'Learning Theory from First Principles',\n",
       "  'Bach'),\n",
       " (2464,\n",
       "  'WQHNK93Z',\n",
       "  1,\n",
       "  'title',\n",
       "  'Non-convex Optimization for Machine Learning',\n",
       "  'Jain2017'),\n",
       " (2465,\n",
       "  'X5T7DYXD',\n",
       "  1,\n",
       "  'title',\n",
       "  'Principles of Foundation Engineering',\n",
       "  'Das2017'),\n",
       " (2469, 'X6FZTY3K', 1, 'title', 'Advanced mechanics of solids', 'Srinath2009'),\n",
       " (2475, 'HVV5DMT2', 1, 'title', 'Stochastic processes', 'Ross2013'),\n",
       " (2478, '4JXH2XM2', 1, 'title', 'Mathematical Control Theory', 'Sontag1998'),\n",
       " (2479,\n",
       "  'KYBHKAVS',\n",
       "  1,\n",
       "  'title',\n",
       "  'Theory of elastic stability',\n",
       "  'Timoshenko2009'),\n",
       " (2480,\n",
       "  'VPXP2AK5',\n",
       "  1,\n",
       "  'title',\n",
       "  'Calculus on manifolds: a modern approach to classical theorems of advanced calculus',\n",
       "  'Spivak1965'),\n",
       " (2512,\n",
       "  'YVSPZIPB',\n",
       "  1,\n",
       "  'title',\n",
       "  'Calculus of variations and optimal control theory: a concise introduction',\n",
       "  'Liberzon2012'),\n",
       " (2514, 'CYF3H42W', 1, 'title', 'Stochastic Steffensen Method', 'Zhao2022'),\n",
       " (2520,\n",
       "  'J8RIPNJ2',\n",
       "  1,\n",
       "  'title',\n",
       "  'Partial Differential Equations in Action',\n",
       "  'Salsa2016'),\n",
       " (2526,\n",
       "  'ULZP4LQ7',\n",
       "  1,\n",
       "  'title',\n",
       "  'Approximate Causal Effect Identification under Weak Confounding',\n",
       "  'Jiang2023'),\n",
       " (2535,\n",
       "  'SHUUHMFJ',\n",
       "  1,\n",
       "  'title',\n",
       "  'Differential Equations, Dynamical Systems, and an Introduction to Chaos',\n",
       "  'Hirsch2013'),\n",
       " (2539,\n",
       "  'ASN6GNPX',\n",
       "  1,\n",
       "  'title',\n",
       "  'Differential equations and dynamical systems',\n",
       "  'Perko2009'),\n",
       " (2545,\n",
       "  'QME944RQ',\n",
       "  1,\n",
       "  'title',\n",
       "  'Optimal Transport for Applied Mathematicians: Calculus of Variations, PDEs, and Modeling',\n",
       "  'Santambrogio2015'),\n",
       " (2549,\n",
       "  'BLK28USM',\n",
       "  1,\n",
       "  'title',\n",
       "  'On the Tendency Toward Convexity of the Vector Sum of Sets',\n",
       "  'Howe2012'),\n",
       " (6494, '9DHKR7F5', 1, 'title', 'Simulation', 'Ross2013a'),\n",
       " (2312,\n",
       "  'TL5MUV7C',\n",
       "  1,\n",
       "  'title',\n",
       "  'Lecture Notes: Transportation Engineering',\n",
       "  'Reddy2014'),\n",
       " (6502,\n",
       "  'L67A9ZG3',\n",
       "  1,\n",
       "  'title',\n",
       "  'Stochastic differential equations: an introduction with applications',\n",
       "  'oksendal2013'),\n",
       " (6504, '8FISQTEU', 1, 'title', 'Overview of Sampling Theory', 'zotero-6504'),\n",
       " (6512,\n",
       "  '3MLSF4GH',\n",
       "  1,\n",
       "  'title',\n",
       "  'Game Theory Meets Constrained Nonlinear Optimization',\n",
       "  'Dewaskar'),\n",
       " (6513,\n",
       "  'REDSUE33',\n",
       "  1,\n",
       "  'title',\n",
       "  'Understanding Notions of Stationarity in Non-Smooth Optimization',\n",
       "  'Li2020a'),\n",
       " (6520,\n",
       "  '7UWS8YYW',\n",
       "  1,\n",
       "  'title',\n",
       "  'Generalized inverse-positivity and splittings of M-matrices',\n",
       "  'Neumann1979'),\n",
       " (6523, 'HUFFMRIC', 1, 'title', 'Algorithmic game theory', 'Nisan2007'),\n",
       " (6525,\n",
       "  'G8AXY2EP',\n",
       "  1,\n",
       "  'title',\n",
       "  'A symmetrization for finite two-person games',\n",
       "  'Jurg1992'),\n",
       " (6526,\n",
       "  'GH7IVJP2',\n",
       "  1,\n",
       "  'title',\n",
       "  'Completely Mixed Strategies in Bimatrix Games',\n",
       "  'Raghavan1970'),\n",
       " (6534, 'JDDMSGAY', 1, 'title', 'Linear Algebra Done Right', 'Axler2024'),\n",
       " (6535, 'F79MNIM5', 1, 'title', 'Applied Nonlinear Analysis', 'Ekeland2006'),\n",
       " (6537,\n",
       "  'X6U8BEQ8',\n",
       "  1,\n",
       "  'title',\n",
       "  'Some topics in two-person games',\n",
       "  'Parthasarathy1971'),\n",
       " (6541, 'XVZDCPUL', 1, 'title', 'Convex Analysis', 'Rockafellar1996'),\n",
       " (6546,\n",
       "  'MU9HSBIG',\n",
       "  1,\n",
       "  'title',\n",
       "  'Equilibrium Points of Bimatrix Games on JSTOR',\n",
       "  'zotero-6546'),\n",
       " (6548,\n",
       "  'FCCGDB8L',\n",
       "  1,\n",
       "  'title',\n",
       "  'Equilibrium Points of Bimatrix Games',\n",
       "  'Lemke1964'),\n",
       " (6550,\n",
       "  'WRSZCDCC',\n",
       "  1,\n",
       "  'title',\n",
       "  'Imitation games and computation',\n",
       "  'Mclennan2010'),\n",
       " (6564,\n",
       "  'VM8XSFG6',\n",
       "  1,\n",
       "  'title',\n",
       "  'Two-person nonzero-sum games and quadratic programming',\n",
       "  'Mangasarian1964'),\n",
       " (6566, 'FTG8QBMD', 1, 'title', 'Nonlinear Programming', 'Kuhn'),\n",
       " (6569,\n",
       "  '987PGFQV',\n",
       "  1,\n",
       "  'title',\n",
       "  'MA-INF 1301: Algorithmic Game Theory and the Internet [Informatik-Abteilung V]',\n",
       "  'zotero-6569'),\n",
       " (6580,\n",
       "  'NHM9EKT4',\n",
       "  1,\n",
       "  'title',\n",
       "  'On the Convergence of Block Coordinate Descent Type Methods',\n",
       "  'zotero-6580'),\n",
       " (6582,\n",
       "  'LW9DN8ED',\n",
       "  1,\n",
       "  'title',\n",
       "  'Convergence analysis of the Fast Subspace Descent method for convex optimization problems',\n",
       "  '2018'),\n",
       " (6585, 'CDL7X8G9', 1, 'title', 'Randomized Subspace Descent', 'Frongillo'),\n",
       " (6589,\n",
       "  'Z6Q6ESC9',\n",
       "  1,\n",
       "  'title',\n",
       "  'On the Convergence of Block Coordinate Descent Type Methods',\n",
       "  'Beck2013'),\n",
       " (6590,\n",
       "  'ZK757MRG',\n",
       "  1,\n",
       "  'title',\n",
       "  'Convergence analysis of the Fast Subspace Descent method for convex optimization problems',\n",
       "  'Chen2020'),\n",
       " (6593, '9G5G4W5K', 1, 'title', 'Convex Optimization', 'Tibshirani'),\n",
       " (6596,\n",
       "  'NPG6855A',\n",
       "  1,\n",
       "  'title',\n",
       "  'A Stochastic Approximation Method',\n",
       "  'Robbins1951'),\n",
       " (6598,\n",
       "  'DDK2DSG6',\n",
       "  1,\n",
       "  'title',\n",
       "  'Approximability of Symmetric Bimatrix Games and Related Experiments',\n",
       "  'Kontogiannis2011'),\n",
       " (6604,\n",
       "  'FGKUKTI2',\n",
       "  1,\n",
       "  'title',\n",
       "  'Theory of linear and integer programming',\n",
       "  'Schrijver2011'),\n",
       " (6605, 'KK8UI6P3', 1, 'title', 'Probability Theory', 'Borkar1995'),\n",
       " (6606, 'UTQJR277', 1, 'title', 'Markov chains', 'Norris1998'),\n",
       " (2428,\n",
       "  '45JPUPDP',\n",
       "  1,\n",
       "  'title',\n",
       "  'Lecture Notes: Probability Methods (EE734)',\n",
       "  'Borkar2020'),\n",
       " (2427,\n",
       "  'G6MQSPLP',\n",
       "  1,\n",
       "  'title',\n",
       "  'Lecture Notes: Probabillity Theory (MA812)',\n",
       "  'K2020'),\n",
       " (2411,\n",
       "  'SG3VP3PZ',\n",
       "  1,\n",
       "  'title',\n",
       "  'Lecture Notes: Linear Systems (IE614)',\n",
       "  'Balamurugan2019'),\n",
       " (2509,\n",
       "  'UNBW8EBI',\n",
       "  1,\n",
       "  'title',\n",
       "  'Lecture Notes: Control of Stochastic Systems (ECE 555)',\n",
       "  'Raginsky2019'),\n",
       " (6640,\n",
       "  '8HIJLTYX',\n",
       "  1,\n",
       "  'title',\n",
       "  'Enumeration of Nash equilibria for two-player games',\n",
       "  'Avis2010'),\n",
       " (6654,\n",
       "  'NPWJRJZ5',\n",
       "  1,\n",
       "  'title',\n",
       "  'Lecture Notes : Nonlinear Systems - Ovidiu Costin (OSU)',\n",
       "  'Costin'),\n",
       " (6656,\n",
       "  'WQMZB9Y5',\n",
       "  1,\n",
       "  'title',\n",
       "  'Linear Conic Optimization for Inverse Optimal Control',\n",
       "  'Pauwels2016'),\n",
       " (6658,\n",
       "  '2JSZ378F',\n",
       "  1,\n",
       "  'title',\n",
       "  'The Bayesian Approach to Inverse Problems',\n",
       "  'Dashti2017'),\n",
       " (6670,\n",
       "  'ZPYIZLPA',\n",
       "  1,\n",
       "  'title',\n",
       "  'Sparse high-dimensional regression: Exact scalable algorithms and phase transitions',\n",
       "  'Bertsimas2020'),\n",
       " (6671,\n",
       "  '6TGWFKRI',\n",
       "  1,\n",
       "  'title',\n",
       "  'Using L1-relaxation and integer programming to obtain dual bounds for sparse PCA',\n",
       "  'Dey2021'),\n",
       " (6678,\n",
       "  'JP7PBGDM',\n",
       "  1,\n",
       "  'title',\n",
       "  'Random Function Iterations for Stochastic Fixed Point Problems',\n",
       "  'Hermer2022'),\n",
       " (6681,\n",
       "  'PWP6TRJY',\n",
       "  1,\n",
       "  'title',\n",
       "  'Controllability of Network Opinion in Erdos-Renyi Graphs using Sparse Control Inputs',\n",
       "  'Joseph2020'),\n",
       " (6683,\n",
       "  'LRHT9IA4',\n",
       "  1,\n",
       "  'title',\n",
       "  'Nonparametric Compositional Stochastic Optimization for Risk-Sensitive Kernel Learning',\n",
       "  'Bedi2021'),\n",
       " (6685,\n",
       "  'TGZS2GAG',\n",
       "  1,\n",
       "  'title',\n",
       "  'Imposing uniqueness to achieve sparsity',\n",
       "  'Dillon2016'),\n",
       " (6687,\n",
       "  'CDWNYPIU',\n",
       "  1,\n",
       "  'title',\n",
       "  'A Unique “Nonnegative” Solution to an Underdetermined System: From Vectors to Matrices',\n",
       "  'Wang2011'),\n",
       " (6689,\n",
       "  'XNB9BHCQ',\n",
       "  1,\n",
       "  'title',\n",
       "  'Lasso formulation of the shortest path problem',\n",
       "  'Dong2020'),\n",
       " (6820, 'EUTTM3QP', 1, 'title', 'Lecture Notes: Markov Chains', 'Norris'),\n",
       " (7144, 'NA5UHIE8', 1, 'title', 'Foundations of Optimization', 'Bazaraa1976'),\n",
       " (7217,\n",
       "  'AZ9RD4MH',\n",
       "  1,\n",
       "  'title',\n",
       "  'Convex Analysis and Nonlinear Optimization - Theory and Examples',\n",
       "  'Borwein2006'),\n",
       " (7219,\n",
       "  'YE8AL5EC',\n",
       "  1,\n",
       "  'title',\n",
       "  'Three modeling paradigms in mathematical programming',\n",
       "  'Pang2010'),\n",
       " (7383,\n",
       "  'TN7ALP6S',\n",
       "  1,\n",
       "  'title',\n",
       "  'Linear Convergence of Gradient and Proximal-Gradient Methods Under the Polyak-Łojasiewicz Condition',\n",
       "  'Karimi2016'),\n",
       " (7463,\n",
       "  'GUT68ITQ',\n",
       "  1,\n",
       "  'title',\n",
       "  'Analyzing Inverse Problems with Invertible Neural Networks',\n",
       "  'Ardizzone2019'),\n",
       " (7485,\n",
       "  'LQW3DPKF',\n",
       "  1,\n",
       "  'title',\n",
       "  'Constrained optimization and Lagrange multiplier methods',\n",
       "  'Bertsekas1996'),\n",
       " (7559,\n",
       "  'LHLWVIXR',\n",
       "  1,\n",
       "  'title',\n",
       "  'Chance-constrained optimization under limited distributional information: A review of reformulations based on sampling and distributional robustness',\n",
       "  'Kucukyavuz2022'),\n",
       " (7560,\n",
       "  'VNHXNV7B',\n",
       "  1,\n",
       "  'title',\n",
       "  'Chance-constrained admission scheduling of elective surgical patients in a dynamic, uncertain setting',\n",
       "  'Vancroonenburg2019'),\n",
       " (7561,\n",
       "  'T2BS2YNW',\n",
       "  1,\n",
       "  'title',\n",
       "  'Chance constrained problem and its applications',\n",
       "  'Peng'),\n",
       " (7583,\n",
       "  'WU63C4YN',\n",
       "  1,\n",
       "  'title',\n",
       "  'A Bridge Between Bilevel Programs and Nash Games',\n",
       "  'Lampariello2017'),\n",
       " (7586,\n",
       "  'P7YVHZEW',\n",
       "  1,\n",
       "  'title',\n",
       "  'Equilibrium modeling and solution approaches inspired by nonconvex bilevel programming',\n",
       "  'Harwood2023'),\n",
       " (7589,\n",
       "  '4IYUWGHI',\n",
       "  1,\n",
       "  'title',\n",
       "  'Fixed-Time Stable Gradient Flows: Applications to Continuous-Time Optimization',\n",
       "  'Garg2021'),\n",
       " (7592,\n",
       "  'DCD846RR',\n",
       "  1,\n",
       "  'title',\n",
       "  'Fixed-Time Stable Proximal Dynamical System for Solving MVIPs',\n",
       "  'Garg2023'),\n",
       " (7594,\n",
       "  'ULW867T9',\n",
       "  1,\n",
       "  'title',\n",
       "  'Generalized Gradient Flows With Provable Fixed-Time Convergence and Fast Evasion of Non-Degenerate Saddle Points',\n",
       "  'Baranwal2023'),\n",
       " (7598,\n",
       "  'QGKF53E9',\n",
       "  1,\n",
       "  'title',\n",
       "  'Exponential Stability of Primal-Dual Gradient Dynamics with Non-Strong Convexity',\n",
       "  'Chen2020a'),\n",
       " (7600,\n",
       "  'GAIIR9TL',\n",
       "  1,\n",
       "  'title',\n",
       "  'Euclidean, metric, and Wassersteingradient flows: an overview',\n",
       "  'Santambrogio2017'),\n",
       " (7602,\n",
       "  '4YZDPSDI',\n",
       "  1,\n",
       "  'title',\n",
       "  'Fixed-Time Convergence for a Class of Nonconvex-Nonconcave Min-Max Problems',\n",
       "  'Garg2022'),\n",
       " (7604,\n",
       "  'MGA3TLYL',\n",
       "  1,\n",
       "  'title',\n",
       "  'Accelerating Distributed Optimization via Fixed-Time Convergent Flows',\n",
       "  'Garg2023a'),\n",
       " (7606,\n",
       "  '6GVSFAUV',\n",
       "  1,\n",
       "  'title',\n",
       "  'Breaking the Convergence Barrier: Optimization via Fixed-Time Convergent Flows',\n",
       "  'Budhraja2022'),\n",
       " (7609,\n",
       "  'P7LFRG9U',\n",
       "  1,\n",
       "  'title',\n",
       "  'Consistent discretization of finite/fixed-time controllers',\n",
       "  'Polyakov2022'),\n",
       " (7611,\n",
       "  'SVUGDZK5',\n",
       "  1,\n",
       "  'title',\n",
       "  'Consistent Discretization of Finite-Time and Fixed-Time Stable Systems',\n",
       "  'Polyakov2019'),\n",
       " (7613,\n",
       "  'U6NKG6QQ',\n",
       "  1,\n",
       "  'title',\n",
       "  'Nonlinear Feedback Design for Fixed-Time Stabilization of Linear Control Systems',\n",
       "  'Polyakov2012'),\n",
       " (7615, 'XKAKTMFV', 1, 'title', 'Reaching a Consensus', 'Degroot1974'),\n",
       " (7617,\n",
       "  'RCMHIXNS',\n",
       "  1,\n",
       "  'title',\n",
       "  'Lecture Notes: Distributed Optimization & Machine Learning',\n",
       "  'Baranwal2024'),\n",
       " (7677,\n",
       "  'P4GT2D7Q',\n",
       "  1,\n",
       "  'title',\n",
       "  'Omega: Optimistic EMA Gradients',\n",
       "  'Ramirez2024'),\n",
       " (7680,\n",
       "  'DJXYCKFC',\n",
       "  1,\n",
       "  'title',\n",
       "  'Smooth monotone stochastic variational inequalities and saddle point problems: A survey',\n",
       "  'Beznosikov2023'),\n",
       " (7686,\n",
       "  'KLLYDBHK',\n",
       "  1,\n",
       "  'title',\n",
       "  'Stochastic Extragradient: General Analysis and Improved Rates',\n",
       "  'Gorbunov2022'),\n",
       " (7689,\n",
       "  'TQ4QXSCX',\n",
       "  1,\n",
       "  'title',\n",
       "  'On the Exponential Stability of Primal-Dual Gradient Dynamics',\n",
       "  'Qu2019'),\n",
       " (7697,\n",
       "  'WSG92MG3',\n",
       "  1,\n",
       "  'title',\n",
       "  'Distributed Coordination for Nonsmooth Convex Optimization via Saddle-Point Dynamics',\n",
       "  'Cortes2019'),\n",
       " (7699,\n",
       "  '74BMW36R',\n",
       "  1,\n",
       "  'title',\n",
       "  'The proximal augmented Lagrangian method for nonsmooth composite optimization',\n",
       "  'Dhingra2019'),\n",
       " (7702,\n",
       "  '5K3XAJ6L',\n",
       "  1,\n",
       "  'title',\n",
       "  'Asymptotic convergence of constrained primal–dual dynamics',\n",
       "  'Cherukuri2016'),\n",
       " (7704,\n",
       "  'IBETDKI2',\n",
       "  1,\n",
       "  'title',\n",
       "  'Stability of primal–dual gradient dynamics and applications to network optimization',\n",
       "  'Feijer2010'),\n",
       " (7706,\n",
       "  'J9K8KK2I',\n",
       "  1,\n",
       "  'title',\n",
       "  'Dynamical properties of hybrid automata',\n",
       "  'Lygeros2003'),\n",
       " (7707,\n",
       "  'DCJY5JKH',\n",
       "  1,\n",
       "  'title',\n",
       "  'Aug-PDG: Linear Convergence of Convex Optimization with Inequality Constraints',\n",
       "  'Meng'),\n",
       " (7709,\n",
       "  'UQHZW2Y2',\n",
       "  1,\n",
       "  'title',\n",
       "  'Semi-global exponential stability of augmented primal–dual gradient dynamics for constrained convex optimization',\n",
       "  'Tang2020'),\n",
       " (7712,\n",
       "  'LH7T69A3',\n",
       "  1,\n",
       "  'title',\n",
       "  'New applications of duality in convex programming',\n",
       "  'Rockafellar1971'),\n",
       " (7720,\n",
       "  'SQ726MI9',\n",
       "  1,\n",
       "  'title',\n",
       "  'Numerical solution of saddle point problems',\n",
       "  'Benzi2005'),\n",
       " (7723,\n",
       "  'RA3DP3NL',\n",
       "  1,\n",
       "  'title',\n",
       "  'Preconditioning Indefinite Systems in Interior Point Methods for Optimization',\n",
       "  'Bergamaschi2004'),\n",
       " (7726,\n",
       "  '76TH2GMP',\n",
       "  1,\n",
       "  'title',\n",
       "  'Linear Convergence of Gradient and Proximal-Gradient Methods Under the Polyak-Lojasiewicz Condition',\n",
       "  'Karimi2020'),\n",
       " (7736,\n",
       "  'XRZVNQ8U',\n",
       "  1,\n",
       "  'title',\n",
       "  'A new one-point residual-feedback oracle for black-box learning and control',\n",
       "  'Zhang2022'),\n",
       " (7738,\n",
       "  '56PZGRND',\n",
       "  1,\n",
       "  'title',\n",
       "  'Random Gradient-Free Minimization of Convex Functions',\n",
       "  'Nesterov2017'),\n",
       " (7742,\n",
       "  'LXBUSJ2D',\n",
       "  1,\n",
       "  'title',\n",
       "  'Model-Free Nonlinear Feedback Optimization',\n",
       "  'He2023'),\n",
       " (7748,\n",
       "  'ZWLANXSB',\n",
       "  1,\n",
       "  'title',\n",
       "  'On Accelerating Distributed Convex Optimizations',\n",
       "  'Chakrabarti2021'),\n",
       " (7750,\n",
       "  'FPQPQFTU',\n",
       "  1,\n",
       "  'title',\n",
       "  'Iterative Pre-Conditioning to Expedite the Gradient-Descent Method',\n",
       "  'Chakrabarti2020'),\n",
       " (7753,\n",
       "  'TR46MDJT',\n",
       "  1,\n",
       "  'title',\n",
       "  'Iterative Pre-Conditioning for Expediting the Gradient-Descent Method: The Distributed Linear Least-Squares Problem',\n",
       "  'Chakrabarti2021a'),\n",
       " (7756,\n",
       "  'VXLLIZZD',\n",
       "  1,\n",
       "  'title',\n",
       "  'On Convergence of the Iteratively Preconditioned Gradient-Descent (IPG) Observer',\n",
       "  'Chakrabarti2024'),\n",
       " (7759,\n",
       "  'KCAMICM6',\n",
       "  1,\n",
       "  'title',\n",
       "  'Fast Exact Multiplication by the Hessian',\n",
       "  'Pearlmutter1994'),\n",
       " (7761,\n",
       "  'Y236DCLL',\n",
       "  1,\n",
       "  'title',\n",
       "  'Luenberger observers for non autonomous nonlinear systems',\n",
       "  'Bernard2018'),\n",
       " (7763,\n",
       "  'V2YWWR4I',\n",
       "  1,\n",
       "  'title',\n",
       "  'A new framework for constrained optimization via feedback control of Lagrange multipliers',\n",
       "  'Cerone2024'),\n",
       " (7766,\n",
       "  'XJ3JSDG6',\n",
       "  1,\n",
       "  'title',\n",
       "  'Analysis and Design of Optimization Algorithms via Integral Quadratic Constraints',\n",
       "  'Lessard2016'),\n",
       " (7768,\n",
       "  'A2RABNVG',\n",
       "  1,\n",
       "  'title',\n",
       "  'Control interpretations for first-order optimization methods',\n",
       "  'Hu2017'),\n",
       " (7770,\n",
       "  'FF4VZCZB',\n",
       "  1,\n",
       "  'title',\n",
       "  'Risk-sensitive control, single controller games and linear programming',\n",
       "  'Borkar2024'),\n",
       " (7773,\n",
       "  '3JG2TTU3',\n",
       "  1,\n",
       "  'title',\n",
       "  'On PI Controllers for Updating Lagrange Multipliers in Constrained Optimization',\n",
       "  'Sohrabi2024'),\n",
       " (7776,\n",
       "  'N2QWIIRS',\n",
       "  1,\n",
       "  'title',\n",
       "  'Responsive Safety in Reinforcement Learning by PID Lagrangian Methods',\n",
       "  'Stooke2020'),\n",
       " (7783,\n",
       "  '3SKSEPUU',\n",
       "  1,\n",
       "  'title',\n",
       "  'A Control Theoretical Approach to Online Constrained Optimization',\n",
       "  'Casti2024'),\n",
       " (7785,\n",
       "  '9V8V8MKM',\n",
       "  1,\n",
       "  'title',\n",
       "  'The Best Things in Life Are Model Free',\n",
       "  'Recht'),\n",
       " (7787,\n",
       "  'BAXV54PN',\n",
       "  1,\n",
       "  'title',\n",
       "  'Constrained Differential Optimization',\n",
       "  'Platt1987'),\n",
       " (7793,\n",
       "  'LVAI3AJ6',\n",
       "  1,\n",
       "  'title',\n",
       "  'Introduction to the calculus of variations and Γ-convergence',\n",
       "  'Chambolle2015'),\n",
       " (7795, 'HDYUPIKM', 1, 'title', 'Introduction to Γ-convergence', 'Muthukumar'),\n",
       " (7797,\n",
       "  'AYX2NTTI',\n",
       "  1,\n",
       "  'title',\n",
       "  'Introduction to Homogenization and Gamma-convergence',\n",
       "  'Braides1993'),\n",
       " (7799,\n",
       "  'BQLRGK8U',\n",
       "  1,\n",
       "  'title',\n",
       "  'Lecture notes on viscosity solutions',\n",
       "  'Calder'),\n",
       " (7801,\n",
       "  'B47L87RA',\n",
       "  1,\n",
       "  'title',\n",
       "  'Viscosity Solutions of Minimization Problems',\n",
       "  'Attouch1996'),\n",
       " (7811,\n",
       "  'DBSLNPCI',\n",
       "  1,\n",
       "  'title',\n",
       "  'Viscosity solutions and optimal control problems',\n",
       "  'Nguyen'),\n",
       " (7813, 'D582IJA7', 1, 'title', 'Network Embedding: An Overview', 'Arsov2019'),\n",
       " (7816, 'UDXME7M7', 1, 'title', 'Stochastic Neighbor Embedding', 'Hinton2002'),\n",
       " (7818,\n",
       "  '4BFFT2QW',\n",
       "  1,\n",
       "  'title',\n",
       "  'Learning a Parametric Embedding by Preserving Local Structure',\n",
       "  'Maaten2009'),\n",
       " (7820,\n",
       "  '5CWI3RNN',\n",
       "  1,\n",
       "  'title',\n",
       "  'Mixed Integer Nonlinear Optimization (GIAN)',\n",
       "  'SvenLeyffer2024'),\n",
       " (7822,\n",
       "  'DZDT7HTU',\n",
       "  1,\n",
       "  'title',\n",
       "  '6.896: Topics in Algorithmic Game Theory',\n",
       "  'Daskalakis'),\n",
       " (7825,\n",
       "  '4Q4F4I5D',\n",
       "  1,\n",
       "  'title',\n",
       "  'Lecture 11: The Lemke-Howson Algorithm',\n",
       "  'Codenotti2011'),\n",
       " (7834,\n",
       "  'T5QRULZF',\n",
       "  1,\n",
       "  'title',\n",
       "  'How to solve a design centering problem',\n",
       "  'Harwood2017'),\n",
       " (7837,\n",
       "  '9778BF5F',\n",
       "  1,\n",
       "  'title',\n",
       "  'Generalized Kuhn-Tucker conditions for mathematical programming problems in a Banach space',\n",
       "  'Guignard1969'),\n",
       " (7840,\n",
       "  'ZINNGGYJ',\n",
       "  1,\n",
       "  'title',\n",
       "  'Mathematical Programs with Vanishing Constraints: Optimality Conditions, Sensitivity, and a Relaxation Method',\n",
       "  'Izmailov2009'),\n",
       " (7862,\n",
       "  'A9E84PHB',\n",
       "  1,\n",
       "  'title',\n",
       "  'Mathematical programs with vanishing constraints: critical point theory',\n",
       "  'Dorsch2012'),\n",
       " (7864,\n",
       "  '7H27TX7X',\n",
       "  1,\n",
       "  'title',\n",
       "  'Stationary conditions for mathematical programs with vanishing constraints using weak constraint qualifications',\n",
       "  'Hoheisel2008'),\n",
       " (7866,\n",
       "  '5LGH473B',\n",
       "  1,\n",
       "  'title',\n",
       "  'On the Abadie and Guignard constraint qualifications for Mathematical Programmes with Vanishing Constraints',\n",
       "  'Hoheisel2009'),\n",
       " (7876,\n",
       "  'ZXQF4GQP',\n",
       "  1,\n",
       "  'title',\n",
       "  'Mathematical programs with vanishing constraints: constraint qualifications, their applications, and a new regularization method',\n",
       "  'Dussault2019'),\n",
       " (7886,\n",
       "  '8LDBL5VL',\n",
       "  1,\n",
       "  'title',\n",
       "  'Mathematical Programs with Vanishing Constraints',\n",
       "  'Hoheisel2009a'),\n",
       " (7910,\n",
       "  'C3XLGBTZ',\n",
       "  1,\n",
       "  'title',\n",
       "  'Mathematical programs with vanishing constraints: optimality conditions and constraint qualifications',\n",
       "  'Achtziger2008'),\n",
       " (7944,\n",
       "  'ANXWV3IN',\n",
       "  1,\n",
       "  'title',\n",
       "  'Nonlinear complementarity as unconstrained and constrained minimization',\n",
       "  'Mangasarian1993'),\n",
       " (7946,\n",
       "  'VZ7BRXYN',\n",
       "  1,\n",
       "  'title',\n",
       "  'A projected–gradient interior–point algorithm for complementarity problems',\n",
       "  'Andreani2011'),\n",
       " (7948,\n",
       "  'MNBZI49C',\n",
       "  1,\n",
       "  'title',\n",
       "  'Global Convergence of Augmented Lagrangian Methods Applied to Optimization Problems with Degenerate Constraints, Including Problems with Complementarity Constraints',\n",
       "  'Izmailov2012'),\n",
       " (7950,\n",
       "  'XZGSSPUZ',\n",
       "  1,\n",
       "  'title',\n",
       "  'A New Augmented Lagrangian Method for MPCCs—Theoretical and Numerical Comparison with Existing Augmented Lagrangian Methods',\n",
       "  'Guo2022'),\n",
       " (7952,\n",
       "  'L6UJV5FP',\n",
       "  1,\n",
       "  'title',\n",
       "  'Convergence Properties of Modified and Partially-Augmented Lagrangian Methods for Mathematical Programs with Complementarity Constraints',\n",
       "  'Luo2010'),\n",
       " (7954,\n",
       "  '48ZT7QX6',\n",
       "  1,\n",
       "  'title',\n",
       "  'Convergence Properties of a Second Order Augmented Lagrangian Method for Mathematical Programs with Complementarity Constraints',\n",
       "  'Andreani2018'),\n",
       " (7958,\n",
       "  'X9ZJMGIT',\n",
       "  1,\n",
       "  'title',\n",
       "  'Convergence of a local regularization approach for mathematical programmes with complementarity or vanishing constraints',\n",
       "  'Hoheisel2012'),\n",
       " (7965,\n",
       "  '83GLC3UG',\n",
       "  1,\n",
       "  'title',\n",
       "  'A New Relaxation Scheme for Mathematical Programs with Equilibrium Constraints',\n",
       "  'Steffensen2010'),\n",
       " (7968,\n",
       "  '4QIX7V6J',\n",
       "  1,\n",
       "  'title',\n",
       "  'Convergence Properties of a Regularization Scheme for Mathematical Programs with Complementarity Constraints',\n",
       "  'Scholtes2001'),\n",
       " (7983,\n",
       "  'TL9T9MBD',\n",
       "  1,\n",
       "  'title',\n",
       "  'Convergence of a Smoothing Continuation Method for Mathematical Progams with Complementarity Constraints',\n",
       "  'Fukushima1999'),\n",
       " (7991,\n",
       "  '74JC6NWT',\n",
       "  1,\n",
       "  'title',\n",
       "  'Nonlinear optimization: characterization of structural stability',\n",
       "  'Jongen1991'),\n",
       " (7999,\n",
       "  'FS9BMUIT',\n",
       "  1,\n",
       "  'title',\n",
       "  'Stability Theory for Systems of Inequalities. Part I: Linear Systems',\n",
       "  'Robinson1975'),\n",
       " (8006,\n",
       "  'JA893DVY',\n",
       "  1,\n",
       "  'title',\n",
       "  'Stability Theory for Systems of Inequalities, Part II: Differentiable Nonlinear Systems',\n",
       "  'Robinson1976'),\n",
       " (8016,\n",
       "  'FNYZEJYX',\n",
       "  1,\n",
       "  'title',\n",
       "  'A class of smoothing functions for nonlinear and mixed complementarity problems',\n",
       "  'Chen1996'),\n",
       " (8022,\n",
       "  'U7CQVDD6',\n",
       "  1,\n",
       "  'title',\n",
       "  'Mathematical Programs with Complementarity Constraints: Stationarity, Optimality, and Sensitivity',\n",
       "  'Scheel2000'),\n",
       " (8034,\n",
       "  'IVMUTJ2K',\n",
       "  1,\n",
       "  'title',\n",
       "  'Nonconvex Structures in Nonlinear Programming',\n",
       "  'Scholtes2004'),\n",
       " (8044,\n",
       "  'LWG9WJYQ',\n",
       "  1,\n",
       "  'title',\n",
       "  'Nonsmooth Approach to Optimization Problems with Equilibrium Constraints',\n",
       "  'Outrata1998'),\n",
       " (8048,\n",
       "  '28UMGK7G',\n",
       "  1,\n",
       "  'title',\n",
       "  'Solving mathematical programs with complementarity constraints as nonlinear programs',\n",
       "  'Fletcher2004'),\n",
       " (8064,\n",
       "  'HE2JLYX2',\n",
       "  1,\n",
       "  'title',\n",
       "  'A Lyapunov analysis of accelerated methods in optimization',\n",
       "  'Wilson2021'),\n",
       " (540,\n",
       "  'CQ7HWCU7',\n",
       "  1,\n",
       "  'title',\n",
       "  'Matrix Algorithms : Basic Decompositions',\n",
       "  'Stewart1999'),\n",
       " (542,\n",
       "  'R25M9S8R',\n",
       "  1,\n",
       "  'title',\n",
       "  'Matrix Algorithms : Eigensystems',\n",
       "  'Stewart1998'),\n",
       " (8069, '4XY63AV6', 1, 'title', '7. On Symmetric Games', 'Gale1951'),\n",
       " (8071,\n",
       "  'DLDZGVE5',\n",
       "  1,\n",
       "  'title',\n",
       "  'First-order penalty methods for bilevel optimization',\n",
       "  'Lu2024'),\n",
       " (8074,\n",
       "  'SDIX8WX2',\n",
       "  1,\n",
       "  'title',\n",
       "  'Principled Penalty-based Methods for Bilevel Reinforcement Learning and RLHF',\n",
       "  'Shen2024'),\n",
       " (8077,\n",
       "  '6RNZQYKF',\n",
       "  1,\n",
       "  'title',\n",
       "  'On Penalty-based Bilevel Gradient Descent Method',\n",
       "  'Shen2023a'),\n",
       " (949,\n",
       "  '4KFX7LWU',\n",
       "  1,\n",
       "  'title',\n",
       "  'Optimality conditions for bilevel programming problem',\n",
       "  'Bard1984'),\n",
       " (8079,\n",
       "  'QB3EMVYW',\n",
       "  1,\n",
       "  'title',\n",
       "  'Linear Convergence of the Primal-Dual Gradient Method for Convex-Concave Saddle Point Problems without Strong Convexity',\n",
       "  'Du2019'),\n",
       " (8085,\n",
       "  'LBHMTUID',\n",
       "  1,\n",
       "  'title',\n",
       "  'A Brief Introduction to Mixed-Integer Nonlinear Programming (MINLP)',\n",
       "  'Camponogara2016'),\n",
       " (8087,\n",
       "  '3JMLJXSI',\n",
       "  1,\n",
       "  'title',\n",
       "  'Identifying and attacking the saddle point problem in high-dimensional non-convex optimization',\n",
       "  'Dauphin2014'),\n",
       " (8091, '7DSXUVHI', 1, 'title', 'Exact Penalty Methods', 'Pillo1994'),\n",
       " (8051,\n",
       "  '5ZVK5NKM',\n",
       "  1,\n",
       "  'title',\n",
       "  'lecture notes Convex Optimization',\n",
       "  'RyanTibshirani2019'),\n",
       " (8083, 'WJP8WVDN', 1, 'title', 'Discrete Optimization', 'Rothvoss2020'),\n",
       " (8130,\n",
       "  'HALFM5Q3',\n",
       "  1,\n",
       "  'title',\n",
       "  'Lecture 2: Positivity and the Perron–Frobenius Theorem',\n",
       "  'Magal2022'),\n",
       " (8132,\n",
       "  'LRIPR8GQ',\n",
       "  1,\n",
       "  'title',\n",
       "  'Lecture 4: Semiﬂows, ω-limit Sets, α-limit Sets, Attraction, and Dissipation',\n",
       "  'Magal2022a'),\n",
       " (8134, '6EZ984XQ', 1, 'title', 'Lecture 3: Monotone Semiﬂows', 'Magal2022b'),\n",
       " (8159,\n",
       "  'RQK3L7C9',\n",
       "  1,\n",
       "  'title',\n",
       "  'Provably Effective Algorithms for Min-Max Optimization',\n",
       "  'Lei'),\n",
       " (8170,\n",
       "  '5ECNRFJ4',\n",
       "  1,\n",
       "  'title',\n",
       "  'Fixed Point Theorems and Applications',\n",
       "  'Pata2019'),\n",
       " (8173,\n",
       "  'RBGAFZ8Z',\n",
       "  1,\n",
       "  'title',\n",
       "  'Deep Learning and Computational Physics (Lecture Notes)',\n",
       "  'Ray2023'),\n",
       " (8165,\n",
       "  'X2CC7EL5',\n",
       "  1,\n",
       "  'title',\n",
       "  'Robbins Monroe and More (Lecture Notes)',\n",
       "  'Bach2016'),\n",
       " (8175,\n",
       "  'E3HHTNYJ',\n",
       "  1,\n",
       "  'title',\n",
       "  'More Is Different: Broken symmetry and the nature of the hierarchical structure of science.',\n",
       "  'Anderson1972'),\n",
       " (8185,\n",
       "  'N7PJYTMR',\n",
       "  1,\n",
       "  'title',\n",
       "  'The Isotonic Regression Problem and Its Dual',\n",
       "  'Barlow1972'),\n",
       " (8188,\n",
       "  'SFR4JTHQ',\n",
       "  1,\n",
       "  'title',\n",
       "  'Nearly-Isotonic Regression',\n",
       "  'Tibshirani2011'),\n",
       " (8189,\n",
       "  'DRKIX57Q',\n",
       "  1,\n",
       "  'title',\n",
       "  'The First Optimal Algorithm for Smooth and Strongly-Convex-Strongly-Concave Minimax Optimization',\n",
       "  'Kovalev2022'),\n",
       " (8199,\n",
       "  '78JBZTIU',\n",
       "  1,\n",
       "  'title',\n",
       "  'Alternating Minimizations Converge to Second-Order Optimal Solutions',\n",
       "  'Li2019'),\n",
       " (8201,\n",
       "  'JFRTDCNU',\n",
       "  1,\n",
       "  'title',\n",
       "  'Alternating Minimization Methods for Strongly Convex Optimization',\n",
       "  'Tupitsa2020'),\n",
       " (8205,\n",
       "  'TDY57UIG',\n",
       "  1,\n",
       "  'title',\n",
       "  'Isotonic regression and isotonic projection',\n",
       "  'Nemeth2015'),\n",
       " (8209,\n",
       "  'C8GVBIK8',\n",
       "  1,\n",
       "  'title',\n",
       "  'Isotonic regression – Joseph Salmon',\n",
       "  'Salmon2024'),\n",
       " (8210, 'UDJBKUTD', 1, 'title', 'Isotonic Regression', 'Pedregosa2013'),\n",
       " (8214,\n",
       "  '8IQK94FZ',\n",
       "  1,\n",
       "  'title',\n",
       "  'Efficient Algorithms for General Isotone Optimization',\n",
       "  'Wang2022'),\n",
       " (8215,\n",
       "  'AE56J47N',\n",
       "  1,\n",
       "  'title',\n",
       "  'Contraction and uniform convergence of isotonic regression',\n",
       "  'Yang2019a'),\n",
       " (8217,\n",
       "  'T4EM8DU9',\n",
       "  1,\n",
       "  'title',\n",
       "  'Estimation of isotonic regression',\n",
       "  'Brunk1970'),\n",
       " (8219, '5JU6B2BV', 1, 'title', 'A successive projection method', 'Han1988'),\n",
       " (8225,\n",
       "  'H68YWB9L',\n",
       "  1,\n",
       "  'title',\n",
       "  'On the Convergence of Alternating Minimization for Convex Programming with Applications to Iteratively Reweighted Least Squares and Decomposition Schemes',\n",
       "  'Beck2015'),\n",
       " (8230,\n",
       "  '488DV58T',\n",
       "  1,\n",
       "  'title',\n",
       "  'Gradient-based algorithms with applications to signal-recovery problems',\n",
       "  'Beck2009b'),\n",
       " (8239, 'SH2J5LD2', 1, 'title', 'Semiparametric regression', 'Ruppert2009'),\n",
       " (8240,\n",
       "  '5TEQWAX4',\n",
       "  1,\n",
       "  'title',\n",
       "  'Semiparametric isotonic regression modelling and estimation for group testing data',\n",
       "  'Yuan2021'),\n",
       " (8244,\n",
       "  'CQIK4QVI',\n",
       "  1,\n",
       "  'title',\n",
       "  'Weight-Sharing Regularization',\n",
       "  'Shakerinava2024'),\n",
       " (8257,\n",
       "  '57RVFNFA',\n",
       "  1,\n",
       "  'title',\n",
       "  'Not Too Close and Not Too Far: Enforcing Monotonicity Requires Penalizing The Right Points',\n",
       "  'Monteiro2021'),\n",
       " (8260,\n",
       "  'D8FEABCQ',\n",
       "  1,\n",
       "  'title',\n",
       "  'Certified Monotonic Neural Networks',\n",
       "  'Liu2022b'),\n",
       " (8261,\n",
       "  '2RCZYDAS',\n",
       "  1,\n",
       "  'title',\n",
       "  'How to Incorporate Monotonicity in Deep Networks While Preserving Flexibility?',\n",
       "  'Gupta2019'),\n",
       " (8269, 'QSJL3LKV', 1, 'title', 'Coordinate Descent Algorithms', 'Wright2015'),\n",
       " (8272,\n",
       "  'E8IHVEV2',\n",
       "  1,\n",
       "  'title',\n",
       "  'An Empirical Distribution Function for Sampling with Incomplete Information',\n",
       "  'Ayer1955'),\n",
       " (8274,\n",
       "  'ME4C8ZU9',\n",
       "  1,\n",
       "  'title',\n",
       "  'Active set algorithms for isotonic regression; A unifying framework',\n",
       "  'Best1990'),\n",
       " (2,\n",
       "  'KHW2498F',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'The profiles of persistent slip markings produced by uniaxial and biaxial cyclic straining in four different polycrystalline materials with f.c.c. structure were investigated using focused ion beam (FIB) cutting and TEM observation of oriented surface foils. Typical shapes of persistent slip markings are extrusions accompanied by parallel intrusions. In some cases only extrusions were developed and intrusions were produced later in fatigue life. In polycrystalline copper extrusions and intrusions appear on the surface of the grain where persistent slip band characterized by ladder-like dislocation structure egress on the surface. Similar features were observed in fatigued austenitic 316L and Sanicro 25 steels but the extrusion and intrusion shapes were more complicated. Crack-like intrusion shapes produce high stress and strain concentration and primary stage I crack starts to grow from the tip of intrusions. The experimental observations were compared with the predictions of the existing crack initiation models.',\n",
       "  'Polak2017'),\n",
       " (4,\n",
       "  'QZ4IPAQV',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Results of the study of the early fatigue damage in a number of model and structural crystalline materials using modern experimental techniques are presented. The dislocation structure of the persistent slip bands and the evolution of the surface relief resulting in the formation of persistent slip markings during cyclic loading are documented. The dislocation mechanisms leading to production of point defects in cyclic loading are described and point defect production and annihilation rates are derived. The kinetics of point defect migration is characterized. The physically based models of the surface relief formation describing the formation of extrusions and intrusions are presented. The models are confronted with experimental evidence. It is concluded that intrusions representing sharp surface crack-like defects play the principal role in the initiation of fatigue cracks.',\n",
       "  'Polak2016'),\n",
       " (6,\n",
       "  '6DGRHLYU',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'The fatigue life of a component can be expressed as the sum of two segments of life: (a) the number of loading cycles required to initiate a crack and (b) the number of cycles it takes that crack to propagate to failure. In this review, the primary emphasis is relating the fatigue crack initiation to the microstructure of the material. Many studies have focused on this phenomenon over the years and the goal of this paper is to put this work in perspective and encourage future work of fatigue in polycrystals based on the material’s microstructure. In order to address fatigue, it is necessary to understand the mechanisms that facilitate crack initiation. Slip irreversibilities exist in a material and accumulate during fatigue loading. At the defect level, irreversibilities are a result of dislocations: annihilating, cross-slipping, penetrating precipitates, transmitting through grain boundaries, and piling-up. These slip irreversibilities are the early signs of damage during cyclic loading. The dislocations subsequently form low-energy, stable structures as a means to accommodate the irreversible slip processes and increasing dislocation density during cyclic forward and reverse loading. The result is strain localizing in a small region within the materials, i.e. persistent slip bands and dislocation cells/bundles. Strain localization is a precursor to crack initiation. This review paper will focus on experimental observations of strain localization and the theory and numerical analysis of both slip irreversibilities and low energy configuration defect structures. This fundamental understanding is necessary to study persistent slip bands in FCC metals and alloys including the appropriate characterization, theory, and modeling. From this fundamental knowledge both micromechanical and crystal plasticity models can be used to predict crack initiation, which are also reviewed. Finally, this review ends with a discussion of the future of fatigue modeling and experiments.',\n",
       "  'Sangid2013'),\n",
       " (8,\n",
       "  'YGGUDQCL',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Analysis of inhomogeneous cyclic straining, observation of surface relief at emerging persistent slip bands (PSBs) and data on point defect generation and migration in fatigued metal single cyrstals and polycrystals are the origin of a new model for fatigue crack initiation in PSBs. Because of the high plastic strain amplitude, non-equilibrium point defects are generated in PSBs. They are distributed inhomogeneously according to the inhomogeneous sink distribution. Point defect migration over short distances causes mass transport and results in the formation of extrusions and intrusions on the metal surface. The distribution of extrusions and intrusions is related to the dislocation substructure of the PSB. Cyclic straining under stress together with the strain concentration at sharp intrusions result in environment-assisted formation of new surfaces and therefore in crack initiation.',\n",
       "  'Polak1987'),\n",
       " (10,\n",
       "  '4MFHUNBV',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'The role of point defects in the formation of surface relief and in the initiation of a fatigue crack in crystalline materials is analyzed. The dislocation interactions in the bands of intensive cyclic slip (persistent slip bands - PSBs) are specified and relations describing the formation and annihilation of interstitial and vacancy type defects in the channels of the ladder-like PSB are derived. The continuous formation, annihilation and primarily the migration of point defects are proposed to be responsible for the mass redistribution within PSB and between PSB and the PSB/matrix boundary. The redistribution of the matter results in local tensile and compressive stresses that are the sources of the principal irreversibility of slip within PSB. Local tensile and compressive stresses are relaxed by dislocation movement within PSB in the direction of the active Burgers vector and lead to the formation of characteristic surface relief in the form of extrusions and intrusions. The intrusions represent crack-like defects and fatigue cracks initiate in the tip of intrusions. © 2013 Elsevier Ltd. All rights reserved.',\n",
       "  'Polak2014'),\n",
       " (12,\n",
       "  'RK9FV68J',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Change points are abrupt variations in time series data. Such abrupt changes may represent transitions that occur between states. Detection of change points is useful in modelling and prediction of time series and is found in application areas such as medical condition monitoring, climate change detection, speech and image analysis, and human activity analysis. This survey article enumerates, categorizes, and compares many of the methods that have been proposed to detect change points in time series. The methods examined include both supervised and unsupervised algorithms that have been introduced and evaluated. We introduce several criteria to compare the algorithms. Finally, we present some grand challenges for the community to consider.',\n",
       "  'Aminikhanghahi2017'),\n",
       " (16,\n",
       "  'XD2QDF6R',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'This article presents a selective survey of algorithms for the offline detection of multiple change points in multivariate time series. A general yet structuring methodological strategy is adopted to organize this vast body of work. More precisely, detection algorithms considered in this review are characterized by three elements: a cost function, a search method and a constraint on the number of changes. Each of those elements is described, reviewed and discussed separately. Implementations of the main algorithms described in this article are provided within a Python package called ruptures.',\n",
       "  'Truong2018'),\n",
       " (18,\n",
       "  'WQ2CIXZK',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  \"This paper reviews the problems and successes of computing turbulent flow. Most of the flow phenomena that are important to modern technology involve turbulence. The review is concerned with methods for turbulent flow computer predictions and their applications, and describes several of them. These computational methods are aimed at simulating either as much detail of the turbulent motion as possible by current computer power or, more commonly, its overall effect on the mean-flow behaviour. The methods are still being developed and some of the most recent concepts involved are discussed. Some success has been achieved with two-equation models for relatively simple hydrodynamic phenomena; indeed, routine design work has been undertaken during the last three decades in several applications of engineering practise, for which extensive studies have optimised these models. Failures are still common for many applications particularly those that involve strong curvature, intermittency, strong buoyancy influences, low-Reynolds-number effects, rapid compression or expansion, strong swirl, and kinetically-influenced chemical reaction. New conceptual developments are needed in these areas, probably along the lines of actually calculating the principal manifestation of turbulence, e.g. intermittency. A start has been made in this direction in the form of 'multi-fluid' models, and full simulations. The turbulence modelling approaches presented here are, Reynolds-Averaged Navier-Stokes (RANS), two-fluid models, Very Large Eddy Simulation (VLES), Unsteady Reynolds-Averaged Navier-Stokes (URANS), Detached Eddy Simulation (DES) and some interesting, relatively recent, hybrid LES/RANS techniques. A large number of relatively recent studies are considered, together with reference to the numerical experiments existing on the subject. The authors hope that they provide the interested reader with most of the appropriate sources of turbulence modelling, exhibiting either as much detail as it is possible, by means of bibliography, or illustrating some of the most recent developments on the numerical modelling of turbulent flows. Thus, the potential user has the appropriate information, for him to select the suitable turbulence model for his own case of interest.\",\n",
       "  'Argyropoulos2015'),\n",
       " (20,\n",
       "  '84HAVXWN',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'A research paper includes several sections, each section having a particular purpose and containing a particular kind of information. This paper is a guide to reading a research paper. It describes the prototypical research paper and explains the purpose for each section. Issues for the astute reader to note are indicated and illustrated with examples from a research paper published in this issue of the journal.',\n",
       "  'Mitzenmacher1988'),\n",
       " (22,\n",
       "  'NYN9JAFT',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'We develop a non-singular, self-consistent framework for computing the stress field and the total elastic energy of a general dislocation microstructure. The expressions are self-consistent in that the driving force defined as the negative derivative of the total energy with respect to the dislocation position, is equal to the force produced by stress, through the Peach-Koehler formula. The singularity intrinsic to the classical continuum theory is removed here by spreading the Burgers vector isotropically about every point on the dislocation line using a spreading function characterized by a single parameter a, the spreading radius. A particular form of the spreading function chosen here leads to simple analytic formulations for stress produced by straight dislocation segments, segment self and interaction energies, and forces on the segments. For any value a>0, the total energy and the stress remain finite everywhere, including on the dislocation lines themselves. Furthermore, the well-known singular expressions are recovered for a=0. The value of the spreading radius a can be selected for numerical convenience, to reduce the stiffness of the dislocation equations of motion. Alternatively, a can be chosen to match the atomistic and continuum energies of dislocation configurations. © 2005 Elsevier Ltd. All rights reserved.',\n",
       "  'Cai2006'),\n",
       " (24,\n",
       "  'XGN74PFB',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'La profesión –formación- docente es un tema crucial en los actuales debates educativos. La existencia de dos decretos y el desplazamiento del verdadero sentido del ser maestro reclaman de los análisis un ejercicio de comprensión del orden discursivo oficial. La calidad es el sustrato de la sociedad de control. En este marco se agencia nuevas prácticas de subjetivación del maestro los cuales podríamos situar en la calidad, flexibilidad, adaptabilidad, eficiencia, eficacia. En cualquier caso, el esfuerzo por hacer del maestro un intelectual de la educación fue borrado. La gran cuestión consiste en saber que discursos regula el saber del docente a la luz de la sociedad de control.',\n",
       "  'PeterWriggers2012'),\n",
       " (26,\n",
       "  'JRFNKVVT',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'CiteSeerX - Scientific documents that cite the following paper: Reinforcement learning: An introduction, chapter 11',\n",
       "  'Andrew1998'),\n",
       " (30,\n",
       "  'LYZ2SXKL',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'In this paper we establish a mathematical framework for a range of inverse problems for functions, given a finite set of noisy observations. The problems are hence underdetermined and are often ill-posed. We study these problems from the viewpoint of Bayesian statistics, with the resulting posterior probability measure being defined on a space of functions. We develop an abstract framework for such problems which facilitates application of an infinite-dimensional version of Bayes theorem, leads to a well-posedness result for the posterior measure (continuity in a suitable probability metric with respect to changes in data), and also leads to a theory for the existence of maximizing the posterior probability (MAP) estimators for such Bayesian inverse problems on function space. A central idea underlying these results is that continuity properties and bounds on the forward model guide the choice of the prior measure for the inverse problem, leading to the desired results on well-posedness and MAP estimators; the PDE analysis and probability theory required are thus clearly dileneated, allowing a straightforward derivation of results. We show that the abstract theory applies to some concrete applications of interest by studying problems arising from data assimilation in fluid mechanics. The objective is to make inference about the underlying velocity field, on the basis of either Eulerian or Lagrangian observations. We study problems without model error, in which case the inference is on the initial condition, and problems with model error in which case the inference is on the initial condition and on the driving noise process or, equivalently, on the entire time-dependent velocity field. In order to undertake a relatively uncluttered mathematical analysis we consider the two-dimensional Navier-Stokes equation on a torus. The case of Eulerian observations - direct observations of the velocity field itself - is then a model for weather forecasting. The case of Lagrangian observations - observations of passive tracers advected by the flow - is then a model for data arising in oceanography. The methodology which we describe herein may be applied to many other inverse problems in which it is of interest to find, given observations, an infinite-dimensional object, such as the initial condition for a PDE. A similar approach might be adopted, for example, to determine an appropriate mathematical setting for the inverse problem of determining an unknown tensor arising in a constitutive law for a PDE, given observations of the solution. The paper is structured so that the abstract theory can be read independently of the particular problems in fluid mechanics which are subsequently studied by application of the theory. © 2009 IOP Publishing Ltd.',\n",
       "  'Cotter2009'),\n",
       " (31,\n",
       "  'T57E9LWP',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'The local size of computational grids used in partial differential equation (PDE)-based probabilistic inverse problems can have a tremendous impact on the numerical results. As a consequence, numerical model identification procedures used in structural or material engineering may yield erroneous, mesh-dependent result. In this work, we attempt to connect the field of adaptive methods for deterministic and forward probabilistic finite-element (FE) simulations and the field of FE-based Bayesian inference. In particular, our target setting is that of exact inference, whereby complex posterior distributions are to be sampled using advanced Markov Chain Monte Carlo (MCMC) algorithms. Our proposal is for the mesh refinement to be performed in a goal-oriented manner. We assume that we are interested in a finite subset of quantities of interest (QoI) such as a combination of latent uncertain parameters and/or quantities to be drawn from the posterior predictive distribution. Next, we evaluate the quality of an approximate inversion with respect to these quantities. This is done by running two chains in parallel: (i) the approximate chain and (ii) an enhanced chain whereby the approximate likelihood function is corrected using an efficient deterministic error estimate of the error introduced by the spatial discretisation of the PDE of interest. One particularly interesting feature of the proposed approach is that no user-defined tolerance is required for the quality of the QoIs, as opposed to the deterministic error estimation setting. This is because our trust in the model, and therefore a good measure for our requirement in terms of accuracy, is fully encoded in the prior. We merely need to ensure that the finite element approximation does not impact the posterior distributions of QoIs by a prohibitively large amount. We will also propose a technique to control the error introduced by the MCMC sampler, and demonstrate the validity of the combined mesh and algorithmic quality control strategy.',\n",
       "  'Kerfriden2019'),\n",
       " (32,\n",
       "  '37AE3RYY',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Inverse problems are often ill posed, with solutions that depend sensitively on data.n any numerical approach to the solution of such problems, regularization of some form is needed to counteract the resulting instability. This paper is based on an approach to regularization, employing a Bayesian formulation of the problem, which leads to a notion of well posedness for inverse problems, at the level of probability measures. The stability which results from this well posedness may be used as the basis for quantifying the approximation, in finite dimensional spaces, of inverse problems for functions. This paper contains a theory which utilizes this stability property to estimate the distance between the true and approximate posterior distributions, in the Hellinger metric, in terms of error estimates for approximation of the underlying forward problem. This is potentially useful as it allows for the transfer of estimates from the numerical analysis of forward problems into estimates for the solution of the related inverse problem. It is noteworthy that, when the prior is a Gaussian random field model, controlling differences in the Hellinger metric leads to control on the differences between expected values of polynomially bounded functions and operators, including the mean and covariance operator. The ideas are applied to some non-Gaussian inverse problems where the goal is determination of the initial condition for the Stokes or Navier-Stokes equation from Lagrangian and Eulerian observations, respectively. © 2010 Society for Industrial and Applied Mathematics.',\n",
       "  'Cotter2010'),\n",
       " (33,\n",
       "  'Z3P3I79V',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'The subject of inverse problems in differential equations is of enormous practical importance, and has also generated substantial mathematical and computational innovation. Typically some form of regularization is required to ameliorate ill-posed behaviour. In this article we review the Bayesian approach to regularization, developing a function space viewpoint on the subject. This approach allows for a full characterization of all possible solutions, and their relative probabilities, whilst simultaneously forcing significant modelling issues to be addressed in a clear and precise fashion. Although expensive to implement, this approach is starting to lie within the range of the available computational resources in many application areas. It also allows for the quantification of uncertainty and risk, something which is increasingly demanded by these applications. Furthermore, the approach is conceptually important for the understanding of simpler, computationally expedient approaches to inverse problems. © 2010 Cambridge University Press.',\n",
       "  'Stuart2010'),\n",
       " (41,\n",
       "  '8T89CPZT',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'The motion of a drilling structure is studied in torsion. The stability of the stationary solution is determined by the direct method of Liapounov, supplemented with results for the linearized method. The stability criterion is firmly based on the form of the boundary condition linked to the rock destruction process. This rock/bit interaction function can be deduced using studies on rock mechanics, based on yield design formalism. Assuming a quasi-static axial evolution, numerical simulations illustrate the instability of the stationary solution: the bit motion can converge on a limit cycle, often called stick-slip. The beam therefore evolves as a complex cone-shaped limit surface. A simple two-degrees-of-freedom system is now considered in both axial and torsional directions, to quantify the quasi-static axial assumption. The instability of the stationary solution is confirmed by the linearized method for the undamped system with the postulated boundary conditions. Even for small damping values the same result is achieved. Even though a limit cycle appears in the axial plane (small amplitude), stick-slip can be described adequately by considering a quasi-static axial evolution.',\n",
       "  'Challamel2000'),\n",
       " (45,\n",
       "  'QT7I59WW',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'The terms \"inverse problems\" and \"ill-posed problems\" have been steadily and surely gaining popularity in modern science since the middle of the 20th century. A little more than fifty years of studying problems of this kind have shown that a great number of problems from various branches of classical mathematics (computational algebra, differential and integral equations, partial differential equations, functional analysis) can be classified as inverse or ill-posed, and they are among the most complicated ones (since they are unstable and usually nonlinear). At the same time, inverse and ill-posed problems began to be studied and applied systematically in physics, geophysics, medicine, astronomy, and all other areas of knowledge where mathematical methods are used. The reason is that solutions to inverse problems describe important properties of media under study, such as density and velocity of wave propagation, elasticity parameters, conductivity, dielectric permittivity and magnetic permeability, and properties and location of inhomogeneities in inaccessible areas, etc. In this paper we consider definitions and classification of inverse and ill-posed problems and describe some approaches which have been proposed by outstanding Russian mathematicians A. N. Tikhonov, V. K. Ivanov and M. M. Lavrentiev. © de Gruyter 2008.',\n",
       "  'Kabanikhin2008'),\n",
       " (47,\n",
       "  '8JHH63I2',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  \"The atomic pair distribution function (PDF) technique is a powerful approach to gain quantitative insight into the structure of materials where the structural coherence extends only over a few nanometres. In this paper, I focus on PDF from synchrotron X-rays and describe what is the PDF and where it came from, as well as key moments on the journey that have contributed to its enormous recent growth and expanding impact in materials science today. Synchrotron X-ray sources played a starring role in this story. This article is part of the theme issue 'Fifty years of synchrotron science: Achievements and opportunities'.\",\n",
       "  'Billinge2019'),\n",
       " (56,\n",
       "  'JP828P7I',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'A dynamic trading game is examined in which two uninformed buyers engage in Bertrand-like competition to attempt to purchase a single object of uncertain quality from an informed seller. It is shown that there exists a unique perfect sequential equilibrium. The game is compared to an analogous bargaining game in which a single uninformed buyer makes offers to a single seller. Despite the fact that in the equilibrium of the competitive game, buyers compete away their surplus, it is shown that sellers can often gain a higher ex ante surplus in the bargaining game. © 1990 The Review of Economic Studies Limited.',\n",
       "  'Vincent1990'),\n",
       " (58,\n",
       "  'XVP6ESUA',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'In this paper we study the mixing times of a Wright-Fisher model for an asexual population. Typically, such models consist of a population of several genotypes where each genotype reproduces at a different rate, genotypes can mutate to each other, and the population is subject to the evolutionary pressure of selection. While such models have been used extensively to study different types of populations, recently, such stochastic finite population models have been used to model viral populations with the goal of understanding the effect of mutagenic drugs. Here, the time it takes for the population to reach a steady state is important both for carrying out simulations and to determine treatment strength and duration. Despite their importance, and having been widely studied, there has been a lack of such bounds for many relevant ranges of model parameters even for the case of two genotypes (single locus); primarily due to their difficulty, see [Ethll]. The main result of this paper is an analytical bound on the mixing time of a Wright-Fisher model for two genotypes when there is no restriction on the mutation rate or the fitness. Our bound explains (in this setting) the observed phenomena that the mixing time is fast (logarithmic in the state space) for any fitness parameter when the mutation rate is high enough. Theoretically, we overcome the difficulty in proving mixing time bounds by showing that different forces are responsible for the rapid mixing depending on how close it is to its steady state. If the chain is sufficiently close to its stationary distribution, an intricate coupling argument establishes rapid mixing. To show that the chain reaches this state quickly, we connect it to the convergence time of a deterministic model for evolution proposed by Eigen. We are hopeful that our insights and techniques will be helpful in establishing rigorous bounds for more general versions of this model.',\n",
       "  'Vishnoi2015'),\n",
       " (61,\n",
       "  'ZV3MJCRK',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'This article is concerned with the evolution of haploid organisms that reproduce asexually. In a seminal piece of work, Eigen and coauthors proposed the quasispecies model in an attempt to understand such an evolutionary process. Their work has impacted antiviral treatment and vaccine design strategies. Yet, predictions of the quasispecies model are at best viewed as a guideline, primarily because it assumes an infinite population size, whereas realistic population sizes can be quite small. In this paper we consider a population genetics-based model aimed at understanding the evolution of such organisms with finite population sizes and present a rigorous study of the convergence and computational issues that arise therein. Our first result is structural and shows that, at any time during the evolution, as the population size tends to infinity, the distribution of genomes predicted by our model converges to that predicted by the quasispecies model. This justifies the continued use of the quasispecies model to derive guidelines for intervention. While the stationary state in the quasispecies model is readily obtained, due to the explosion of the state space in our model, exact computations are prohibitive. Our second set of results are computational in nature and address this issue. We derive conditions on the parameters of evolution under which our stochastic model mixes rapidly. Further, for a class of widely used fitness landscapes we give a fast deterministic algorithm which computes the stationary distribution of our model. These computational tools are expected to serve as a framework for the modeling of strategies for the deployment of mutagenic drugs. © Copyright 2012, Mary Ann Liebert, Inc. 2012.',\n",
       "  'Dixit2012'),\n",
       " (91,\n",
       "  'KMHD79T9',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'This paper studies bidding dynamics where values and bidding opportunities follow an unrestricted joint Markov process, independent across agents. Bids cannot be retracted, as is frequently the case in auctions. Our main methodological contribution is that we construct a mapping from this general stochastic process into a distribution of values that is independent of the type of auction considered. The equilibria of a static auction with this distribution of values is used to characterize the equilibria of the dynamic auction, making this general class very tractable. As a result of the option of future rebidding, early bids are shaded and under mild conditions increase toward the end of the auction. Our results are consistent with repeated bidding and skewness of the time distribution of winning bids, two puzzling observations in dynamic auctions. As an application, we estimate the model by matching moments from eBay auctions.',\n",
       "  'Hopenhayn2016'),\n",
       " (107,\n",
       "  '4J8VQFF3',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'In this paper we study the mixing time of evolutionary Markov chains over populations of a fixed size (N) in which each individual can be one of m types. These Markov chains have the property that they are guided by a dynamical system from the m-dimensional probability simplex to itself. Roughly, given the current state of the Markov chain, which can be viewed as a probability distribution over the m types, the next state is generated by applying this dynamical system to this distribution, and then sampling from it N times. Many processes in nature, from biology to sociology, are evolutionary and such chains can be used to model them. In this study, the mixing time is of particular interest as it determines the speed of evolution and whether the statistics of the steady state can be efficiently computed. In a recent result [Panageas, Srivastava, Vishnoi, Soda, 2016], it was suggested that the mixing time of such Markov chains is connected to the geometry of this guiding dynamical system. In particular, when the dynamical system has a fixed point which is a global attractor, then the mixing is fast. The limit sets of dynamical systems, however, can exhibit more complex behavior: they could have multiple fixed points that are not necessarily stable, periodic orbits, or even chaos. Such behavior arises in important evolutionary settings such as the dynamics of sexual evolution and that of grammar acquisition. In this paper we prove that the geometry of the dynamical system can also give tight mixing time bounds when the dynamical system has multiple fixed points and periodic orbits. We show that the mixing time continues to remain small in the presence of several unstable fixed points and is exponential in N when there are two or more stable fixed points. As a consequence of our results, we obtain a phase transition result for the mixing time of the sexual/grammar model mentioned above. We arrive at the conclusion that in the interesting parameter regime for these models, i.e., when there are multiple stable fixed points, the mixing is slow. Our techniques strengthen the connections between Markov chains and dynamical systems and we expect that the tools developed in this paper should have a wider applicability.',\n",
       "  'Panageas2016a'),\n",
       " (141,\n",
       "  'V7IPA6PF',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'The thesis investigates a number of auction formats both theoretically and empirically. The effect of different auction rules on the final price and on bidder valuations is analysed. Results from an experimental sale of real goods, testing revenue equivalence of the open and sealed- bid second-price auction do not conform to theoretical predictions: the open auction leading to significantly lower prices than the sealed-bid auction. It turns out that the open auction format allows bidders to satisfy a tendency to “stick together” with their valuations. The empirical results motivate a dynamic bidding model of interdependent valuations, bidders being uncertain about their valuations and learning from the exit-prices of their rivals. Furthermore, bidding behaviour on the Internet is investigated in the hard close and the automatically extended auction. Late bidding is shown to be a rational strategy in the hard close auction, but not in the automatically extended auction. Theoretical results show that the expected final price and seller revenue is lower in the hard close auction than in the automatically extended auction, where prestige-concerns can lead to an explosive final price. Moreover, Yahoo auction data confirms the strong presence of late bidding in the hard-close auction and the seller’s preference for the automatically extended auction.',\n",
       "  'Schindler2003'),\n",
       " (143,\n",
       "  'RJQX4CAD',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  \"The molecular quasi-species model describes the physicochemical organization of monomers into an ensemble of heteropolymers with combinatorial complexity by ongoing template polymerization. Polynucleotides belong to the simplest class of such molecules. The quasi-species itself represents the stationary distribution of macromolecular sequences maintained by chemical reactions effecting error-prone replication and by transport processes. It is obtained deterministically, by mass-action kinetics, as the dominant eigenvalue of a value matrix, W, which is derived directly from chemical rate coefficients, but it also exhibits stochastic features, being composed to a significant fraction of unique individual macromolecular sequences. The quasi-species model demonstrates how macromolecular information originates through specific nonequilibrium autocatalytic reactions and thus forms a bridge between reaction kinetics and molecular evolution. Selection and evolutionary optimization appear as new features in physical chemistry. Concentration bias in the production of mutants is a new concept in population genetics, relevant to frequently mutating populations, which is shown to greatly enhance the optimization properties. The present theory relates to asexually replicating ensembles, but this restriction is not essential. A sharp transition is exhibited between a drifting population of essentially random macromolecular sequences and a localized population of close relatives. This transition at a threshold error rate was found to depend on sequence lengths, distributions of selective values, and population sizes. It has been determined generically for complex landscapes and for special cases, and, it was shown to persist genetically in the presence of nearly neutral mutants. Replication dynamics has much in common with the equilibrium statistics of complex spin systems: the error threshold is equivalent to a magnetic order-disorder transition. A rational function of the replication accuracy plays the role of temperature. Experimental data obtained from test-tube evolution of polynucleotides and from studies of natural virus populations support the quasi-species model. The error threshold seems to set a limit to the genome lengths of several classes of RNA viruses. In addition, the results are relevant even in eucaryotes where they contribute to the exon-intron debate. 1. Molecular Selection Our knowledge of physical and chemical systems is, in a final analysis, based on models derived from repeatable experiments. While none of the classic and rather besieged list of properties rounded up to support the intuition of a distinction between the living and nonliving-metabolism, self-reproduction, irritability, and adaptability, for example-intrinsically limit the application of the scientific method, a determining role by unique or individual entities comes into conflict with the requirement of repeatability. Combinatorial variety, such as that in heteropolymers based on even very small numbers of different bases, even just two, readily provides numbers of different entities so enormous that neither consecutive nor parallel physical realization is possible. The physical chemistry of finite systems of such macromolecules must deal with both known regularities and the advent of unique co-polymeric sequences. Normally this would present no difficulty in a statistical mechanical analysis of typical behavior, where rare events play no significant role, but with autocatalytic polymeri-zation processes even unique single molecules may be amplified to determine the fate of the entire system. Potentially creative, self-organizing around unique events, the dynamics of this simplest living chemical system is invested with regularities that both allow and limit efficient adaptation. The quasi-species model is a study of these regularities. The fundamental regularity in living organisms that has invited explanation is adaptation. Why are organisms so well fitted to their environments? At a more chemical level, why are enzymes fThis is an abridged account of the quasi-species theory that has been submitted in comprehensive form to Advances in Chemical Physics.' 0022-3654/88/2092-6881 $01.50/0 optimal catalysts? Darwin's theory of natural selection has provided biologists with a framework for the answer to this question. The present model is constructed along Darwinian lines but in terms of specific macromolecules, chemical reactions, and physical processes that make the notion of survival of the fittest precise. Not only does the model give an understanding of the physical limitations of adaptation, but also it provides new insight into the role of chance in the process. For an understanding of the structure of this minimal chemical model it is first necessary to recall the conceptual basis of Darwin's theory. Darwin recognized that new inheritable adaptive properties were not induced by the environment but arose independently in the production of offspring. Lasting adaptive changes in a population could only come about by natural selection of the heritable traits or genotype based on the full characteristics or phenotype relevant for producing offspring. A process of chance, i.e., uncorrelated with the developed phenotype, controls changes in the genotype from one generation to the next and generates the diversity necessary for selection. Three factors have probably prevented chemists from gaining a clear insight into these phenomena in the past, despite the discovery of the polymeric nature of the genotype (DNA): the complexity of a minimum replication phenotype, the problem of dealing with a huge number of variants, and the nonequilibrium nature of these ongoing processes.\",\n",
       "  'Eigen1988'),\n",
       " (145,\n",
       "  'W34DX4WH',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'In this paper we prove that the mixing time of a broad class of evolutionary dynamics in finite, unstructured populations is roughly logarithmic in the size of the state space. An important special case of such a stochastic process is the Wright-Fisher model from evolutionary biology (with selection and mutation) on a population of size N over m genotypes. Our main result implies that the mixing time of this process is O(log N) for all mutation rates and fitness landscapes, and solves the main open problem from [4].In particular, it significantly extends the main result in [18] who proved this for m = 2. Biologically, such models have been used to study the evolution of viral populations with applications to drug design strategies countering them. Here the time it takes for the population to reach a steady state is important both for the estimation of the steady-state structure of the population as well in the modeling of the treatment strength and duration. Our result, that such populations exhibit rapid mixing, makes both of these approaches sound. Technically, we make a novel connection between Markov chains arising in evolutionary dynamics and dynamical systems on the probability simplex. This allows us to use the local and global stability properties of the fixed points of such dynamical systems to construct a contractive coupling in a fairly general setting. We expect that our mixing time result would be useful beyond the evolutionary biology setting, and the techniques used here would find applications in bounding the mixing times of Markov chains which have a natural underlying dynamical system.',\n",
       "  'Panageas2016'),\n",
       " (147,\n",
       "  'RRJJLKRQ',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'The use of simulation for high-dimensional intractable computations has revolutionized applied mathematics. Designing, improving and understanding the new tools leads to (and leans on) fascinating mathematics, from representation theory through micro-local analysis. © 2008 American Mathematical Society.',\n",
       "  'Diaconis2009'),\n",
       " (149,\n",
       "  '8GHW6KVJ',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'The topic of Uncertainty Quantification (UQ) has witnessed massive developments in response to the promise of achieving risk mitigation through scientific prediction. It has led to the integration of ideas from mathematics, statistics and engineering being used to lend credence to predictive assessments of risk but also to design actions (by engineers, scientists and investors) that are consistent with risk aversion. The objective of this Handbook is to facilitate the dissemination of the forefront of UQ ideas to their audiences. We recognize that these audiences are varied, with interests ranging from theory to application, and from research to development and even execution.',\n",
       "  'Ghanem2017'),\n",
       " (255,\n",
       "  'Q7YDEKGJ',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'This chapter focuses on the theory of functional analysis. First, the Euler equation was obtained using techniques of dubious validity. Then the linearity of the equation was exploited to establish the existence of a solution of this equation and then to demonstrate its uniqueness. Finally, the quadratic nature of the functional was used to show that the function so obtained provided an absolute minimum. The chapter discusses the fundamental concepts of Hilbert space and then applies these ideas to the basic variational problem. Following this, the analytic aspects of solving the Euler equation and some of the computational problems are described in the chapter. © 1967, Academic Press, Inc.',\n",
       "  'Yosida1967'),\n",
       " (270,\n",
       "  'HJ2GJUKM',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Probabilistic thinking is of growing importance in many areas of mathemat-ics. This paper highlights the beautiful mathematical framework, coupled with practical algorithms, which results from thinking probabilistically about inverse problems arising in partial differential equations. Many inverse problems in the physical sciences require the determination of an un-known field from a finite set of indirect measurements. Examples include oceanography, oil recovery, water resource management and weather forecasting. In the Bayesian ap-proach to these problems, the unknown and the data are modelled as a jointly varying random variable, typically linked through solution of a partial differential equation, and the solution of the inverse problem is the distribution of the unknown given the data. This approach provides a natural way to provide estimates of the unknown field, to-gether with a quantification of the uncertainty associated with the estimate. It is hence a useful practical modelling tool. However it also provides a very elegant mathematical framework for inverse problems: whilst the classical approach to inverse problems leads to ill-posedness, the Bayesian approach leads to a natural well-posedness and stability theory. Furthermore this framework provides a way of deriving and developing algo-rithms which are well-suited to the formidable computational challenges which arise from the conjunction of approximations arising from the numerical analysis of partial differen-tial equations, together with approximations of central limit theorem type arising from sampling of measures.',\n",
       "  'Stuart2017'),\n",
       " (273,\n",
       "  '7I2IXUME',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'This two-volume text provides a complete overview of the theory of Banach spaces, emphasising its interplay with classical and harmonic analysis (particularly Sidon sets) and probability. The authors give a full exposition of all results, as well as numerous exercises and comments to complement the text and aid graduate students in functional analysis. The book will also be an invaluable reference volume for researchers in analysis. Volume 1 covers the basics of Banach space theory, operatory theory in Banach spaces, harmonic analysis and probability. The authors also provide an annex devoted to compact Abelian groups. Volume 2 focuses on applications of the tools presented in the first volume, including Dvoretzky’s theorem, spaces without the approximation property, Gaussian processes, and more. In volume 2, four leading experts also provide surveys outlining major developments in the field since the publication of the original French edition.',\n",
       "  'Li2017'),\n",
       " (275,\n",
       "  '7CEDI6HD',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'To guarantee the correction of numerical simulation programs implementing the finite element method, it is necessary to formalize the mathematical notions and results that allow to establish the soundness of the method. The Lax-Milgram theorem is one of those theoretical cornerstones: under some completeness and coercivity assumptions, it states existence and uniqueness of the solution to the weak formulation of boundary value problems. The purpose of this document is to provide the formal proof community with a very detailed pen-and-paper proof of the Lax-Milgram theorem.',\n",
       "  'Clement2016'),\n",
       " (277,\n",
       "  'C85XBEGL',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'We introduce an alternative approach for the analysis and numerical approximation of the optimal feedback control mapping. It consists in looking at a typical optimal control problem in such a way that feasible controls are mappings depending both in time and space. In this way, the feedback form of the problem is built-in from the very beginning. Optimality conditions are derived for one such optimal mapping, which by construction is the optimal feedback mapping of the problem. In formulating optimality conditions, costates in feedback form are solutions of linear, first-order transport systems, while optimal descent directions are solutions of appropriate obstacle problems. We treat situations with no constraint-sets for control and state, as well as the more general case where a constraint-set is considered for the control variable.',\n",
       "  'Pedregal2017'),\n",
       " (292,\n",
       "  'WRHMBIR4',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Regularization methods are a key tool in the solution of inverse problems. They are used to introduce prior knowledge and allow a robust approximation of ill-posed (pseudo-) inverses. In the last two decades interest has shifted from linear to nonlinear regularization methods, even for linear inverse problems. The aim of this paper is to provide a reasonably comprehensive overview of this shift towards modern nonlinear regularization methods, including their analysis, applications and issues for future research. In particular we will discuss variational methods and techniques derived from them, since they have attracted much recent interest and link to other fields, such as image processing and compressed sensing. We further point to developments related to statistical inverse problems, multiscale decompositions and learning theory.',\n",
       "  'Benning2018'),\n",
       " (295,\n",
       "  '89QEPBBX',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'In this article a unified approach to iterative soft-thresholding algorithms for the solution of linear operator equations in infinite dimensional Hilbert spaces is presented. We formulate the algorithm in the framework of generalized gradient methods and present a new convergence analysis. As main result we show that the algorithm converges with linear rate as soon as the underlying operator satisfies the so-called finite basis injectivity property or the minimizer possesses a so-called strict sparsity pattern. Moreover it is shown that the constants can be calculated explicitly in special cases (i.e. for compact operators). Furthermore, the techniques also can be used to establish linear convergence for related methods such as the iterative thresholding algorithm for joint sparsity and the accelerated gradient projection method. © 2008 Birkhäuser Boston.',\n",
       "  'Bredies2008'),\n",
       " (297,\n",
       "  'WY8SJYFL',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  \"The main purpose of this paper is to suggest a method for finding the minimum of a function f(x) subject to the constraint g(x)=0. The method consists of replacing f by F=f+λg+1/2 cg2, where c is a suitably large constant, and computing the appropriate value of the Lagrange multiplier. Only the simplest algorithm is presented. The remaining part of the paper is devoted to a survey of known methods for finding unconstrained minima, with special emphasis on the various gradient techniques that are available. This includes Newton's method and the method of conjugate gradients. © 1969 Plenum Publishing Corporation.\",\n",
       "  'Hestenes1969'),\n",
       " (301,\n",
       "  'GIUQPQU6',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Several recent algorithms for solving nonlinear programming problems with equality constraints have made use of an augmented \"penalty\" Lagrangian function, where terms involving squares of the constraint functions are added to the ordinary Lagrangian. In this paper, the corresponding penalty Lagrangian for problems with inequality constraints is described, and its relationship with the theory of duality is examined. In the convex case, the modified dual problem consists of maximizing a differentiable concave function (indirectly defined) subject to no constraints at all. It is shown that any maximizing sequence for the dual can be made to yield, in a general way, an asymptotically minimizing sequence for the primal which typically converges at least as rapidly. © 1973 The Mathematical Programming Society.',\n",
       "  'Rockafellar1973'),\n",
       " (302,\n",
       "  'CYTFSGT7',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'The purpose of this volume is to present the principles of the Augmented Lagrangian Method, together with numerous applications of this method to the numerical solution of boundary-value problems for partial differential equations or inequalities arising in Mathematical Physics, in the Mechanics of Continuous Media and in the Engineering Sciences.',\n",
       "  'Fortin2000'),\n",
       " (304,\n",
       "  'JQV64ZJ9',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'ppt slides on ISTA algos: 4 derivations on IST algos (as Expectation Minimization (Nowak), Majorization-Minimization (Daubechies), Forward-Backward Splitting (Bruck, Combettes, Wasj), Separable Approximation(Wright, Nowak)), convergence results, TwIST (two step IST) and SpaRSA, warm start and continuation',\n",
       "  'Figueiredo2009'),\n",
       " (308,\n",
       "  'EEVMXG2E',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'A relaxed randomized Kaczmarz algorithm is investigated in a least squares regression setting by a learning theory approach. When the sampling values are accurate and the regression function (conditional means) is linear, such an algorithm has been well studied in the community of non-uniform sampling. In this paper, we are mainly interested in the different case of either noisy random measurements or a nonlinear regression function. In this case, we show that relaxation is needed. A necessary and sufficient condition on the sequence of relaxation parameters or step sizes for the convergence of the algorithm in expectation is presented. Moreover, polynomial rates of convergence, both in expectation and in probability, are provided explicitly. As a result, the almost sure convergence of the algorithm is proved by applying the Borel-Cantelli Lemma.',\n",
       "  'Lin2015'),\n",
       " (310,\n",
       "  'ZSMZTCTJ',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'In practical scenarios, it is often necessary to be able to make predictions with very limited access to the features of any example. We provide one natural formulation as an online sparse regression problem with squared loss, and ask whether it is possible to achieve sublinear regret with efficient algorithms (i.e. polynomial running time in the natural parameters of the problem).',\n",
       "  'Kale2014'),\n",
       " (312,\n",
       "  '88HI884G',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'The Kaczmarz algorithm (KA) is a popular method for solving a system of linear equations. In this note we derive a new exponential convergence result for the KA. The key allowing us to establish the new result is to rewrite the KA in such a way that its solution path can be interpreted as the output from a particular dynamical system. The asymptotic stability results of the corresponding dynamical system can then be leveraged to prove exponential convergence of the KA. The new bound is also compared to existing bounds.',\n",
       "  'Dai2015'),\n",
       " (322,\n",
       "  'WVB3X3QG',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Any sequence of points in Rn obtained by successive projections of a point on elements of a finite set of hyperplanes is bounded. © 1984.',\n",
       "  'Aharoni1984'),\n",
       " (325,\n",
       "  'G9459S3D',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'The simulations indicate that the existing hard thresholding technique independent of the residual function may cause a dramatic increase or numerical oscillation of the residual. This inherent drawback of the hard thresholding renders the traditional thresholding algorithms unstable and thus generally inefficient for solving practical sparse optimization problems. How to overcome this weakness and develop a truly efficient thresholding method is a fundamental question in this field. The aim of this paper is to address this question by proposing a new thresholding technique based on the notion of optimal k-thresholding. The central idea for this new development is to connect the k-thresholding directly to the residual reduction during the course of algorithms. This leads to a natural design principle for the efficient thresholding methods. Under the restricted isometry property, we prove that the optimal thresholding based algorithms are globally convergent to the solution of sparse optimization problems. The numerical experiments demonstrate that when solving sparse optimization problems, the traditional hard thresholding methods have been significantly transcended by the proposed algorithms which can even outperform the classic `1-minimization method in many situations.',\n",
       "  'Zhao2020'),\n",
       " (327,\n",
       "  '5R635KPF',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'In applications like medical imaging, error correction, and sensor networks, one needs to solve large-scale linear systems that may be corrupted by a small number of arbitrarily large corruptions. We consider solving such large-scale systems of linear equations Ax = b that are inconsistent due to corruptions in the measurement vector b. With this as our motivating example, we develop an approach for this setting that allows detection of the corrupted entries and thus convergence to the \"\"true\"\" solution of the original system. We provide analytical justification for our approaches as well as experimental evidence on real and synthetic systems.',\n",
       "  'Haddock2019a'),\n",
       " (329,\n",
       "  'YBSBB3MW',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'We develop a family of reformulations of an arbitrary consistent linear system into a stochastic problem. The reformulations are governed by two user-defined parameters: a positive definite matrix defining a norm, and an arbitrary discrete or continuous distribution over random matrices. Our reformulation has several equivalent interpretations, allowing for researchers from various communities to leverage their domain specific insights. In particular, our reformulation can be equivalently seen as a stochastic optimization problem, stochastic linear system, stochastic fixed point problem and a probabilistic intersection problem. We prove sufficient, and necessary and sufficient conditions for the reformulation to be exact. Further, we propose and analyze three stochastic algorithms for solving the reformulated problem—basic, parallel and accelerated methods—with global linear convergence rates. The rates can be interpreted as condition numbers of a matrix which depends on the system matrix and on the reformulation parameters. This gives rise to a new phenomenon which we call stochastic preconditioning, and which refers to the problem of finding parameters (matrix and distribution) leading to a sufficiently small condition number. Our basic method can be equivalently interpreted as stochastic gradient descent, stochastic Newton method, stochastic proximal point method, stochastic fixed point method, and stochastic projection method, with fixed stepsize (relaxation parameter), applied to the reformulations.',\n",
       "  'Richtarik2017'),\n",
       " (331,\n",
       "  '9CXPLTPC',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'The Kaczmarz and Gauss-Seidel methods aim to solve a linear $m \\\\times n$ system $\\\\boldsymbol{X} \\\\boldsymbol{\\\\beta} = \\\\boldsymbol{y}$ by iteratively refining the solution estimate; the former uses random rows of $\\\\boldsymbol{X}$ {to update $\\\\boldsymbol{\\\\beta}$ given the corresponding equations} and the latter uses random columns of $\\\\boldsymbol{X}$ {to update corresponding coordinates in $\\\\boldsymbol{\\\\beta}$}. Interest in these methods was recently revitalized by a proof of Strohmer and Vershynin showing linear convergence in expectation for a \\\\textit{randomized} Kaczmarz method variant (RK), and a similar result for the randomized Gauss-Seidel algorithm (RGS) was later proved by Lewis and Leventhal. Recent work unified the analysis of these algorithms for the overcomplete and undercomplete systems, showing convergence to the ordinary least squares (OLS) solution and the minimum Euclidean norm solution respectively. This paper considers the natural follow-up to the OLS problem, ridge regression, which solves $(\\\\boldsymbol{X}^* \\\\boldsymbol{X} + \\\\lambda \\\\boldsymbol{I}) \\\\boldsymbol{\\\\beta} = \\\\boldsymbol{X}^* \\\\boldsymbol{y}$. We present particular variants of RK and RGS for solving this system and derive their convergence rates. We compare these to a recent proposal by Ivanov and Zhdanov to solve this system, that can be interpreted as randomly sampling both rows and columns, which we argue is often suboptimal. Instead, we claim that one should always use RGS (columns) when $m > n$ and RK (rows) when $m < n$. This difference in behavior is simply related to the minimum eigenvalue of two related positive semidefinite matrices, $\\\\boldsymbol{X}^* \\\\boldsymbol{X} + \\\\lambda \\\\boldsymbol{I}_n$ and $\\\\boldsymbol{X} \\\\boldsymbol{X}^* + \\\\lambda \\\\boldsymbol{I}_m$ when $m > n$ or $m < n$.',\n",
       "  'Hefny2017'),\n",
       " (335,\n",
       "  'DIQ7LN5L',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'We combine two iterative algorithms for solving large-scale systems of linear inequalities, the relaxation method of Agmon, Motzkin et al. and the randomized Kaczmarz method. In doing so, we obtain a family of algorithms that generalize and extend both techniques. We prove several convergence results, and our computational experiments show our algorithms often outperform the original methods.',\n",
       "  'DeLoera2017'),\n",
       " (337,\n",
       "  '79HTE7YU',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'In this paper, we investigate a class of heuristic schemes to solve the NP-hard problem of minimizing ℓ-norm over a convex set. A well-known approximation is to consider the convex problem of minimizing ℓ1-norm. We are interested in finding improved results in cases where the problem in ℓ1-norm does not provide an optimal solution to the ℓ-norm problem. We consider a relaxation technique using a family of smooth concave functions depending on a parameter. Some other relaxations have already been tried in the literature and the aim of this paper is to provide a more general context. This motivation allows deriving new theoretical results that are valid for general constraint set. We use a homotopy algorithm, starting from a solution to the problem in ℓ1-norm and ending in a solution of the problem in ℓ-norm. The new results are existence of the solutions of the subproblem, convergence of the scheme, a monotonicity of the solutions and an exact penalization theorem independent of the data.',\n",
       "  'Haddou2020'),\n",
       " (339,\n",
       "  'TX9IQDAE',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Iterative linear solvers have gained recent popularity due to their computational efficiency and low memory footprint for large-scale linear systems. The relaxation method, or Motzkin’s method, can be viewed as an iterative method that projects the current estimation onto the solution hyperplane corresponding to the most violated constraint. Although this leads to an optimal selection strategy for consistent systems, for inconsistent least square problems, the strategy presents a tradeoff between convergence rate and solution accuracy. We provide a theoretical analysis that shows Motzkin’s method offers an initially accelerated convergence rate and this acceleration depends on the dynamic range of the residual. We quantify this acceleration for Gaussian systems as a concrete example. Lastly, we include experimental evidence on real and synthetic systems that support the analysis.',\n",
       "  'Haddock2019'),\n",
       " (341,\n",
       "  'DH56LMMZ',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'It is now well understood that (1) it is possible to reconstruct sparse signals exactly from what appear to be highly incomplete sets of linear measurements and (2) that this can be done by constrained ℓ1 minimization. In this paper, we study a novel method for sparse signal recovery that in many situations outperforms ℓ1 minimization in the sense that substantially fewer measurements are needed for exact recovery. The algorithm consists of solving a sequence of weighted ℓ1- minimization problems where the weights used for the next iteration are computed from the value of the current solution. We present a series of experiments demonstrating the remarkable performance and broad applicability of this algorithm in the areas of sparse signal recovery, statistical estimation, error correction and image processing. Interestingly, superior gains are also achieved when our method is applied to recover signals with assumed near-sparsity in overcomplete representations-not by reweighting theℓ1 norm of the coefficient sequence as is common, but by reweighting the ℓ1 norm of the transformed object. An immediate consequence is the possibility of highly efficient data acquisition protocols by improving on a technique known as Compressive Sensing. © 2008 Birkhäuser Boston.',\n",
       "  'Candes2008a'),\n",
       " (343,\n",
       "  '8M6PSFR3',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'In this article we dwell into the class of so called ill-posed Linear Inverse Problems (LIP) which simply refers to the task of recovering the entire signal from its relatively few random linear measurements. Such problems arise in variety of settings with applications ranging from medical image processing, recommender systems, etc. We propose a slightly generalised version of the error constrained linear inverse problem and obtain a novel and equivalent convex-concave min-max reformulation by providing an exposition to its convex geometry. Saddle points of the min-max problem are completely characterised in terms of a solution to the LIP, and vice versa. Applying simple saddle point seeking ascend-descent type algorithms to solve the min-max problems provides novel and simple algorithms to find a solution to the LIP. Moreover, reformulation of an LIP as the min-max problem provided in this article is crucial in developing methods to solve the dictionary learning problem with almost sure recovery constraints.',\n",
       "  'Sheriff2020'),\n",
       " (347,\n",
       "  'LVTDCDNJ',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'The Kaczmarz method for solving linear systems of equations is an iterative algorithm that has found many applications ranging from computer tomography to digital signal processing. Despite the popularity of this method, useful theoretical estimates for its rate of convergence are still scarce. We introduce a randomized version of the Kaczmarz method for consistent, overdetermined linear systems and we prove that it converges with expected exponential rate. Furthermore, this is the first solver whose rate does not depend on the number of equations in the system. The solver does not even need to know the whole system but only a small random part of it. It thus outperforms all previously known methods on general extremely overdetermined systems. Even for moderately overdetermined systems, numerical simulations as well as theoretical analysis reveal that our algorithm can converge faster than the celebrated conjugate gradient algorithm. Furthermore, our theory and numerical simulations confirm a prediction of Feichtinger et al. in the context of reconstructing bandlimited functions from nonuniform sampling. © 2008 Birkhäuser Boston.',\n",
       "  'Strohmer2009'),\n",
       " (353,\n",
       "  'IME9ZZQJ',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'The Kaczmarz algorithm is a simple iterative scheme for solving consistent linear systems. At each step, the method projects the current iterate onto the solution space of a single constraint. Hence, it requires very low cost per iteration and storage, and it has a linear rate of convergence. Distributed implementations of Kaczmarz have become, in recent years, the de facto architectural choice for large-scale linear systems. Therefore, in this paper we develop a family of randomized block Kaczmarz algorithms that uses at each step a subset of the constraints and extrapolated stepsizes, and can be deployed on distributed computing units. Our approach is based on several new ideas and tools, including stochastic selection rule for the blocks of rows, stochastic conditioning of the linear system, and novel strategies for designing extrapolated stepsizes. We prove that randomized block Kaczmarz algorithm converges linearly in expectation, with a rate depending on the geometric properties of the matrix and its submatrices and on the size of the blocks. Our convergence analysis reveals that the algorithm is most effective when it is given a good sampling of the rows into well-conditioned blocks. Besides providing a general framework for the design and analysis of randomized block Kaczmarz methods, our results resolve an open problem in the literature related to the theoretical understanding of observed practical efficiency of extrapolated block Kaczmarz methods.',\n",
       "  'Necoara2019'),\n",
       " (359,\n",
       "  'VPRN5GHF',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'This paper investigates the randomized version of the Kaczmarz method to solve linear systems in the case where the adjoint of the system matrix is not exact—a situation we refer to as “mismatched adjoint”. We show that the method may still converge both in the over- and underdetermined consistent case under appropriate conditions, and we calculate the expected asymptotic rate of linear convergence. Moreover, we analyze the inconsistent case and obtain results for the method with mismatched adjoint as for the standard method. Finally, we derive a method to compute optimized probabilities for the choice of the rows and illustrate our findings with numerical examples.',\n",
       "  'Lorenz2018'),\n",
       " (361,\n",
       "  'BEYVQHE4',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'The time-frequency and time-scale communities have recently developed a large number of overcomplete waveform dictionaries-stationary wavelets, wavelet packets, cosine packets, chirplets, and warplets, to name a few. Decomposition into overcomplete systems is not unique, and several methods for decomposition have been proposed, including the method of frames (MOF), matching pursuit (MP), and, for special dictionaries, the best orthogonal basis (BOB). Basis pursuit (BP) is a principle for decomposing a signal into an \"optimal\" superposition of dictionary elements, where optimal means having the smallest l1 norm of coefficients among all such decompositions. We give examples exhibiting several advantages over MOF, MP, and BOB, including better sparsity and superresolution. BP has interesting relations to ideas in areas as diverse as ill-posed problems, abstract harmonic analysis, total variation denoising, and multiscale edge denoising. BP in highly overcomplete dictionaries leads to large-scale optimization problems. With signals of length 8192 and a wavelet packet dictionary, one gets an equivalent linear program of size 8192 by 212,992. Such problems can be attacked successfully only because of recent advances in linear and quadratic programming by interior-point methods. We obtain reasonable success with a primal-dual logarithmic barrier method and conjugate-gradient solver.',\n",
       "  'Chen2001'),\n",
       " (363,\n",
       "  'VUTD2L26',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'The linearized Bregman method is a method to calculate sparse solutions to systems of linear equations. We formulate this problem as a split feasibility problem, propose an algorithmic framework based on Bregman projections, and prove a general convergence result for this framework. Convergence of the linearized Bregman method will be obtained as a special case. Our approach also allows for several generalizations such as other objective functions, incremental iterations, incorporation of non-Gaussian noise models, and box constraints. © 2014 Society for Industrial and Applied Mathematics.',\n",
       "  'Lorenz2014'),\n",
       " (365,\n",
       "  'RF983IJ3',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Extracting information from nonlinear measurements is a fundamental challenge in data analysis. In this work, we consider separable inverse problems, where the data are modeled as a linear combination of functions that depend nonlinearly on certain parameters of interest. These parameters may represent neuronal activity in a human brain, frequencies of electromagnetic waves, fluorescent probes in a cell, or magnetic relaxation times of biological tissues. Separable nonlinear inverse problems can be reformulated as underdetermined sparse-recovery problems, and solved using convex programming. This approach has had empirical success in a variety of domains, from geophysics to medical imaging, but lacks a theoretical justification. In particular, compressed-sensing theory does not apply, because the measurement operators are deterministic and violate incoherence conditions such as the restricted-isometry property. Our main contribution is a theory for sparse recovery adapted to deterministic settings. We show that convex programming succeeds in recovering the parameters of interest, as long as their values are sufficiently distinct with respect to the correlation structure of the measurement operator. The theoretical results are illustrated through numerical experiments for two applications: heat-source localization and estimation of brain activity from electroencephalography data.',\n",
       "  'Bernstein2020'),\n",
       " (369,\n",
       "  'PSU4NWDX',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'In this paper, we propose a paradigm of control, called a maximum hands-off control. A hands-off control is defined as a control that has a short support per unit time. The maximum hands-off control is the minimum support (or sparsest) per unit time among all controls that achieve control objectives. For finite horizon continuous-time control, we show the equivalence between the maximum hands-off control and L1-optimal control under a uniqueness assumption called normality. This result rationalizes the use of L1 optimality in computing a maximum hands-off control. The same result is obtained for discrete-time hands-off control. We also propose an L1/L2-optimal control to obtain a smooth hands-off control. Furthermore, we give a self-triggered feedback control algorithm for linear time-invariant systems, which achieves a given sparsity rate and practical stability in the case of plant disturbances. An example is included to illustrate the effectiveness of the proposed control.',\n",
       "  'Nagahara2016'),\n",
       " (371,\n",
       "  'Z79VBMX2',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'The following problem is considered: given a matrix A in Rm×n, (m rows and n columns), a vector b in Rm, and ε > 0, compute a vector x satisfying ∥Ax - b∥2 ≤ ε if such exists, such that x has the fewest number of non-zero entries over all such vectors. It is shown that the problem is NP-hard, but that the well-known greedy heuristic is good in that it computes a solution with at most [18 Opt(ε/2)∥A+∥22 ln(∥b∥2/ε)] non-zero entries, where Opt(ε/2) is the optimum number of nonzero entries at error ε/2, A is the matrix obtained by normalizing each column of A with respect to the L2 norm, and A+ is its pseudo-inverse.',\n",
       "  'Natarajan1995'),\n",
       " (374,\n",
       "  'C74YT8SQ',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'The Kaczmarz algorithm is an iterative method for reconstructing a signal x∈ℝd from an overcomplete collection of linear measurements yn = 〈x,φn〉, n ≥ 1. We prove quantitative bounds on the rate of almost sure exponential convergence in the Kaczmarz algorithm for suitable classes of random measurement vectors {φn}n=1∞ ⊂ ℝd. Refined convergence results are given for the special case when each φn has i.i.d. Gaussian entries and, more generally, when each φn/{double pipe}φn{double pipe} is uniformly distributed on Sd-1. This work on almost sure convergence complements the mean squared error analysis of Strohmer and Vershynin for randomized versions of the Kaczmarz algorithm. © 2012 Springer Science+Business Media, LLC.',\n",
       "  'Chen2012'),\n",
       " (376,\n",
       "  '4T9KPRM5',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Compressed Sensing uses a small number of random, linear measurements to acquire a sparse signal. Nonlinear algorithms, such as ℓ1 minimization, are used to reconstruct the signal from the measured data. This paper proposes row-action methods as a computational approach to solving the ℓ1 optimization problem. This paper presents a specific row-action method and provides extensive empirical evidence that it is an effective technique for signal reconstruction. This approach offers several advantages over interior-point methods, including minimal storage and computational requirements, scalability, and robustness. © 2006 IEEE.',\n",
       "  'Sra2006'),\n",
       " (378,\n",
       "  'SYH8KZNE',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'The problem of finding a minimum ℓ1-norm solution to an underdetermined linear system is an important problem in compressed sensing, where it is also known as basis pursuit. We propose a heuristic optimality check as a general tool for ℓ1-minimization, which often allows for early termination by \"guessing\" a primaldual optimal pair based on an approximate support.Moreover,we provide an extensive numerical comparison of various state-of-the-art ℓ1-solvers that have been proposed during the last decade, on a large test set with a variety of explicitly given matrices and several right-hand sides per matrix reflecting different levels of solution difficulty. The results, as well as improvements by the proposed heuristic optimality check, are analyzed in detail to provide an answer to the question which algorithm is the best.',\n",
       "  'Lorenz2015'),\n",
       " (386,\n",
       "  'VGZGUCNJ',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'The distributed Kaczmarz algorithm is an adaptation of the standard Kaczmarz algorithm to the situation in which data is distributed throughout a network represented by a tree. We isolate substructures of the network and study convergence of the distributed Kaczmarz algorithm for relatively large relaxation parameters associated to these substructures. If the system is consistent, then the algorithm converges to the solution of minimal norm; however, if the system is inconsistent, then the algorithm converges to an approximated least-squares solution that is dependent on the parameters and the network topology. We show that the relaxation parameters may be larger than the standard upper-bound in literature in this context and provide numerical experiments to support our results.',\n",
       "  'Borgard2020'),\n",
       " (388,\n",
       "  'D4MI8XVT',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'The iterative method for solving system of linear equations, due to Kaczmarz [2], is investigated. It is shown that the method works well for both singular and non-singular systems and it determines the affine space formed by the solutions if they exist. The method also provides an iterative procedure for computing a generalized inverse of a matrix. © 1971 Springer-Verlag.',\n",
       "  'Tanabe1971'),\n",
       " (407,\n",
       "  'MMP9VPRQ',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Projection adjustment is a technique that improves the rate of convergence, as measured by the number of iterations needed to achieve a given level of performance, of the Kaczmarz algorithm (KA) for iteratively solving a system of consistent linear equations, however at the cost of requiring additional time per iteration and increased storage. This hinders the applicability of the previously published Kaczmarz algorithm with projection adjustment (KAPA) to large-scale problems. An enhancement EKAPA of KAPA that uses projection adjustment only for a small subset of the equations is proposed for significantly reducing the time and storage requirements. An analysis of the behavior of EKAPA is provided. An illustration is given to show that EKAPA using a small subset of the equations for projection adjustment can achieve a speed-up over KA similar to that of KAPA in terms of the number of iterations, but requires much less computer time and storage; hence, it is more suitable for large-scale problems.',\n",
       "  'Lin2020'),\n",
       " (409,\n",
       "  'EYTHARPP',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Principal component analysis (PCA) is one of the most widely used dimensionality reduction tools in scientific data analysis. The PCA direction, given by the leading eigenvector of a covariance matrix, is a linear combination of all features with nonzero loadingsthis impedes interpretability. Sparse principal component analysis (SPCA) is a framework that enhances interpretability by incorporating an additional sparsity requirement in the feature weights (factor loadings) while finding a direction that explains the maximal variation in the data. However, unlike PCA, the optimization problem associated with the SPCA problem is NP-hard. While many heuristic algorithms based on variants of the power method are used to obtain good solutions, they do not provide certificates of optimality on the solution-quality via associated dual bounds. Dual bounds are available via standard semidefinite programming (SDP) based relaxations, which may not be tight and the SDPs are difficult to scale using off-The-shelf solvers. In this paper, we present a convex integer programming (IP) framework to solve the SPCA problem to near-optimality, with an emphasis on deriving associated dual bounds. We present worst-case results on the quality of the dual bound provided by the convex IP. We empirically observe that the dual bounds are significantly better than worst-case performance, and are superior to the SDP bounds on some real-life instances. Moreover, solving the convex IP model using commercial IP solvers appears to scale much better that solving the SDP-relaxation using commercial solvers. To the best of our knowledge, we obtain the best dual bounds for real and artificial instances for SPCA problems involving covariance matrices of size up to 2000 × 2000.',\n",
       "  'Dey2018'),\n",
       " (411,\n",
       "  'XE8H3Y3H',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'We consider the least squares regression problem, penalized with a combination of the `0 and `2 norms (a.k.a. `0`2 regularization). Recent work presents strong evidence that the resulting `0-based estimators can outperform popular sparse learning methods, under many important high-dimensional settings. However, exact computation of `0-based estimators remains a major challenge. Indeed, state-of-the-art mixed integer programming (MIP) methods for `0`2-regularized regression face difficulties in solving many statistically interesting instances when the number of features p ∼ 104. In this work, we present a new exact MIP framework for `0`2-regularized regression that can scale to p ∼ 107, achieving over 3600x speed-ups compared to the fastest exact methods. Unlike recent work, which relies on modern MIP solvers, we design a specialized nonlinear BnB framework, by critically exploiting the problem structure. A key distinguishing component in our algorithm lies in efficiently solving the node relaxations using specialized first-order methods, based on coordinate descent (CD). Our CD-based method effectively leverages information across the BnB nodes, through using warm starts, active sets, and gradient screening. In addition, we design a novel method for obtaining dual bounds from primal solutions, which certifiably works in high dimensions. Experiments on synthetic and real high-dimensional datasets demonstrate that our method is not only significantly faster than the state of the art, but can also deliver certifiably optimal solutions to statistically challenging instances that cannot be handled with existing methods. We open source the implementation through our toolkit L0BnB.',\n",
       "  'Hazimeh2020'),\n",
       " (413,\n",
       "  'TWW88JF8',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'MANY mathematical and applied problems can be reduced to finding some common point of a system (finite or infinite) of convex sets. Usually each of the sets is such that it is not difficult to find the projection of any point on to this set. In this paper we shall consider various methods of finding points from the intersection of sets, using projection on to a separate set as an elementary operation. The strong convergence of the sequences obtained in this way is proved. Applications are given to various problems, including the problem of best approximation and problems of optimal control. Particular attention is paid in the latter case to problems with restrictions on the phase coordinates. © 1970.',\n",
       "  'Gubin1967'),\n",
       " (415,\n",
       "  'AL9X9BHE',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'We shall in this paper consider the problem of computing a generalized solution of a given linear system of equations. The matrix will be partitioned by blocks of rows or blocks of columns. The generalized inverses of the blocks are then used as data to Jacobi- and SOR-types of iterative schemes. It is shown that the methods based on partitioning by rows converge towards the minimum norm solution of a consistent linear system. The column methods converge towards a least squares solution of a given system. For the case with two blocks explicit expressions for the optimal values of the iteration parameters are obtained. Finally an application is given to the linear system that arises from reconstruction of a two-dimensional object by its one-dimensional projections. © 1980 Springer-Verlag.',\n",
       "  'Elfving1980'),\n",
       " (417,\n",
       "  'MT6UGWKU',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'This paper brings together and discusses theory and applications of methods, identified and labelled as row-action methods, for linear feasibility problems (find $x \\\\in {\\\\bf R}^n $, such that $Ax \\\\leqq b$), linearly constrained optimization problems (minimize $f(x)$, subject to $Ax \\\\leqq b$) and some interval convex programming problems (minimize $f(x)$, subject to $c \\\\leqq Ax \\\\leqq b$).\\\\n\\\\nThe main feature of row-action methods is that they are iterative procedures which, without making any changes to the original matrix A, use the rows of A, one row at a time. Such methods are important and have demonstrated effectiveness for problems with large or huge matrices which do not enjoy any detectable or usable structural pattern, apart from a high degree of sparaseness.\\\\n\\\\nFields of application where row-action methods are used in various ways include image reconstruction from projection, operations research and game theory, learning theory, pattern recognition and transportation theory. A row-action method for the nonlinear convex feasibility problem is also presented.\\\\n\\\\n\\\\n\\\\nRead More: http://epubs.siam.org.proxy-um.researchport.umd.edu/doi/abs/10.1137/1023097',\n",
       "  'Censor1981'),\n",
       " (419,\n",
       "  'A6NAGGAQ',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Some methods of successive approximation for the solution of simultaneous linear equations are discussed. The coefficient matrix A of the linear system is assumed to be sparse. It is shown that savings in the computer storage and the computing time are possible, if there exists a subset of the rows (columns) of A, consisting of only orthogonal rows (columns). Such savings are also possible, if for some permutation matrices P and Q, PAQ has a particular structure, viz., singly bordered block diagonal form. It is shown that the set of orthogonal rows (columns) of A, as well as P and Q can be determined by using some results from graph theory (e.g., incidence matrices, row and column graphs, points of attachment). Geometrical interpretations of the methods and their inter-relatiohip are given. © 1969 The British Computer Society.',\n",
       "  'Tewarson1969'),\n",
       " (429,\n",
       "  'MF96NTJF',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'We present TRex, a flexible and robust Tomographic Reconstruction framework using proximal algorithms. We provide an overview and perform an experimental comparison between the famous iterative reconstruction methods in terms of reconstruction quality in sparse view situations. We then derive the proximal operators for the four best methods. We show the flexibility of our framework by deriving solvers for two noise models: Gaussian and Poisson; and by plugging in three powerful regularizers. We compare our framework to state of the art methods, and show superior quality on both synthetic and real datasets.',\n",
       "  'Aly2016'),\n",
       " (431,\n",
       "  'TX46X33P',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'For solving large-scale systems of linear equations by iteration methods, we introduce an effective probability criterion for selecting the working rows from the coefficient matrix and construct a greedy randomized Kaczmarz method. It is proved that this method converges to the unique least-norm solution of the linear system when it is consistent. Theoretical analysis demonstrates that the convergence rate of the greedy randomized Kaczmarz method is much faster than the randomized Kaczmarz method, and numerical results also show that the greedy randomized Kaczmarz method is more efficient than the randomized Kaczmarz method.',\n",
       "  'Bai2018'),\n",
       " (432,\n",
       "  'VSS69KW9',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'For solving large sparse systems of linear equations by iteration methods, we further generalize the greedy randomized Kaczmarz method by introducing a relaxation parameter in the involved probability criterion, obtaining a class of relaxed greedy randomized Kaczmarz methods. We prove the convergence of these methods when the linear system is consistent, and show that these methods can be more efficient than the greedy randomized Kaczmarz method if the relaxation parameter is chosen appropriately.',\n",
       "  'Bai2018a'),\n",
       " (433,\n",
       "  'HHLDD6VN',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'For solving large sparse, overdetermined, and inconsistent system of linear equations by iteration methods, by further reconstructing the randomized extended Kaczmarz method proposed by Zouzias and Freris in 2013 (SIAM J. Matrix Anal. Appl. 34 (2013), 773–793), we propose a partially randomized extended Kaczmarz method. When the coefficient matrix is assumed to be of full column rank, we prove the convergence and derive an upper bound for the expected convergence rate of the partially randomized extended Kaczmarz method. This bound could be smaller than that of the randomized extended Kaczmarz method under certain conditions. Moreover, with numerical results we show that the partially randomized extended Kaczmarz method can be much more effective than the randomized extended Kaczmarz method.',\n",
       "  'Bai2019'),\n",
       " (435,\n",
       "  'VZRC3QKE',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  \"Component averaging (CAV) is introduced as a new iterative parallel technique suitable for large and sparse unstructured systems of linear equations. It simultaneously projects the current iterate onto all the system's hyperplanes, and is thus inherently parallel. However, instead of orthogonal projections and scalar weights (as used, for example, in Cimmino's method), it uses oblique projections and diagonal weighting matrices, with weights related to the sparsity of the system matrix. These features provide for a practical convergence rate which approaches that of algebraic reconstruction technique (ART) (Kaczmarz's row-action algorithm) - even on a single processor. Furthermore, the new algorithm also converges in the inconsistent case. A proof of convergence is provided for unit relaxation, and the fast convergence is demonstrated on image reconstruction problems of the Herman head phantom obtained within the SNARK93 image reconstruction software package. Both reconstructed images and convergence plots are presented. The practical consequences of the new technique are far reaching for real-world problems in which iterative algorithms are used for solving large, sparse, unstructured and often inconsistent systems of linear equations. © 2001 Elsevier Science B.V.\",\n",
       "  'Censor2001a'),\n",
       " (436,\n",
       "  '95D9N8YW',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Component averaging (CAV) was recently introduced by Censor, Gordon, and Gordon as a new iterative parallel technique suitable for large and sparse unstructured systems of linear equations. Based on earlier work of Byrne and Censor, it uses diagonal weighting matrices, with pixel-related weights determined by the sparsity of the system matrix. CAV is inherently parallel (similar to the very slowly converging Cimmino method) but its practical convergence on problems of image reconstruction from projections is similar to that of the algebraic reconstruction technique (ART). Parallel techniques are becoming more important for practical image reconstruction since they are relevant not only for supercomputers but also for the increasingly prevalent multiprocessor workstations. This paper reports on experimental results with a block-iterative version of component averaging (BICAV). When BICAV is optimized for block size and relaxation parameters, its very first iterates are far superior to those of CAV, and more or less on a par with ART. Similar to CAV, BICAV is also inherently parallel. The fast convergence is demonstrated on problems of image reconstruction from projections, using the SNARK93 image reconstruction software package. Detailed plots of various measures of convergence, and reconstructed images are presented. © 2001 IEEE.',\n",
       "  'Censor2001'),\n",
       " (437,\n",
       "  '7CZVWJYY',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Frequently, we use the Moore-Penrose pseudoinverse (MPP) even in cases when we do not require all of its defining properties. But if the running time and the storage size are critical, we can do better. By discarding some constraints needed for the MPP, we gain freedom to optimize other aspects of the new pseudoinverse. A sparser pseudoinverse reduces the amount of computation and storage. We propose a method to compute a sparse pseudoinverse and show that it offers sizable improvements in speed and storage, with a small loss in the least-squares performance. Differently from previous approaches, we do not attempt to approximate the MPP, but rather to produce an exact but sparse pseudoinverse. In the underdetermined (compressed sensing) scenario we prove that the rescaled sparse pseudoinverse yields an unbiased estimate of the unknown vector, and we demonstrate its potential in iterative sparse recovery algorithms, pointing out directions for future research. © 2013 IEEE.',\n",
       "  'Dokmanic2013'),\n",
       " (438,\n",
       "  '7NVZXNFN',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'While single measurement vector (SMV) models have been widely studied in signal processing, there is a surging interest in addressing the multiple measurement vectors (MMV) problem. In the MMV setting, more than one measurement vector is available and the multiple signals to be recovered share some commonalities such as a common support. Applications in which MMV is a naturally occurring phenomenon include online streaming, medical imaging, and video recovery. This work presents a stochastic iterative algorithm for the support recovery of jointly sparse corrupted MMV. We present a variant of the sparse randomized Kaczmarz algorithm for corrupted MMV and compare our proposed method with an existing Kaczmarz type algorithm for MMV problems. We also showcase the usefulness of our approach in the online (streaming) setting and provide empirical evidence that suggests the robustness of the proposed method to the number of corruptions and the distribution from which the corruptions are drawn.',\n",
       "  'Durgin2019'),\n",
       " (439,\n",
       "  'QB74Q2ZG',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Magnetic Particle Imaging (MPI) is a preclinical imaging technique capable of visualizing the spatio-temporal distribution of magnetic nanoparticles. The image reconstruction of this fast and dynamic process relies on efficiently solving an ill-posed inverse problem. Current approaches to reconstruct the tracer concentration from its measurements are either adapted to image characteristics of MPI but suffer from higher computational complexity and slower convergence or are fast but lack in the image quality of the reconstructed images. In this work we propose a novel MPI reconstruction method to combine the advantages of both approaches into a single algorithm. The underlying sparsity prior is based on an undecimated wavelet transform and is integrated into a fast row-action framework to solve the corresponding MPI minimization problem. Its performance is numerically evaluated against a classical FISTA approach on simulated and real MPI data. We also compare the results to the state-of-the-art MPI reconstruction methods. In all cases, our approach shows better reconstruction results and at the same time accelerates the convergence rate of the underlying row-action algorithm.',\n",
       "  'Lieb2020'),\n",
       " (441,\n",
       "  'LDBE7W2Q',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Algorithms based on linearized Bregman iterations are able to perform sparse reconstruction at a low computational complexity. Especially the Least-Mean-Squares (LMS) and Kacz-marz variants of linearized Bregman iterations proved to be very feasible for fixed-point digital hardware implementation. We present a method that we call microkicking for improving the convergence speed of linearized Bregman based algorithms. This method can be implemented with only a negligible complexity overhead leading to significantly faster convergence for both variants of the linearized Bregman iterations. We furthermore show simulation results demonstrating the performance gains achievable by microkicking.',\n",
       "  'Lunglmayr2018'),\n",
       " (446,\n",
       "  'Z9S8PHGD',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Typical greedy algorithms for sparse reconstruction problems, such as orthogonal matching pursuit and iterative thresholding, seek strictly sparse solutions. Recent work in the literature suggests that given a priori knowledge of the distribution of the sparse signal coefficients, better results can be obtained by a weighted averaging of several sparse solutions. Such a combination of solutions, while not strictly sparse, approximates an MMSE estimator and can outperform strictly sparse solvers in terms of l-2 reconstruction error. We introduce a novel method for obtaining such an approximate MMSE estimator by replacing the deterministic thresholding operator of Iterative Hard Thresholding with a randomized version. We demonstrate the improvement in performance experimentally for both synthetic 1D signals and real images. © 2014 IEEE.',\n",
       "  'Crandall2014'),\n",
       " (456,\n",
       "  'MLJV6IUI',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'In this paper, we present a new framework of bi-level unconstrained minimization for development of accelerated methods in Convex Programming. These methods use approximations of the high-order proximal points, which are solutions of some auxiliary parametric optimization problems. For computing these points, we can use different methods, and, in particular, the lower-order schemes. This opens a possibility for the latter methods to overpass traditional limits of the Complexity Theory. As an example, we obtain a new second-order method with the convergence rate O(k- 4) , where k is the iteration counter. This rate is better than the maximal possible rate of convergence for this type of methods, as applied to functions with Lipschitz continuous Hessian. We also present new methods with the exact auxiliary search procedure, which have the rate of convergence O(k-(3p+1)/2) , where p≥ 1 is the order of the proximal operator. The auxiliary problem at each iteration of these schemes is convex.',\n",
       "  'Nesterov2021'),\n",
       " (458,\n",
       "  'VSMIFTFG',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'In this paper we analyze several new methods for solving optimization problems with the objective function formed as a sum of two terms: one is smooth and given by a black-box oracle, and another is a simple general convex function with known structure. Despite the absence of good properties of the sum, such problems, both in convex and nonconvex cases, can be solved with efficiency typical for the first part of the objective. For convex problems of the above structure, we consider primal and dual variants of the gradient method (with convergence rate O(1/k)), and an accelerated multistep version with convergence rate O(k2/1), where k is the iteration counter. For nonconvex problems with this structure, we prove convergence to a point from which there is no descent direction. In contrast, we show that for general nonsmooth, nonconvex problems, even resolving the question of whether a descent direction exists from a point is NP-hard. For all methods, we suggest some efficient \"line search\" procedures and show that the additional computational work necessary for estimating the unknown problem class parameters can only multiply the complexity of each iteration by a small constant factor. We present also the results of preliminary computational experiments, which confirm the superiority of the accelerated scheme. © 2012 Springer-Verlag Berlin Heidelberg and Mathematical Optimization Society.',\n",
       "  'Nesterov2013'),\n",
       " (460,\n",
       "  'J3D4KXAP',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'In this paper we analyze several new methods for solving optimization problems with the objective function formed as a sum of two convex terms: one is smooth and given by a black-box oracle, and another is general but simple and its structure is known. Despite to the bad properties of the sum, such problems, both in convex and nonconvex cases, can be solved with efficiency typical for the good part of the objective. For convex problems of the above structure, we consider primal and dual variants of the gradient method (converge as O 1 k), and an accelerated multistep version with convergence rate O 1 k 2 , where k is the iteration counter. For all methods, we suggest some efficient \"line search\" procedures and show that the additional computational work necessary for estimating the unknown problem class parameters can only multiply the complexity of each iteration by a small constant factor. We present also the results of preliminary computational experiments, which confirm the superiority of the accelerated scheme.',\n",
       "  'Nesterov2007'),\n",
       " (465,\n",
       "  'WETI5VIH',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'In this paper, we propose an alternative version of the randomized Kaczmarz method, which chooses each row of the coefficient matrix A with probability proportional to the square of the Euclidean norm of the residual of each corresponding equation. We prove that it converges with expected linear rate and the convergence rate of this method is better than the Strohmer and Vershynin’s RK method. Numerical experiments also show this method is more efficient than the RK method.',\n",
       "  'Guan2020'),\n",
       " (469,\n",
       "  'FUT5TMMB',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'The Kaczmarz method is an iterative algorithm for solving systems of linear equalities and inequalities, that iteratively projects onto these constraints. Recently, Strohmer and Vershynin [J. Fourier Anal. Appl., 15(2):262-278, 2009] gave a non-asymptotic convergence rate analysis for this algorithm, spurring numerous extensions and generalizations of the Kaczmarz method. Rather than the randomized selection rule analyzed in that work, in this paper we instead discuss greedy and approximate greedy selection rules. We show that in some applications the computational costs of greedy and random selection are comparable, and that in many cases greedy selection rules give faster convergence rates than random selection rules. Further, we give the first multi-step analysis of Kaczmarz methods for a particular greedy rule, and propose a provably-faster randomized selection rule for matrices with many pairwise-orthogonal rows.',\n",
       "  'Nutini2016'),\n",
       " (471,\n",
       "  '6E5SB8X8',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Using the concept of stable Hilbert space splittings, we provide a unified approach to the convergence analysis for multiplicative Schwarz methods (a version of alternating directions methods), and in particular Kaczmarz-type methods for solving linear systems. We consider both fixed cyclic and randomized ordering strategies, and cover block versions as well. For the classical Kaczmarz method with cyclic ordering for solving general linear systems Ax=b, a new convergence rate estimate in terms of the generalized condition number of A and logarithmically depending on the rank of A is presented.',\n",
       "  'Oswald2015'),\n",
       " (473,\n",
       "  'ENVTBAC7',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'In this paper, we make a theoretical analysis of the convergence rates of Kaczmarz and extended Kaczmarz projection algorithms for some of the most practically used control sequences. We first prove an at least linear convergence rate for the Kaczmarz-Tanabe and its extended version methods (the one in which a complete set of projections using row/column indices is performed in each iteration). Then, we apply the main ideas of this analysis in establishing an at least sublinear, respectively, linear convergence rate for the Kaczmarz algorithm with almost cyclic and the remotest set control strategies, and their extended versions, respectively. These results complete the existing ones related to the random selection procedures.',\n",
       "  'Popa2018'),\n",
       " (475,\n",
       "  '8P3KFZIB',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'We consider the class of iterative shrinkage-thresholding algorithms (ISTA) for solving linear inverse problems arising in signal/image processing. This class of methods, which can be viewed as an ex- tension of the classical gradient algorithm, is attractive due to its simplicity and thus is adequate for solving large-scale problems even with dense matrix data. However, such methods are also known to converge quite slowly. In this paper we present a new fast iterative shrinkage-thresholding algorithm (FISTA) which preserves the computational simplicity of ISTA but with a global rate of convergence which is proven to be significantly better, both theoretically and practically. Initial promising nu- merical results for wavelet-based image deblurring demonstrate the capabilities of FISTA which is shown to be faster than ISTA by several orders of magnitude. Key',\n",
       "  'Beck2009'),\n",
       " (485,\n",
       "  '5FHLXDFU',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'The Agricultural Ingeneer Dr. Teruo Higa, professor of Horticulture at the University of The Ryukyus in Okinawa, Japan creates a technique in the 80th de- cade related with the use of efficient microorganism. This technology is the basis of the present review aim at providing information about groups of benevolent microorganisms such as: lactic acid bacteria, photo- trophic bacteria, actinomycetes group, yeast group and fungi present in natural ecosystems which are physiologically compatible with each other. Efficient Microorganisms, as a microbial inoculantion, resto- re soil microbiological balance, improve its physical and chemical conditions, increase crop production and protection, preserve natural resources, and ge- nerate a more sustainable agriculture and environ- ment. They can be used in the livestock (cattle, por- ciculture and poultry) for animal husbandry and the increase of productive variables. All this maximizes the efficiency of the systems and the management of excreta and facilities.',\n",
       "  'WalterGautschi2012'),\n",
       " (487,\n",
       "  '9FRTCCVW',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'It is now well-known that one can reconstruct sparse or compressible signals accurately from a very limited number of measurements, possibly contaminated with noise. This technique known as \"compressed sensing\" or \"compressive sampling\" relies on properties of the sensing matrix such as the restricted isometry property. In this Note, we establish new results about the accuracy of the reconstruction from undersampled measurements which improve on earlier estimates, and have the advantage of being more elegant. To cite this article: E.J. Candès, C. R. Acad. Sci. Paris, Ser. I 346 (2008). © 2008 Académie des sciences.',\n",
       "  'Candes2008'),\n",
       " (491,\n",
       "  'E3I2UYZD',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'We show that various inverse problems in signal recovery can be formulated as the generic problem of minimizing the sum of two convex functions with certain regularity properties. This formulation makes it possible to derive existence, uniqueness, characterization, and stability results in a unified and standardized fashion for a large class of apparently disparate problems. Recent results on monotone operator splitting methods are applied to establish the convergence of a forward-backward algorithm to solve the generic problem. In turn, we recover, extend, and provide a simplified analysis for a variety of existing iterative methods. Applications to geometry/texture image decomposition schemes are also discussed. A novelty of our framework is to use extensively the notion of a proximity operator, which was introduced by Moreau in the 1960s. © 2005 Society for Industrial and Applied Mathematics.',\n",
       "  'Combettes2005'),\n",
       " (493,\n",
       "  'BNY6DVKZ',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'In this paper, we investigate an L 0 optimization problem with constraints in a form of Volterra integral equation and the L ∞ norm. In particular, the equivalence theorem among the L p optimizations with pϵ [0, 1] is derived, which provides the following twofold extension of the existing results: First, these theoretical results enable us to solve sparse optimal control problems without imposing the finite dimensionality of the system to be controlled, which was the crucial assumption for the derivation of the existing results. Second, the relationship between the partial state constrained problem and output controllability is newly characterized.',\n",
       "  'Ikeda2019'),\n",
       " (495,\n",
       "  'HG8KKJXW',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'A reinforcement learning algorithm is proposed in order to solve a multi-criterion Markov decision process, i.e., an MDP with a vector running cost. Specifically, it combines a Q-learning scheme for a weighted linear combination of the prescribed running costs with an incremental version of replicator dynamics that updates the weights. The objective is that the time averaged vector cost meets prescribed asymptotic bounds. Under mild assumptions, it is shown that the scheme achieves the desired objective.',\n",
       "  'Shah2018a'),\n",
       " (499,\n",
       "  '7FKHG94Q',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Four well-known multicriteria decision methods are described in this paper, and illustrated with a simple example and are then shown to be fundamentally related by appropriate transformation from paired comparison data. Finally, two sets of several criteria each are given so that researchers and users can use them to evaluate the merits and shortcomings of each of these methods. © 2003 Elsevier Science Ltd. All rights reserved.',\n",
       "  'Cho2003'),\n",
       " (501,\n",
       "  'IFISIJDE',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'In recent years, the employment of multiple criteria decision analysis (MCDA) techniques in solving complex real-world problems has increased exponentially. The willingness to build advanced decision models, with higher capabilities to support decision making in a wide range of applications, promotes the integration of MCDA techniques with efficient systems such as intelligence and expert systems, geographic information systems, etc. Amongst the most applied MCDA techniques are Analytic Hierarchy Process (AHP) and Technique for Order of Preference by Similarity to Ideal Solution (TOPSIS). The development of a comprehensive perspective on research activities associated with the applications of these methods provides insights into the contributions of countries, institutes, authors and journals towards the advancements of these methods. Furthermore, it helps in identifying the status and trends of research. This in turn will help researchers in shaping up and improving future research activities and investments. To meet these aims, a bibliometric analysis based on data harvested from Scopus database was carried out to identify a set of bibliometric performance indicators (i.e. quantitative indicators such as productivity, and qualitative indicators such as citations and Hirsch index (h-index)). Additionally, bibliometric visualization maps were employed to identify the hot spots of research. The total research output was 10,188 documents for AHP and 2412 documents for TOPSIS. China took a leading position in AHP research (3513 documents; 34.5%). It was also the leading country in TOPSIS research (846 documents; 35.1%). The most collaborated country in AHP research was the United States, while in case of TOPSIS it was China. The United States had gained the highest h-index (78) in AHP research, while in TOPSIS it was Taiwan with h-index of 46. Expert Systems with Applications journal was the most productive journal in AHP (204; 2.0%) and TOPSIS research (125; 5.2%), simultaneously. University of Tehran, Iran and Islamic Azad University, Iran were the most productive institutions in AHP (173; 1.7%) and TOPSIS (115; 4.8%) research, simultaneously. The major hot topics that utilized AHP and will continue to be active include different applications of geographic information systems, risk modeling and supply chain management. While for TOPSIS, they are supply chain management and sustainability research. Overall, this analysis has shown increasing recognition of powerful of MCDA techniques to support strategic decisions. The efficacy of these methods in the previous context promotes their progress and advancements.',\n",
       "  'Zyoud2017'),\n",
       " (505,\n",
       "  'TFA9C9J3',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'In the field of complex networks, how to identify influential nodes in the network is still an important research topic. In this paper, a method to identify the influence of the node based on Analytic Hierarchy Process (AHP) is proposed. AHP, as a multiple attribute decision making (MADM) technique has become an important branch of decision making since then. Every centrality measure has its own disadvantages and limitations, thus we consider several different centrality measures as the multi-attribute of complex network in AHP application. AHP is used to aggregate the multi-attribute to obtain the evaluation of the influence of each node. The experiments on four real networks and an informative network show the efficiency and practicability of the proposed method.',\n",
       "  'Bian2017'),\n",
       " (507,\n",
       "  '7GYZYRFR',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'In a paper published in Management Science in 1984, Korhonen, Wallenius, and Zionts presented the idea and method based on convex-cone dominance in the discrete Multiple Criteria Decision Making framework. In our current paper, we revisit the old idea from a new standpoint and provide the mathematical theory leading to a dual-cone based approach to solving such problems. Our paper makes the old results computationally more tractable. The results provided in the present paper also help extend the theory.',\n",
       "  'Korhonen2016'),\n",
       " (509,\n",
       "  'E7PULVTQ',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'This paper presents a summary of the discrete mathematical part of my work, the Analytic Hierarchy Process (AHP) and its generalization to dependence and feedback, the Analytic Network Process (ANP), for measuring tangible and intangible factors, particularly as applied to decision making. The factors of the decision are arranged in hierarchical or network structures and judgments are then made by the decision maker, or by an expert, about the dominant element for each pair with respect to a common property. From simple judgments on two elements at a time with respect to a common property, priority vectors are obtained that are combined throughout the structure to give the best outcome for a decision. The judgments may be inconsistent, and there is a mathematical way to measure inconsistency so that the outlying judgments may be revised by the decision maker in an acceptable way or a decision may be delayed until more consistent information is obtained. In practical applications using either hierarchical or network structures, decisions are often analyzed in separate parts for their benefits, opportunities, costs, and risks, and the results are then combined in an appropriate way into an overall synthesis of those priorities. The mathematics has been generalized in the literature to the Neural Network Process (NNP), the continuous case for modeling how the brain synthesizes signals. There has been a diversity of applications over the past 30 to 40 years, and some of these are reported here. A brief mention is made of other methods of decision making and how AHP/ANP may compare with them. © 2013 INFORMS.',\n",
       "  'Saaty2013'),\n",
       " (511,\n",
       "  'SRTWUGVT',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'The Kaczmarz method for solving a linear system $Ax = b$ interprets such a system as a collection of equations $\\\\left\\\\langle a_i, x\\\\right\\\\rangle = b_i$, where $a_i$ is the $i-$th row of $A$, then picks such an equation and corrects $x_{k+1} = x_k + \\\\lambda a_i$ where $\\\\lambda$ is chosen so that the $i-$th equation is satisfied. Convergence rates are difficult to establish. Assuming the rows to be normalized, $\\\\|a_i\\\\|_{\\\\ell^2}=1$, Strohmer \\\\& Vershynin established that if the order of equations is chosen at random, $\\\\mathbb{E}~ \\\\|x_k - x\\\\|_{\\\\ell^2}$ converges exponentially. We prove that if the $i-$th row is selected with likelihood proportional to $\\\\left|\\\\left\\\\langle a_i, x_k \\\\right\\\\rangle - b_i\\\\right|^{p}$, where $0<p<\\\\infty$, then $\\\\mathbb{E}~\\\\|x_k - x\\\\|_{\\\\ell^2}$ converges faster than the purely random method. As $p \\\\rightarrow \\\\infty$, the method de-randomizes and explains, among other things, why the maximal correction method works well. We empirically observe that the method computes approximations of small singular vectors of $A$ as a byproduct.',\n",
       "  'Steinerberger2021'),\n",
       " (513,\n",
       "  'V67NRB5K',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'In this paper it is shown that the principal eigenvector is a necessary representation of the priorities derived from a positive reciprocal pairwise comparison judgment matrix A=(aij) when A is a small perturbation of a consistent matrix. When providing numerical judgments, an individual attempts to estimate sequentially an underlying ratio scale and its equivalent consistent matrix of ratios. Near consistent matrices are essential because when dealing with intangibles, human judgment is of necessity inconsistent, and if with new information one is able to improve inconsistency to near consistency, then that could improve the validity of the priorities of a decision. In addition, judgment is much more sensitive and responsive to large rather than to small perturbations, and hence once near consistency is attained, it becomes uncertain which coefficients should be perturbed by small amounts to transform a near consistent matrix to a consistent one. If such perturbations were forced, they could be arbitrary and thus distort the validity of the derived priority vector in representing the underlying decision. © 2002 Elsevier Science B.V. All rights reserved.',\n",
       "  'Saaty2003'),\n",
       " (515,\n",
       "  '8SLY8AR8',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Principal component analysis (PCA) is one of the most powerful tools for analyzing matrices in machine learning. In this paper, we study methods to accelerate power iteration in the stochastic setting by adding a momentum term. While in the deterministic setting, power iteration with momentum has optimal iteration complexity, we show that naively adding momentum to a stochastic method does not always result in acceleration. We perform a novel, tight variance analysis that reveals a “breaking-point variance” beyond which this acceleration does not occur. Combining this insight with modern variance reduction techniques yields a simple version of power iteration with momentum that achieves the optimal iteration complexities in both the online and oine setting. Our methods are embarrassingly parallel and can produce wall-clock-time speedups. Our approach is very general and applies to many non-convex optimization problems that can now be accelerated using the same technique.',\n",
       "  'Xu2018'),\n",
       " (517,\n",
       "  'SJSRIGG4',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  \"A lot of research has been devoted to the critical analysis of the Analytic Hierarchy Process (AHP), from various perspectives. However, as far as we know, no one has addressed a fundamental problem, discussed in this paper, concerning the meaning of the priority vector derived from the principal eigenvalue method used in AHP. The role of AHP's consistency ratio is also analysed. © 2006 Elsevier B.V. All rights reserved.\",\n",
       "  'BanaECosta2008'),\n",
       " (519,\n",
       "  'I323E2P2',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Although many papers describe the evolution of the analytic hierarchy process (AHP), most adopt a subjective approach. This paper examines the pattern of development of the AHP research field using social network analysis and scientometrics, and identifies its intellectual structure. The objectives are: (i) to trace the pattern of development of AHP research; (ii) to identify the patterns of collaboration among authors; (iii) to identify the most important papers underpinning the development of AHP; and (iv) to discover recent areas of interest. We analyse two types of networks: social networks, that is, co-authorship networks, and cognitive mapping or the network of disciplines affected by AHP. Our analyses are based on 8441 papers published between 1979 and 2017, retrieved from the ISI Web of Science database. To provide a longitudinal perspective on the pattern of evolution of AHP, we analyse these two types of networks during the three periods 1979–1990, 1991–2001 and 2002–2017. We provide some basic statistics on AHP journals and researchers, review the main topics and applications of integrated AHPs and provide direction for future research by highlighting some open questions.',\n",
       "  'Emrouznejad2017'),\n",
       " (521,\n",
       "  '82ZMDNIX',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Analytic hierarchy process (AHP) has been considerably criticized for possible rank reversal phenomenon caused by the addition or deletion of an alternative. This paper looks into the cause of rank reversal phenomenon and finds that rank reversal is caused by change of local priorities before and after an alternative is added or deleted. An approach is therefore proposed to keep the local priorities unchanged to avoid rank reversal phenomenon. Two well-known numerical examples are re-examined using the proposed approach to demonstrate its validity and practicability in rank preservation. © 2005 Elsevier B.V. All rights reserved.',\n",
       "  'Wang2006'),\n",
       " (523,\n",
       "  '6KLGI39Z',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  \"In the last twenty years many features of Saaty's Analytic Hierachy Process (AHP) have been criticised, especially the additive hierarchical composition of con- ventionM AHP, which leads to the possibility of occurrence of' the Rank Reversal phenomenon (adding art irrelevant alternative may cause a reversal in the ranking at the top). In this paper we show another feature of AHP which may be, and in many application contexts will indeed be, an even stronger shortcoming of the method. It consists in the fact that the addition of indifferent criteria (for which all alternatives perform equally) causes a significant alteration of the aggregated priorities of alternatives, with important consequences. In hierarchies with four or more levels, rank reversal may happen. Since in almost all applications of AHP the set of criteria is not fixed ex-ante but is variable and is constructed in accor- dance with reasons of :relevance and simplicity, almost all applications of AHP are potentially flawed.\",\n",
       "  'Perez2006'),\n",
       " (525,\n",
       "  'HM8M7WRJ',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Identifying the most influential spreaders in complex networks is crucial for optimally using the network structure and designing efficient strategies to accelerate information dissemination or prevent epidemic outbreaks. In this paper, by taking into account the centrality of a node and its neighbors’ centrality which depends on the diffusion importance of links, we propose a novel influence measure, the weight neighborhood centrality, to quantify the spreading ability of nodes in complex networks. To evaluate the performance of our method, we use the Susceptible–Infected–Recovered (SIR) model to simulate the epidemic spreading process on six real-world networks and four artificial networks. By measuring the rank imprecision and the rank correlation between the rank lists generated by simulation results via SIR and the ones generated by centrality measures, it shows that in general the weight neighborhood centrality can rank the spreading ability of nodes more accurately than its benchmark centrality, especially when using the degree k or coreness ks as the benchmark centrality. Further, we compare the monotonicity and the computational complexity of different ranking methods, which show that our method not only can be better at distinguishing the spreading ability of nodes but also can be used in large-scale networks due to the high computation efficiency.',\n",
       "  'Wang2017'),\n",
       " (527,\n",
       "  'P8SC88WX',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Measuring node importance in complex networks has great theoretical and practical significance for network stability and robustness. A variety of network centrality criteria have been presented to address this problem, but each of them focuses only on certain aspects and results in loss of information. Therefore, this paper proposes a relatively comprehensive and effective method to evaluate node importance in complex networks using a multicriteria decision-making method. This method not only takes into account degree centrality, closeness centrality, and betweenness centrality, but also uses an entropy weighting method to calculate the weight of each criterion, which can overcome the influence of the subjective factor. To illustrate the effectiveness and feasibility of the proposed method, four experiments were conducted to rank node importance on four real networks. The experimental results showed that the proposed method can rank node importance more comprehensively and accurately than a single centrality criterion.',\n",
       "  'Yang2019'),\n",
       " (529,\n",
       "  'K3G2NVWK',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Systemic decision making is a new approach for dealing with complex multiactor decision making problems in which the actors’ individual preferences on a fixed set of alternatives are incorporated in a holistic view in accordance with the “principle of tolerance”. The new approach integrates all the preferences, even if they are encapsulated in different individual theoretical models or approaches; the only requirement is that they must be expressed as some kind of probability distribution. In this paper, assuming the analytic hierarchy process (AHP) is the multicriteria technique employed to rank alternatives, the authors present a new methodology based on a Bayesian analysis for dealing with AHP systemic decision making in a local context (a single criterion). The approach integrates the individual visions of reality into a collective one by means of a tolerance distribution, which is defined as the weighted geometric mean of the individual preferences expressed as probability distributions. A mathematical justification of this distribution, a study of its statistical properties and a Monte Carlo method for drawing samples are also provided. The paper further presents a number of decisional tools for the evaluation of the acceptance of the tolerance distribution, the construction of tolerance paths that increase representativeness and the extraction of the relevant knowledge of the subjacent multiactor decisional process from a cognitive perspective. Finally, the proposed methodology is applied to the AHP-multiplicative model with lognormal errors and a case study related to a real-life experience in local participatory budgets for the Zaragoza City Council (Spain).',\n",
       "  'Moreno-jimenez2016'),\n",
       " (531,\n",
       "  'VFP28VF9',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'The POCS-method (projection onto convex subsets) has been proposed as an efficient way of recovering a band-limited signal from irregular sampling values. However, both the ordinary POCS-method (which uses one sampling point at a given time, i.e. consists of a succession of projections onto affine hyperplanes) and the one-step method (which uses all sampling values at the same time) become extremely slow if the number of sampling points gets large. Already for midsize 2D-problems (e.g. 128 X 128 images) one may easy run into memory problems. Based on the theory of pseudo-inverse matrices new efficient variants of the POCS- method (so to say intermediate versions) are described, which make use of a finite number of sampling points at each step. Depending on the computational environment appropriate strategies of designing those families of sampling points (either many families with few points, or few families with many points, overlapping families or disjoint ones...) have to be found. We also report on numerical results for these algorithms.',\n",
       "  'Feichtinger1992'),\n",
       " (534,\n",
       "  'W4QW6JRQ',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Three iterative procedures for approximating to the solutions of linear eigenvalue problems for systems with a finite number of degrees of freedom are discussed. Two of the procedures are closely related to known iterative procedures while the third is original. The procedures are shown to possess quadratic, geometric and cubic convergence. All three procedures lie within the framework of the relaxation method, each representing a particular manner of fixing the freedom of choice existent in the relaxation method. The study was made to investigate the convergence and behaviour in the large of the relaxation method and to provide guiding principles for the relaxation computer. One particular result of importance is that orthogonalization of trial modes is not essential to the success of the relaxation method.',\n",
       "  'Crandall1951'),\n",
       " (538,\n",
       "  'HLCYPHXQ',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Randomized Kaczmarz is a simple iterative method for finding solutions of linear systems Ax = b. We point out that the arising sequence (xk) ∞k =1 tends to converge to the solution x in an interesting way: Generically, as k → ∞, xk -x tends to the singular vector of A corresponding to the smallest singular value. This has interesting consequences: In particular, the error analysis of Strohmer and Vershynin is optimal. It also quantifles the \"preconvergence\"phenomenon where the method initially seems to converge faster. This fact also allows for a fast computation of vectors x for which the Rayleigh quotient ∥Ax∥= ∥x∥ is small: Solve Ax = 0 via randomized Kaczmarz.',\n",
       "  'Steinerberger2021a'),\n",
       " (540,\n",
       "  'CQ7HWCU7',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'This thorough, concise, and superbly written volume is the first in a self-contained five-volume series devoted to matrix algorithms. It focuses on the computation of matrix decompositions - the factorization of matrices into products of similar ones. The first two chapters provide the required background from mathematics and computer science needed to work effectively in matrix computations. The remaining chapters are devoted to the computation and applications of the LU and QR decompositions. The series is aimed at the nonspecialist who needs more than black-box proficiency with matrix computations. A certain knowledge of elementary analysis and linear algebra is assumed, as well as a reasonable amount of programming experience. The guiding principle, that if something is worth explaining, it is worth explaining fully, has necessarily restricted the scope of the series, but the selection of topics should give the reader a sound basis for further study.',\n",
       "  'Stewart1999'),\n",
       " (542,\n",
       "  'R25M9S8R',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'This book, Basic Decompositions, is the first volume in a projected five-volume series entitled Matrix Algorithms. The other four volumes will treat eigensystems, iterative methods for linear systems, sparse direct methods, and special topics, including fast algorithms for structured matrices. My intended audience is the nonspecialist whose needs cannot be satisfied by black boxes. It seems to me that these people will be chiefly interested in the methods themselves — how they are derived and how they can be adapted to particular problems. Consequently, the focus of the series is on algorithms, with such topics as rounding-error analysis and perturbation theory introduced impromptu as needed. My aim is to bring the reader to the point where he or she can go to the research literature to augment what is in the series. The series is self-contained. The reader is assumed to have a knowledge of elementary analysis and linear algebra and a reasonable amount of programming experience — about what you would expect from a beginning graduate engineer or an undergraduate in an honors program. Although strictly speaking the individual volumes are not textbooks, they are intended to teach, and my guiding principle has been that if something is worth explaining it is worth explaining fully. This has necessarily restricted the scope of the series, but I hope the selection of topics will give the reader a sound basis for further study. The focus of this and part of the next volume will be the computation of matrix decompositions — that is, the factorization of matrices into products of simpler ones. This decompositional approach to matrix computations is relatively new: it achieved its definitive form in the early 1960s, thanks to the pioneering work of Alston Householder and James Wilkinson. Before then, matrix algorithms were addressed to specific problems—the solution of linear systems, for example — and were presented at the scalar level in computational tableaus. The decompositional approach has two advantages. First, by working at the matrix level it facilitates the derivation and analysis of matrix algorithms. Second, by deemphasizing specific problems, the approach turns the decomposition into a computational platform from which a variety of problems can be solved. Thus the initial cost of computing a decomposition can pay for itself many times over. In this volume we will be chiefly concerned with the LU and the QR decompositions along with certain two-sided generalizations.',\n",
       "  'Stewart1998'),\n",
       " (544,\n",
       "  'BXLPZVV6',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Buildings with intermittent occupancy may not perform thermally the same as typical commercial and residential facilities. Thermal comfort requirements require careful envelope design coupled with the appropriate air-conditioning system operation strategies. One of the most prominent examples of such buildings is mosques. Mosques are usually occupied five intermittent times day and night all year round. Like any other building, they have to be mechanically air-conditioned to achieve the required thermal comfort for worshippers especially in harsh climatic regions. This paper describes the physical and operating characteristics typical for the intermittently occupied mosques as well as the results of the thermal optimization of a medium size mosque in the two hot-dry and hot-humid Saudi Arabian cities of Riyadh and Jeddah. The analysis utilizes a direct search optimization technique that is coupled to an hourly energy simulation program. Based on that, design guidelines are presented for the optimum thermal performance of mosques in these two cities in addition to other design and operating factors that need to be considered for mosques in general. © 2009 The Author(s).',\n",
       "  'Pelillo2012'),\n",
       " (546,\n",
       "  'HZTU3C3U',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Kaczmarz method is one popular iterative method for solving inverse problems, especially in computed tomography. Recently, it was established that a randomized version of the method enjoys an exponential convergence for well-posed problems, and the convergence rate is determined by a variant of the condition number. In this work, we analyze the preasymptotic convergence behavior of the randomized Kaczmarz method, and show that the low-frequency error (with respect to the right singular vectors) decays faster during first iterations than the high-frequency error. Under the assumption that the initial error is smooth (e.g. sourcewise representation), the result explains the fast empirical convergence behavior, thereby shedding new insights into the excellent performance of the randomized Kaczmarz method in practice. Further, we propose a simple strategy to stabilize the asymptotic convergence of the iteration by means of variance reduction. We provide extensive numerical experiments to confirm the analysis and to elucidate the behavior of the algorithms.',\n",
       "  'Jiao2017'),\n",
       " (548,\n",
       "  'TPFVSV5G',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Stochastic gradient descent (SGD) and its variants are among the most successful approaches for solving large-scale optimization problems. At each iteration, SGD employs an unbiased estimator of the full gradient computed from one single randomly selected data point. Hence, it scales well with problem size and is very attractive for handling truly massive dataset, and holds significant potentials for solving large-scale inverse problems. In this work, we rigorously establish its regularizing property under a priori early stopping rule for linear inverse problems, and also prove convergence rates under the canonical sourcewise condition. This is achieved by combining tools from classical regularization theory and stochastic analysis. Further, we analyze its preasymptotic weak and strong convergence behavior, in order to explain the fast initial convergence typically observed in practice. The theoretical findings shed insights into the performance of the algorithm, and are complemented with illustrative numerical experiments.',\n",
       "  'Jin2019'),\n",
       " (550,\n",
       "  '7FCS6PNW',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'We consider data in the form of pairwise comparisons of n items, with the goal of identifying the top k items for some value of k < n, or alternatively, recovering a ranking of all the items. We analyze the Borda counting algorithm that ranks the items in order of the number of pairwise comparisons won, and show it has three attractive features: (a) it is an optimal method achieving the information-theoretic limits up to constant factors; (b) it is robust in that its optimality holds without imposing conditions on the underlying matrix of pairwise-comparison probabilities, in contrast to some prior work that applies only to the BTL parametric model; and (c) its computational efficiency leads to speed-ups of several orders of magnitude. We address the problem of exact recovery, and for the top-k recovery problem we also extend our results to obtain sharp guarantees for approximate recovery under the Hamming distortion metric, and more generally, to any arbitrary error requirement that satisfies a simple and natural monotonicity condition. In doing so, we introduce a general framework that allows us to treat a variety of problems in the literature in an unified manner.',\n",
       "  'Shah2018'),\n",
       " (556,\n",
       "  'ZMN6RFF5',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'A common problem in machine learning is to rank a set of n items based on pairwise comparisons. Here ranking refers to partitioning the items into sets of pre-specified sizes according to their scores, which includes identification of the top-k items as the most prominent special case. The score of a given item is defined as the probability that it beats a randomly chosen other item. Finding an exact ranking typically requires a prohibitively large number of comparisons, but in practice, approximate rankings are often adequate. Accordingly, we study the problem of finding approximate rankings from pairwise comparisons. We analyze an active ranking algorithm that counts the number of comparisons won, and decides whether to stop or which pair of items to compare next, based on confidence intervals computed from the data collected in previous steps. We show that this algorithm succeeds in recovering approximate rankings using a number of comparisons that is close to optimal up to logarithmic factors. We also present numerical results, showing that in practice, approximation can drastically reduce the number of comparisons required to estimate a ranking.',\n",
       "  'Heckel2018'),\n",
       " (562,\n",
       "  'JX3BNSI3',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Mathematics of multi objective optimization edited by P. Serafini （CISM courses and lectures, no. 289） New York ; Springer, c1985 : Wien : New York',\n",
       "  'Centre1985'),\n",
       " (564,\n",
       "  'FQV6EMP5',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'In the paper an algorithm for the linear bilevel programming problem is offered which applies mainly the usual simplex method with an additional rule for including slack variables into the basis. The algorithm bases on the full description of the feasible set in a neighbourhood of a feasible point. This description is obtained using the theory of subgradients as well as the concept of “active constraints”. The result is an algorithm which seems to be easier to implement as other published procedures also based on the theorem that every solvable linear bilevel programming problem has a basic solution. © 1987, Taylor & Francis Group, LLC. All rights reserved.',\n",
       "  'Dempe1987'),\n",
       " (568,\n",
       "  '57F2NNHD',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'A Legendre transform is a procedure for expressing the information content of some function by using a different independent variable, namely, the derivative of this function with respect to (one of) its argument(s). These notes explain how this is done and why simply performing some sort of algebraic substitution instead would destroy information.',\n",
       "  'Debnath2020'),\n",
       " (570,\n",
       "  'M5ZNE2YE',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  \"The concave-convex procedure (CCCP) is an iterative algorithm that solves d.c. (difference of convex functions) programs as a sequence of convex programs. In machine learning,CCCP is extensively used in many learning algorithms, including sparse support vector machines (SVMs), transductive SVMs, and sparse principal component analysis. Though CCCP is widely used in many applications, its convergence behavior has not gotten a lot of specific attention. Yuille and Rangarajan analyzed its convergence in their original paper; however, we believe the analysis is not complete. The convergence of CCCP can be derived from the convergence of the d.c. algorithm (DCA), proposed in the global optimization literature to solve general d.c. programs, whose proof relies on d.c. duality. In this note, we follow a different reasoning and show how Zangwill's global convergence theory of iterative algorithms provides a natural framework to prove the convergence of CCCP. This underlines Zangwill's theory as a powerful and general framework to deal with the convergence issues of iterative algorithms, after also being used to prove the convergence of algorithms like expectation-maximization and generalized alternating minimization. In this note, we provide a rigorous analysis of the convergence of CCCP by addressing two questions:When does CCCP find a local minimum or a stationary point of the d.c. program under consideration? and when does the sequence generated by CCCP converge? We also present an open problem on the issue of local convergence of CCCP.\",\n",
       "  'Sriperumbudur2012'),\n",
       " (574,\n",
       "  'UZMJ8VMK',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'We consider the problem of decomposing a multivariate polynomial as the difference of two convex polynomials. We introduce algebraic techniques which reduce this task to linear, second order cone, and semidefinite programming. This allows us to optimize over subsets of valid difference of convex decompositions (dcds) and find ones that speed up the convex–concave procedure. We prove, however, that optimizing over the entire set of dcds is NP-hard.',\n",
       "  'Ahmadi2018'),\n",
       " (576,\n",
       "  'W5LFHVT6',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  \"The concave-convex procedure (CCCP) is a majorization-minimization algorithm that solves d.c. (difference of convex functions) programs as a sequence of convex programs. In machine learning, CCCP is extensively used in many learning algorithms like sparse support vector machines (SVMs), transductive SVMs, sparse principal component analysis, etc. Though widely used in many applications, the convergence behavior of CCCP has not gotten a lot of specific attention. Yuille and Rangarajan analyzed its convergence in their original paper, however, we believe the analysis is not complete. Although the convergence of CCCP can be derived from the convergence of the d.c. algorithm (DCA), its proof is more specialized and technical than actually required for the specific case of CCCP. In this paper, we follow a different reasoning and show how Zangwill's global convergence theory of iterative algorithms provides a natural framework to prove the convergence of CCCP, allowing a more elegant and simple proof. This underlines Zangwill's theory as a powerful and general framework to deal with the convergence issues of iterative algorithms, after also being used to prove the convergence of algorithms like expectation- maximization, generalized alternating minimization, etc. In this paper, we provide a rigorous analysis of the convergence of CCCP by addressing these questions: (i) When does CCCP find a local minimum or a stationary point of the d.c. program under consideration? (ii) When does the sequence generated by CCCP converge? We also present an open problem on the issue of local convergence of CCCP.\",\n",
       "  'Sriperumbudur2009'),\n",
       " (578,\n",
       "  'LUQX94JI',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'The main goal of this manuscript is to introduce a discrete dynamical system defined by symmetric matrices and a real parameter. By construction, we rediscovery the Power Iteration Method from the Projected Gradient Method. Convergence of the discrete dynamical system solution is established. Finally, we consider two applications, the first one consists in find a solution of non linear equation problem and the other one consists in verifies the optimality conditions when we solve quadratic optimization problems over linear equality constraints.',\n",
       "  'Carrasco-gutierrez2019'),\n",
       " (580,\n",
       "  'M7S7S55S',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'We investigate the convex–concave procedure, a local heuristic that utilizes the tools of convex optimization to find local optima of difference of convex (DC) programming problems. The class of DC problems includes many difficult problems such as the traveling salesman problem. We extend the standard procedure in two major ways and describe several variations. First, we allow for the algorithm to be initialized without a feasible point. Second, we generalize the algorithm to include vector inequalities. We then present several examples to demonstrate these algorithms.',\n",
       "  'Lipp2016'),\n",
       " (582,\n",
       "  'CRLGH2XJ',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Many neural networks can be derived as optimization dynamics for suitable objective functions. We show that such networks can be designed by repeated transformations of one objective into another with the same fixpoints. We exhibit a collection of algebraic transformations which reduce network cost and increase the set of objective functions that are neurally implementable. The transformations include simplification of products of expressions, functions of one or two expressions, and sparse matrix products (all of which may be interpreted as Legendre transformations); also the minimum and maximum of a set of expressions. These transformations introduce new interneurons which force the network to seek a saddle point rather than a minimum. Other transformations allow control of the network dynamics, by reconciling the Lagrangian formalism with the need for fixpoints. We apply the transformations to simplify a number of structured neural networks, beginning with the standard reduction of the winner-take-all network from ∂(N2) connections to ∂(N). Also susceptible are inexact graph-matching, random dot matching, convolutions and coordinate transformations, and sorting. Simulations show that fixpoint-preserving transformations may be applied repeatedly and elaborately, and the example networks still robustly converge. © 1990.',\n",
       "  'Mjolsness1990'),\n",
       " (584,\n",
       "  'R6HPJ6C5',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'We present a novel optimizing network architecture with applications in vision, learning, pattern recognition, and combinatorial optimization. This architecture is constructed by combining the following techniques: (1) deterministic annealing, (2) self-amplification, (3) algebraic transformations, (4) clocked objectives, and (5) softassign. Deterministic annealing in conjunction with self-amplification avoids poor local minima and ensures that a vertex of the hypercube is reached. Algebraic transformations and clocked objectives help partition the relaxation into distinct phases. The problems considered have doubly stochastic matrix constraints or minor variations thereof. We introduce a new technique, softassign, which is used to satisfy this constraint. Experimental results on different problems are presented and discussed.',\n",
       "  'Rangarajan1996'),\n",
       " (588,\n",
       "  'F7I8V8EZ',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Multi-level programming is characterized as mathematical programming to solve decentralized planning problems. The decision variables are partitioned among ordered levels. A decision-maker at one level of the hierarchy may have his own objective function and decision space, but may be influenced by other levels. During the last 10 years, a special case of the multi-level programming problem, the linear bi-level programming (BLP) problem, has been studied with increasing interest in the area of mathematical programming problems. This paper attempts to review the literature on the linear BLP problems. It presents the basic models and the characterizations of the problem, the areas for application, the existing solution approaches, and the related models and areas for further research. © 1991, Operational Research Society Ltd.',\n",
       "  'WEn1991'),\n",
       " (590, 'RPU6LD4M', 2, 'abstractNote', 'juikhhuihuio', 'Yeh1996'),\n",
       " (592,\n",
       "  'EAHRA8M8',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'This paper provides an introductory survey of a class of optimization problems known as bilevel programming. We motivate this class through a simple application, and then proceed with the general formulation of bilevel programs. We consider various cases (linear, linear-quadratic, nonlinear), describe their main properties and give an overview of solution approaches. © Springer-Verlag 2005.',\n",
       "  'Colson2005'),\n",
       " (603,\n",
       "  '253SZQ85',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'In this paper, we study the relationship between bilevel optimization and multicriteria optimization. Given a bilevel optimization problem, we introduce an order relation such that the optimal solutions of the bilevel problem are the nondominated points with respect to the order relation. In the case where the lower-level problem of the bilevel optimization problem is convex and continuously differentiable in the lower-level variables, this order relation is equivalent to a second, more tractable order relation. Then, we show how to construct a (nonconvex) cone for which we can prove that the nondominated points with respect to the order relation induced by the cone are also nondominated points with respect to any of the two order relations mentioned before. We comment also on the practical and computational implications of our approach. © 2006 Springer Science + Business Media, Inc.',\n",
       "  'Fliege2006'),\n",
       " (606,\n",
       "  'ZF6XYRPG',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'An algorithm is presented using sensitivity analysis to solve a linear two-stage optimization problem. The underlying theory rests on a set of first order optimality conditions that parallel the Kuhn-Tucker conditions associated with a one-dimensional parametric linear program. The solution to the original problem is uncovered by systematically varying the parameter over the unit interval and solving the corresponding linear program. Finite convergence is established under nondegenerate assumptions. A discussion is also presented of other solution techniques including branch and bound and vertex enumeration and gives an example highlighting their computational and storage requirements. By these measures, the algorithm presented has an overall advantage. Finally, a comparison is drawn between bicriteria and bilevel programming, and underscored by way of an example.',\n",
       "  'Bard1983'),\n",
       " (608,\n",
       "  'XSW4NMKH',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'We show that (L+1)-level linear programs are as difficult as level L of the polynomial-time hierarchy, even if one only considers problems with unique optimal solutions. © 1992 J.C. Baltzer AG, Scientific Publishing Company.',\n",
       "  'Blair1992'),\n",
       " (615,\n",
       "  'XICLQE69',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'In this note the relationship between linear bilevel and bicriteria programming problems is clarified. This paper points out that the bicriteria programming algorithm is not suitable for all bilevel programming problems in searching for optimal solutions. A counterexample is given to demonstrate the shortcomings of the results by Ünlü [1, Comput. Opns Res. 14, 173-179 (1987)] and Bard [2, Opns Res. 31, 670-684 (1983)]. We also propose a sufficient condition to use the bicriteria programming algorithm for solving a bilevel programming problem. © 1989.',\n",
       "  'Wen1989'),\n",
       " (618,\n",
       "  'TM8EZDRS',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'In this paper the relationship between bilevel and bicriteria programming is utilized to develop an algorithm for linear bilevel programming through the adaptation of a bicriteria programming algorithm proposed previously. Some computational results are also given. © 1987.',\n",
       "  'unlu1987'),\n",
       " (622,\n",
       "  'XGAGTYCV',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  \"The Bilinear Programming Problem is a structured quadratic programming problem whose objective function is, in general, neither convex nor concave. Making use of the formal linearity of a dual formulation of the problem, we give a necessary and sufficient condition for optimality, and an algorithm to find an optimal solution. 0. Introduction The Bilinear Programming Problem, in its general form, is to determine x, an n-vector, and y, an n'-vector, to maximize cTx + x-r QXy + dry, subject to Ax <-a, Bry <~ b, (0.1) x~>0, y~>0, where A is an m by n matrix, B T an rn' by n' matrix, QT an n by n' matrix, c, d, a and b are n, n', m m'-vectors respectively. We will assume that X = {x [ Ax <~ a, x ~> 0} and Y = {y I BTy <-b, y >1 0} are bounded and nonempty. It can be easily verified that the set of all optimal solutions of (0.1) contains at least one element (x*, y*), such that x* is a vertex of X and y* is a vertex of Y. It can be directly derived from the Duality Theory that (0.1) is equivalent to the problem of determining x, an n-vector, and u, an m'-vector, to maximize (crx + min b TU), subject to Ax<~a, B u > i d + Q x , (0.2) x~>0, u~>0. In this paper, we solve (0.2) directly. First, in Sections 1 and 2, some geometric properties of the solution set are determined, and a necessary and\",\n",
       "  'Gallo1977'),\n",
       " (628,\n",
       "  'N4F8UE57',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'The paper presents a unified framework to derive and analyze 10 different adaptive algorithms, some well-known, to compute the first principal eigenvector of the correlation matrix of a random vector sequence. Since adaptive principal eigenvector algorithms have originated from a diverse set of disciplines, including ad hoc methods, it is necessary to examine them in a unified framework. In a common framework consisting of five steps, we analyze the derivation, convergence, and rate results for many well-known algorithms as well as two new adaptive algorithms. In the process, we offer fresh perspectives on the known algorithms, and derive new results for others. The common framework also allows us to comparatively study the 10 algorithms. Finally, we show experimental results to support our analyses. © 2005 Elsevier Ltd. All rights reserved.',\n",
       "  'Chatterjee2005'),\n",
       " (630,\n",
       "  '3RZCATAN',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Microstructural heterogeneity affects the macro-scale behavior of materials. Conversely, load distribution at the macro-scale changes the microstructural response. These up-scaling and down-scaling relations are often modeled using multiscale finite element (FE) approaches such as FE-squared ($FE^2$). However, $FE^2$ requires numerous calculations at the micro-scale, which often renders this approach intractable. This paper reports an enormously faster machine learning (ML) based approach for multiscale mechanics modeling. The proposed ML-driven multiscale analysis approach uses an ML-model that predicts the local stress tensor fields in a linear elastic fiber-reinforced composite microstructure. This ML-model, specifically a U-Net deep convolutional neural network (CNN), is trained separately to perform the mapping between the spatial arrangement of fibers and the corresponding 2D stress tensor fields. This ML-model provides effective elastic material properties for up-scaling and local stress tensor fields for subsequent down-scaling in a multiscale analysis framework. Several numerical examples demonstrate a substantial reduction in computational cost using the proposed ML-driven approach when compared with the traditional multiscale modeling approaches such as full-scale FE analysis, and homogenization based $FE^2$ analysis. This approach has tremendous potential in efficient multiscale analysis of complex heterogeneous materials, with applications in uncertainty quantification, design, and optimization.',\n",
       "  'Gupta2022'),\n",
       " (633,\n",
       "  'F86H99HR',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Bilevel optimization is defined as a mathematical program, where an optimization problem contains another optimization problem as a constraint. These problems have received significant attention from the mathematical programming community. Only limited work exists on bilevel problems using evolutionary computation techniques; however, recently there has been an increasing interest due to the proliferation of practical applications and the potential of evolutionary algorithms in tackling these problems. This paper provides a comprehensive review on bilevel optimization from the basic principles to solution strategies; both classical and evolutionary. A number of potential application problems are also discussed. To offer the readers insights on the prominent developments in the field of bilevel optimization, we have performed an automated text-analysis of an extended list of papers published on bilevel optimization to date. This paper should motivate evolutionary computation researchers to pay more attention to this practical yet challenging area.',\n",
       "  'Sinha2018'),\n",
       " (635,\n",
       "  'LBNYQCYI',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'This paper introduces and studies the convergence properties of a new class of explicit ∈-subgradient methods for the task of minimizing a convex function over a set of minimizers of another convex minimization problem. The general algorithm specializes to some important cases, such as first-order methods applied to a varying objective function, which have computationally cheap iterations. We present numerical experimentation concerning certain applications where the theoretical framework encompasses efficient algorithmic techniques, enabling the use of the resulting methods to solve very large practical problems arising in tomographic image reconstruction.',\n",
       "  'Helou2017'),\n",
       " (637,\n",
       "  'YPVY67LG',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Bilevel optimization has been developed for many machine learning tasks with large-scale and high-dimensional data. This paper considers a constrained bilevel optimization problem, where the lower-level optimization problem is convex with equality and inequality constraints and the upper-level optimization problem is non-convex. The overall objective function is non-convex and non-differentiable. To solve the problem, we develop a gradient-based approach, called gradient approximation method, which determines the descent direction by computing several representative gradients of the objective function inside a neighborhood of the current estimate. We show that the algorithm asymptotically converges to the set of Clarke stationary points, and demonstrate the efficacy of the algorithm by the experiments on hyperparameter optimization and meta-learning.',\n",
       "  'Xu2023'),\n",
       " (639,\n",
       "  'RUGZRURD',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Bilevel optimization problems, which are problems where two optimization problems are nested, have more and more applications in machine learning. In many practical cases, the upper and the lower objectives correspond to empirical risk minimization problems and therefore have a sum structure. In this context, we propose a bilevel extension of the celebrated SARAH algorithm. We demonstrate that the algorithm requires $\\\\mathcal{O}((n+m)^{\\\\frac12}\\\\varepsilon^{-1})$ gradient computations to achieve $\\\\varepsilon$-stationarity with $n+m$ the total number of samples, which improves over all previous bilevel algorithms. Moreover, we provide a lower bound on the number of oracle calls required to get an approximate stationary point of the objective function of the bilevel problem. This lower bound is attained by our algorithm, which is therefore optimal in terms of sample complexity.',\n",
       "  'Dagreou2023'),\n",
       " (641,\n",
       "  '6EJKG24B',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Event or Neuromorphic cameras are novel biologically inspired sensors that record data based on the change in light intensity at each pixel asynchronously. They have a temporal resolution of microseconds. This is useful for scenes with fast moving objects that can cause motion blur in traditional cameras, which record the average light intensity over an exposure time for each pixel synchronously. This paper presents a bilevel inverse problem framework for neuromorphic imaging. Existence of solution to the inverse problem is established. Second order sufficient conditions are derived under special situations for this nonconvex problem. A second order Newton type solver is derived to solve the problem. The efficacy of the approach is shown on several examples.',\n",
       "  'Antil2023'),\n",
       " (643,\n",
       "  'WWPIMYFJ',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Bilevel Optimization Programming is used to model complex and conflicting interactions between agents, for example in Robust AI or Privacy-preserving AI. Integrating bilevel mathematical programming within deep learning is thus an essential objective for the Machine Learning community. Previously proposed approaches only consider single-level programming. In this paper, we extend existing single-level optimization programming approaches and thus propose Differentiating through Bilevel Optimization Programming (BiGrad) for end-to-end learning of models that use Bilevel Programming as a layer. BiGrad has wide applicability and can be used in modern machine learning frameworks. BiGrad is applicable to both continuous and combinatorial Bilevel optimization problems. We describe a class of gradient estimators for the combinatorial case which reduces the requirements in terms of computation complexity; for the case of the continuous variable, the gradient computation takes advantage of the push-back approach (i.e. vector-jacobian product) for an efficient implementation. Experiments show that the BiGrad successfully extends existing single-level approaches to Bilevel Programming.',\n",
       "  'Alesiani2023'),\n",
       " (645,\n",
       "  'K3KMVWJU',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'This paper considers a bilevel program, which has many applications in practice. To develop effective numerical algorithms, it is generally necessary to transform the bilevel program into a single-level optimization problem. The most popular approach is to replace the lower-level program by its KKT conditions and then the bilevel program can be reformulated as a mathematical program with equilibrium constraints (MPEC for short). However, since the MPEC does not satisfy the Mangasarian-Fromovitz constraint qualification at any feasible point, the well-developed nonlinear programming theory cannot be applied to MPECs directly. In this paper, we apply the Wolfe duality to show that, under very mild conditions, the bilevel program is equivalent to a new single-level reformulation (WDP for short) in the globally and locally optimal sense. We give an example to show that, unlike the MPEC reformulation, WDP may satisfy the Mangasarian-Fromovitz constraint qualification at its feasible points. We give some properties of the WDP reformulation and the relations between the WDP and MPEC reformulations. We further propose a relaxation method for solving WDP and investigate its limiting behavior. Comprehensive numerical experiments indicate that, although solving WDP directly does not perform very well in our tests, the relaxation method based on the WDP reformulation is quite efficient.',\n",
       "  'Li2023a'),\n",
       " (647,\n",
       "  'FA7WG6GX',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Distributionally robust optimization (DRO) and invariant risk minimization (IRM) are two popular methods proposed to improve out-of-distribution (OOD) generalization performance of machine learning models. While effective for small models , it has been observed that these methods can be vulnerable to overfitting with large overparam-eterized models. This work proposes a principled method, Model Agnostic samPLe rEweighting (MAPLE), to effectively address OOD problem, especially in overparameterized scenarios. Our key idea is to find an effective reweighting of the training samples so that the standard empirical risk minimization training of a large model on the weighted training data leads to superior OOD generalization performance. The overfitting issue is addressed by considering a bilevel formulation to search for the sample reweighting, in which the generalization complexity depends on the search space of sample weights instead of the model size. We present theoretical analysis in linear case to prove the insensitivity of MAPLE to model size, and empirically verify its superiority in surpassing state-of-the-art methods by a large margin.',\n",
       "  'Zhou2022'),\n",
       " (649,\n",
       "  'XDKFE4C6',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Fair resource allocation is one of the most important topics in communication networks. Existing solutions almost exclusively assume each user utility function is known and concave. This paper seeks to answer the following question: how to allocate resources when utility functions are unknown, even to the users? This answer has become increasingly important in the next-generation AI-aware communication networks where the user utilities are complex and their closed-forms are hard to obtain. In this paper, we provide a new solution using a distributed and data-driven bilevel optimization approach, where the lower level is a distributed network utility maximization (NUM) algorithm with concave surrogate utility functions, and the upper level is a data-driven learning algorithm to find the best surrogate utility functions that maximize the sum of true network utility. The proposed algorithm learns from data samples (utility values or gradient values) to autotune the surrogate utility functions to maximize the true network utility, so works for unknown utility functions. For the general network, we establish the nonasymptotic convergence rate of the proposed algorithm with nonconcave utility functions. The simulations validate our theoretical results and demonstrate the great effectiveness of the proposed method in a real-world network.',\n",
       "  'Ji2023'),\n",
       " (651,\n",
       "  'ADGV57P4',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Implementing Dedicated Bus Lanes (DBLs) is a well-known intervention to improve the effectiveness of public transit. A DBL is a designated lane for bus transit, which avoids congestion and substantially lowers the travel time for riders. This makes transit more attractive, encouraging more travelers to adopt public transportation. This paper studies to what extent the benefits of DBLs apply to On-Demand Multimodal Transit Systems (ODMTS). ODMTS is a novel type of transit system that combines traditional rail and bus networks with on-demand shuttles. Previous case studies have shown that ODMTS may simultaneously improve travel time, reduce system cost, and attract new passengers compared to existing fixed-route systems. Those benefits were shown for an ideal world without traffic congestion, and this paper hypothesizes that the advantages of ODMTS can be even more pronounced in the real world. This paper explores this hypothesis by creating realistic congestion scenarios and solving bilevel optimization problems to design ODMTS under these scenarios. The impact of DBLs on ODMTS is evaluated with a comprehensive case study in the Metro Atlanta Area. The results show that DBLs can significantly improve travel times and are effective at increasing adoption of the system.',\n",
       "  'Lu2023'),\n",
       " (653,\n",
       "  'X9GETDTD',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Bilevel optimization enjoys a wide range of applications in hyper-parameter optimization, meta-learning and reinforcement learning. However, bilevel optimization problems are difficult to solve. Recent progress on scalable bilevel algorithms mainly focuses on bilevel optimization problems where the lower-level objective is either strongly convex or unconstrained. In this work, we tackle the bilevel problem through the lens of the penalty method. We show that under certain conditions, the penalty reformulation recovers the solutions of the original bilevel problem. Further, we propose the penalty-based bilevel gradient descent (PBGD) algorithm and establish its finite-time convergence for the constrained bilevel problem without lower-level strong convexity. Experiments showcase the efficiency of the proposed PBGD algorithm.',\n",
       "  'Shen2023'),\n",
       " (655,\n",
       "  'YIBEDJ89',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Bilevel Optimization has witnessed notable progress recently with new emerging efficient algorithms, yet it is underexplored in the Federated Learning setting. It is unclear how the challenges of Federated Learning affect the convergence of bilevel algorithms. In this work, we study Federated Bilevel Optimization problems. We first propose the FedBiO algorithm that solves the hyper-gradient estimation problem efficiently, then we propose FedBiOAcc to accelerate FedBiO. FedBiO has communication complexity $O(\\\\epsilon^{-1.5})$ with linear speed up, while FedBiOAcc achieves communication complexity $O(\\\\epsilon^{-1})$, sample complexity $O(\\\\epsilon^{-1.5})$ and also the linear speed up. We also study Federated Bilevel Optimization problems with local lower level problems, and prove that FedBiO and FedBiOAcc converges at the same rate with some modification.',\n",
       "  'Li2023'),\n",
       " (657,\n",
       "  'YJRCTNBF',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Federated bilevel optimization has received increasing attention in various emerging machine learning and communication applications. Recently, several Hessian-vector-based algorithms have been proposed to solve the federated bilevel optimization problem. However, several important properties in federated learning such as the partial client participation and the linear speedup for convergence (i.e., the convergence rate and complexity are improved linearly with respect to the number of sampled clients) in the presence of non-i.i.d.~datasets, still remain open. In this paper, we fill these gaps by proposing a new federated bilevel algorithm named FedMBO with a novel client sampling scheme in the federated hypergradient estimation. We show that FedMBO achieves a convergence rate of $\\\\mathcal{O}\\\\big(\\\\frac{1}{\\\\sqrt{nK}}+\\\\frac{1}{K}+\\\\frac{\\\\sqrt{n}}{K^{3/2}}\\\\big)$ on non-i.i.d.~datasets, where $n$ is the number of participating clients in each round, and $K$ is the total number of iteration. This is the first theoretical linear speedup result for non-i.i.d.~federated bilevel optimization. Extensive experiments validate our theoretical results and demonstrate the effectiveness of our proposed method.',\n",
       "  'Huang2023'),\n",
       " (659,\n",
       "  'MB3PV964',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'We present a bilevel optimization approach to the reconstruction of desired demands in optimal control problems over transportation networks of tree type via given noisy measurements. First, the existence of optimal solutions associated with such hierarchical optimization problems is discussed in detail. Second, we investigate the numerical solution of the problem via a suitable discretization strategy and techniques from quadratic optimization. Third, some numerical experiments are presented to visualize variable features of the underlying model including different strategies of how to observe the network.',\n",
       "  'Mehlitz2022'),\n",
       " (661,\n",
       "  'UL4U7BAC',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Simple bilevel problems are optimization problems in which we want to find an optimal solution to an inner problem that minimizes an outer objective function. Such problems appear in many machine learning and signal processing applications as a way to eliminate undesirable solutions. In our work, we suggest a new approach that is designed for bilevel problems with simple outer functions, such as the l1 norm, which are not required to be either smooth or strongly convex. In our new ITerative Approximation and Level-set EXpansion (ITALEX) approach, we alternate between expanding the level-set of the outer function and approximately optimizing the inner problem over this level-set. We show that optimizing the inner function through first-order methods such as proximal gradient and generalized conditional gradient results in a feasibility convergence rate of O(1/k), which up to now was a rate only achieved by bilevel algorithms for smooth and strongly convex outer functions. Moreover, we prove an O(1/k) rate of convergence for the outer function, contrary to existing methods, which only provide asymptotic guarantees. We demonstrate this performance through numerical experiments.',\n",
       "  'Doron2022'),\n",
       " (663,\n",
       "  'FQB6LZJ4',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'We present a differentiable formulation of rigid-body contact dynamics for objects and robots represented as compositions of convex primitives. Existing optimization-based approaches simulating contact between convex primitives rely on a bilevel formulation that separates collision detection and contact simulation. These approaches are unreliable in realistic contact simulation scenarios because isolating the collision detection problem introduces contact location non-uniqueness. Our approach combines contact simulation and collision detection into a unified single-level optimization problem. This disambiguates the collision detection problem in a physics-informed manner. Compared to previous differentiable simulation approaches, our formulation features improved simulation robustness and a reduction in computational complexity by more than an order of magnitude. We illustrate the contact and collision differentiability on a robotic manipulation task requiring optimization-through-contact. We provide a numerically efficient implementation of our formulation in the Julia language called Silico.jl.',\n",
       "  'Cleach2022'),\n",
       " (667,\n",
       "  'CFXI8JAW',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  \"The homology groups of a simplicial complex reveal fundamental properties of the topology of the data or the system and the notion of topological stability naturally poses an important yet not fully investigated question. In the current work, we study the stability in terms of the smallest perturbation sufficient to change the dimensionality of the corresponding homology group. Such definition requires an appropriate weighting and normalizing procedure for the boundary operators acting on the Hodge algebra's homology groups. Using the resulting boundary operators, we then formulate the question of structural stability as a spectral matrix nearness problem for the corresponding higher-order graph Laplacian. We develop a bilevel optimization procedure suitable for the formulated matrix nearness problem and illustrate the method's performance on a variety of synthetic quasi-triangulation datasets and transportation networks.\",\n",
       "  'Guglielmi2023'),\n",
       " (669,\n",
       "  'BMZEGTY7',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'In this paper we study a class of unconstrained and constrained bilevel optimization problems in which the lower-level part is a convex optimization problem, while the upper-level part is possibly a nonconvex optimization problem. In particular, we propose penalty methods for solving them, whose subproblems turn out to be a structured minimax problem and are suitably solved by a first-order method developed in this paper. Under some suitable assumptions, an \\\\emph{operation complexity} of ${\\\\cal O}(\\\\varepsilon^{-4}\\\\log\\\\varepsilon^{-1})$ and ${\\\\cal O}(\\\\varepsilon^{-7}\\\\log\\\\varepsilon^{-1})$, measured by their fundamental operations, is established for the proposed penalty methods for finding an $\\\\varepsilon$-KKT solution of the unconstrained and constrained bilevel optimization problems, respectively. To the best of our knowledge, the methodology and results in this paper are new.',\n",
       "  'Lu2023a'),\n",
       " (671,\n",
       "  'GDC37DJN',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Theoretical properties of bilevel problems are well studied when the lower-level problem is strongly convex. In this work, we focus on bilevel optimization problems without the strong-convexity assumption. In these cases, we first show that the common local optimality measures such as KKT condition or regularization can lead to undesired consequences. Then, we aim to identify the mildest conditions that make bilevel problems tractable. We identify two classes of growth conditions on the lower-level objective that leads to continuity. Under these assumptions, we show that the local optimality of the bilevel problem can be defined via the Goldstein stationarity condition of the hyper-objective. We then propose the Inexact Gradient-Free Method (IGFM) to solve the bilevel problem, using an approximate zeroth order oracle that is of independent interest. Our non-asymptotic analysis demonstrates that the proposed method can find a $(\\\\delta, \\\\varepsilon)$ Goldstein stationary point for bilevel problems with a zeroth order oracle complexity that is polynomial in $d, 1/\\\\delta$ and $1/\\\\varepsilon$.',\n",
       "  'Chen2023'),\n",
       " (673,\n",
       "  'KAE6K34D',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  \"Continual Learning is considered a key step toward next-generation Artificial Intelligence. Among various methods, replay-based approaches that maintain and replay a small episodic memory of previous samples are one of the most successful strategies against catastrophic forgetting. However, since forgetting is inevitable given bounded memory and unbounded tasks, how to forget is a problem continual learning must address. Therefore, beyond simply avoiding catastrophic forgetting, an under-explored issue is how to reasonably forget while ensuring the merits of human memory, including 1. storage efficiency, 2. generalizability, and 3. some interpretability. To achieve these simultaneously, our paper proposes a new saliency-augmented memory completion framework for continual learning, inspired by recent discoveries in memory completion separation in cognitive neuroscience. Specifically, we innovatively propose to store the part of the image most important to the tasks in episodic memory by saliency map extraction and memory encoding. When learning new tasks, previous data from memory are inpainted by an adaptive data generation module, which is inspired by how humans complete episodic memory. The module's parameters are shared across all tasks and it can be jointly trained with a continual learning classifier as bilevel optimization. Extensive experiments on several continual learning and image classification benchmarks demonstrate the proposed method's effectiveness and efficiency.\",\n",
       "  'Bai2022'),\n",
       " (675,\n",
       "  'CESDTW8Q',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'In this paper, we study a class of bilevel optimization problems, also known as simple bilevel optimization, where we minimize a smooth objective function over the optimal solution set of another convex constrained optimization problem. Several iterative methods have been developed for tackling this class of problems. Alas, their convergence guarantees are not satisfactory as they are either asymptotic for the upper-level objective, or the convergence rates are slow and sub-optimal. To address this issue, in this paper, we introduce a generalization of the Frank-Wolfe (FW) method to solve the considered problem. The main idea of our method is to locally approximate the solution set of the lower-level problem via a cutting plane, and then run a FW-type update to decrease the upper-level objective. When the upper-level objective is convex, we show that our method requires ${\\\\mathcal{O}}(\\\\max\\\\{1/\\\\epsilon_f,1/\\\\epsilon_g\\\\})$ iterations to find a solution that is $\\\\epsilon_f$-optimal for the upper-level objective and $\\\\epsilon_g$-optimal for the lower-level objective. Moreover, when the upper-level objective is non-convex, our method requires ${\\\\mathcal{O}}(\\\\max\\\\{1/\\\\epsilon_f^2,1/(\\\\epsilon_f\\\\epsilon_g)\\\\})$ iterations to find an $(\\\\epsilon_f,\\\\epsilon_g)$-optimal solution. We further prove stronger convergence guarantees under the H\\\\\"olderian error bound assumption on the lower-level problem. To the best of our knowledge, our method achieves the best-known iteration complexity for the considered bilevel problem. We also present numerical experiments to showcase the superior performance of our method compared with state-of-the-art methods.',\n",
       "  'Jiang2022'),\n",
       " (677,\n",
       "  'IQ825D6K',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Stochastic approximation (SA) with multiple coupled sequences has found broad applications in machine learning such as bilevel learning and reinforcement learning (RL). In this paper, we study the finite-time convergence of nonlinear SA with multiple coupled sequences. Different from existing multi-timescale analysis, we seek for scenarios where a fine-grained analysis can provide the tight performance guarantee for multi-sequence single-timescale SA (STSA). At the heart of our analysis is the smoothness property of the fixed points in multi-sequence SA that holds in many applications. When all sequences have strongly monotone increments, we establish the iteration complexity of $\\\\mathcal{O}(\\\\epsilon^{-1})$ to achieve $\\\\epsilon$-accuracy, which improves the existing $\\\\mathcal{O}(\\\\epsilon^{-1.5})$ complexity for two coupled sequences. When all but the main sequence have strongly monotone increments, we establish the iteration complexity of $\\\\mathcal{O}(\\\\epsilon^{-2})$. The merit of our results lies in that applying them to stochastic bilevel and compositional optimization problems, as well as RL problems leads to either relaxed assumptions or improvements over their existing performance guarantees.',\n",
       "  'Shen2022'),\n",
       " (679,\n",
       "  '5367LCVU',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Bilevel optimization have gained growing interests, with numerous applications found in meta learning, minimax games, reinforcement learning, and nested composition optimization. This paper studies the problem of distributed bilevel optimization over a network where agents can only communicate with neighbors, including examples from multi-task, multi-agent learning and federated learning. In this paper, we propose a gossip-based distributed bilevel learning algorithm that allows networked agents to solve both the inner and outer optimization problems in a single timescale and share information via network propagation. We show that our algorithm enjoys the $\\\\mathcal{O}(\\\\frac{1}{K \\\\epsilon^2})$ per-agent sample complexity for general nonconvex bilevel optimization and $\\\\mathcal{O}(\\\\frac{1}{K \\\\epsilon})$ for strongly convex objective, achieving a speedup that scales linearly with the network size. The sample complexities are optimal in both $\\\\epsilon$ and $K$. We test our algorithm on the examples of hyperparameter tuning and decentralized reinforcement learning. Simulated experiments confirmed that our algorithm achieves the state-of-the-art training efficiency and test accuracy.',\n",
       "  'Yang2022'),\n",
       " (681,\n",
       "  '5Q6BWBWV',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'We present a new model for finding the optimal placement of electric vehicle charging stations across a multi-period time frame so as to maximise electric vehicle adoption. Via the use of advanced discrete choice models and user classes, this work allows for a granular modelling of user attributes and their preferences in regard to charging station characteristics. Instead of embedding an analytical probability model in the formulation, we adopt a simulation approach and pre-compute error terms for each option available to users for a given number of scenarios. This results in a bilevel optimisation model that is, however, intractable for all but the simplest instances. Using the pre-computed error terms to calculate the users covered by each charging station allows for a maximum covering model, for which solutions can be found more efficiently than for the bilevel formulation. The maximum covering formulation remains intractable in some instances, so we propose rolling horizon, greedy, and GRASP heuristics to obtain good quality solutions more efficiently. Extensive computational results are provided, which compare the maximum covering formulation with the current state-of-the-art, both for exact solutions and the heuristic methods. Keywords: Electric vehicle charging stations, facility location, integer programming, discrete choice models, maximum covering',\n",
       "  'Lamontagne2022'),\n",
       " (683,\n",
       "  'NK5ANRRN',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  \"In this paper, we propose a framework for fast trajectory planning for unmanned aerial vehicles (UAVs). Our framework is reformulated from an existing bilevel optimization, in which the lower-level problem solves for the optimal trajectory with a fixed time allocation, whereas the upper-level problem updates the time allocation using analytical gradients. The lower-level problem incorporates the safety-set constraints (in the form of inequality constraints) and is cast as a convex quadratic program (QP). Our formulation modifies the lower-level QP by excluding the inequality constraints for the safety sets, which significantly reduces the computation time. The safety-set constraints are moved to the upper-level problem, where the feasible waypoints are updated together with the time allocation using analytical gradients enabled by the OptNet. We validate our approach in simulations, where our method's computation time scales linearly with respect to the number of safety sets, in contrast to the state-of-the-art that scales exponentially.\",\n",
       "  'Chen2022a'),\n",
       " (685,\n",
       "  'S68TSLNF',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Bilevel optimization has been successfully applied to many important machine learning problems. Algorithms for solving bilevel optimization have been studied under various settings. In this paper, we study the nonconvex-strongly-convex bilevel optimization under a decentralized setting. We design decentralized algorithms for both deterministic and stochastic bilevel optimization problems. Moreover, we analyze the convergence rates of the proposed algorithms in difference scenarios including the case where data heterogeneity is observed across agents. Numerical experiments on both synthetic and real data demonstrate that the proposed methods are efficient.',\n",
       "  'Chen2022b'),\n",
       " (687,\n",
       "  'DT386PQ6',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'We analyse the asymptotic properties of a continuous-time, two-timescale stochastic approximation algorithm designed for stochastic bilevel optimisation problems in continuous-time models. We obtain the weak convergence rate of this algorithm in the form of a central limit theorem. We also demonstrate how this algorithm can be applied to several continuous-time bilevel optimisation problems.',\n",
       "  'Sharrock2022'),\n",
       " (691,\n",
       "  'AGQTPT2D',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'In this paper, we focus on the nonconvex-strongly-convex bilevel optimization problem (BLO). In this BLO, the objective function of the upper-level problem is nonconvex and possibly nonsmooth, and the lower-level problem is smooth and strongly convex with respect to the underlying variable $y$. We show that the feasible region of BLO is a Riemannian manifold. Then we transform BLO to its corresponding unconstrained constraint dissolving problem (CDB), whose objective function is explicitly formulated from the objective functions in BLO. We prove that BLO is equivalent to the unconstrained optimization problem CDB. Therefore, various efficient unconstrained approaches, together with their theoretical results, can be directly applied to BLO through CDB. We propose a unified framework for developing subgradient-based methods for CDB. Remarkably, we show that several existing efficient algorithms can fit the unified framework and be interpreted as descent algorithms for CDB. These examples further demonstrate the great potential of our proposed approach.',\n",
       "  'Hu2022'),\n",
       " (693,\n",
       "  'XQ492XTK',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  '(Stochastic) bilevel optimization is a frequently encountered problem in machine learning with a wide range of applications such as meta-learning, hyper-parameter optimization, and reinforcement learning. Most of the existing studies on this problem only focused on analyzing the convergence or improving the convergence rate, while little effort has been devoted to understanding its generalization behaviors. In this paper, we conduct a thorough analysis on the generalization of first-order (gradient-based) methods for the bilevel optimization problem. We first establish a fundamental connection between algorithmic stability and generalization error in different forms and give a high probability generalization bound which improves the previous best one from $\\\\bigO(\\\\sqrt{n})$ to $\\\\bigO(\\\\log n)$, where $n$ is the sample size. We then provide the first stability bounds for the general case where both inner and outer level parameters are subject to continuous update, while existing work allows only the outer level parameter to be updated. Our analysis can be applied in various standard settings such as strongly-convex-strongly-convex (SC-SC), convex-convex (C-C), and nonconvex-nonconvex (NC-NC). Our analysis for the NC-NC setting can also be extended to a particular nonconvex-strongly-convex (NC-SC) setting that is commonly encountered in practice. Finally, we corroborate our theoretical analysis and demonstrate how iterations can affect the generalization error by experiments on meta-learning and hyper-parameter optimization.',\n",
       "  'Ding2022'),\n",
       " (695,\n",
       "  '6TXXEUBF',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Bilevel optimization (BO) is useful for solving a variety of important machine learning problems including but not limited to hyperparameter optimization, meta-learning, continual learning, and reinforcement learning. Conventional BO methods need to differentiate through the low-level optimization process with implicit differentiation , which requires expensive calculations related to the Hessian matrix. There has been a recent quest for first-order methods for BO, but the methods proposed to date tend to be complicated and impractical for large-scale deep learning applications. In this work, we propose a simple first-order BO algorithm that depends only on first-order gradient information, requires no implicit differentiation, and is practical and efficient for large-scale non-convex functions in deep learning. We provide a non-asymptotic convergence analysis of the proposed method to stationary points for non-convex objectives and present empirical results that show its superior practical performance.',\n",
       "  'Liu2022'),\n",
       " (697,\n",
       "  '87SPSVBW',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  \"Distributed renewable resources owned by prosumers can be an effective way of fortifying grid resilience and enhancing sustainability. However, prosumers serve their own interests and their objectives are unlikely to align with that of society. This paper develops a bilevel model to study the optimal design of retail electricity tariffs considering the balance between economic efficiency and energy equity. The retail tariff entails a fixed charge and a volumetric charge tied to electricity usage to recover utilities' fixed costs. We analyze solution properties of the bilevel problem and prove an optimal rate design, which is to use fixed charges to recover fixed costs and to balance energy equity among different income groups. This suggests that programs similar to CARE (California Alternative Rate of Energy), which offer lower retail rates to low-income households, are unlikely to be efficient, even if they are politically appealing.\",\n",
       "  'Chen2022c'),\n",
       " (699,\n",
       "  '3QN8LT7X',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Bilevel optimization has been applied to a wide variety of machine learning models. Numerous stochastic bilevel optimization algorithms have been developed in recent years. However, most of them restrict their focus on the single-machine setting so that they are incapable of handling the distributed data. To address this issue, under the setting where all participants compose a network and perform the peer-to-peer communication in this network, we developed two novel distributed stochastic bilevel optimization algorithms based on the gradient tracking communication mechanism and two different gradient estimators. Additionally, we show that they can achieve $O(\\\\frac{1}{\\\\epsilon^{2}(1-\\\\lambda)^2})$ and $O(\\\\frac{1}{\\\\epsilon^{3/2}(1-\\\\lambda)^2})$ convergence rate respectively to obtain the $\\\\epsilon$-accuracy solution, where $1-\\\\lambda$ denotes the spectral gap of the communication network. To our knowledge, this is the first work achieving these theoretical results. Finally, we applied our algorithms to practical machine learning models, and the experimental results confirmed the efficacy of our algorithms.',\n",
       "  'Gao2022'),\n",
       " (701,\n",
       "  '7WAJK947',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Online optimization is a well-established optimization paradigm that aims to make a sequence of correct decisions given knowledge of the correct answer to previous decision tasks. Bilevel programming involves a hierarchical optimization problem where the feasible region of the so-called outer problem is restricted by the graph of the solution set mapping of the inner problem. This paper brings these two ideas together and studies an online bilevel optimization setting in which a sequence of time-varying bilevel problems are revealed one after the other. We extend the known regret bounds for single-level online algorithms to the bilevel setting. Specifically, we introduce new notions of bilevel regret, develop an online alternating time-averaged gradient method that is capable of leveraging smoothness, and provide regret bounds in terms of the path-length of the inner and outer minimizer sequences.',\n",
       "  'Tarzanagh2022'),\n",
       " (703,\n",
       "  'HINCII67',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  \"In an inverse game problem, one needs to infer the cost function of the players in a game such that a desired joint strategy is a Nash equilibrium. We study the inverse game problem for a class of multiplayer matrix games, where the cost perceived by each player is corrupted by random noise. We provide sufficient conditions for the players' quantal response equilibrium - a generalization of the Nash equilibrium to games with perception noise - to be unique. We develop efficient optimization algorithms for inferring the cost matrix based on semidefinite programs and bilevel optimization. We demonstrate the application of these methods in encouraging collision avoidance and fair resource allocation.\",\n",
       "  'Yu2023'),\n",
       " (705,\n",
       "  'TL4JSHM6',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Bilevel optimization has found extensive applications in modern machine learning problems such as hyperparameter optimization, neural architecture search, meta-learning, etc. While bilevel problems with a unique inner minimal point (e.g., where the inner function is strongly convex) are well understood, such a problem with multiple inner minimal points remains to be challenging and open. Existing algorithms designed for such a problem were applicable to restricted situations and do not come with a full guarantee of convergence. In this paper, we adopt a reformulation of bilevel optimization to constrained optimization, and solve the problem via a primal-dual bilevel optimization (PDBO) algorithm. PDBO not only addresses the multiple inner minima challenge, but also features fully first-order efficiency without involving second-order Hessian and Jacobian computations, as opposed to most existing gradient-based bilevel algorithms. We further characterize the convergence rate of PDBO, which serves as the first known non-asymptotic convergence guarantee for bilevel optimization with multiple inner minima. Our experiments demonstrate desired performance of the proposed approach.',\n",
       "  'Sow2022'),\n",
       " (707,\n",
       "  'U7VSMGYS',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'We present a method for supervised learning of sparsity-promoting regularizers for denoising signals and images. Sparsity-promoting regularization is a key ingredient in solving modern signal reconstruction problems; however, the operators underlying these regularizers are usually either designed by hand or learned from data in an unsupervised way. The recent success of supervised learning (mainly convolutional neural networks) in solving image reconstruction problems suggests that it could be a fruitful approach to designing regularizers. Towards this end, we propose to denoise signals using a variational formulation with a parametric, sparsity-promoting regularizer, where the parameters of the regularizer are learned to minimize the mean squared error of reconstructions on a training set of ground truth image and measurement pairs. Training involves solving a challenging bilievel optimization problem; we derive an expression for the gradient of the training loss using the closed-form solution of the denoising problem and provide an accompanying gradient descent algorithm to minimize it. Our experiments with structured 1D signals and natural images show that the proposed method can learn an operator that outperforms well-known regularizers (total variation, DCT-sparsity, and unsupervised dictionary learning) and collaborative filtering for denoising. While the approach we present is specific to denoising, we believe that it could be adapted to the larger class of inverse problems with linear measurement models, giving it applicability in a wide range of signal reconstruction settings.',\n",
       "  'Ghosh2022'),\n",
       " (709,\n",
       "  'EST4982Y',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Generalizable manipulation requires that robots be able to interact with novel objects and environment. This requirement makes manipulation extremely challenging as a robot has to reason about complex frictional interaction with uncertainty in physical properties of the object. In this paper, we study robust optimization for control of pivoting manipulation in the presence of uncertainties. We present insights about how friction can be exploited to compensate for the inaccuracies in the estimates of the physical properties during manipulation. In particular, we derive analytical expressions for stability margin provided by friction during pivoting manipulation. This margin is then used in a bilevel trajectory optimization algorithm to design a controller that maximizes this stability margin to provide robustness against uncertainty in physical properties of the object. We demonstrate our proposed method using a 6 DoF manipulator for manipulating several different objects.',\n",
       "  'Shirai2022'),\n",
       " (711,\n",
       "  'PIINZZSZ',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Generating multi-contrasts/modal MRI of the same anatomy enriches diagnostic information but is limited in practice due to excessive data acquisition time. In this paper, we propose a novel deep-learning model for joint reconstruction and synthesis of multi-modal MRI using incomplete k-space data of several source modalities as inputs. The output of our model includes reconstructed images of the source modalities and high-quality image synthesized in the target modality. Our proposed model is formulated as a variational problem that leverages several learnable modality-specific feature extractors and a multimodal synthesis module. We propose a learnable optimization algorithm to solve this model, which induces a multi-phase network whose parameters can be trained using multi-modal MRI data. Moreover, a bilevel-optimization framework is employed for robust parameter training. We demonstrate the effectiveness of our approach using extensive numerical experiments.',\n",
       "  'Bian2022'),\n",
       " (713,\n",
       "  'T9IPBG26',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Topology Optimization (TO), which maximizes structural robustness under material weight constraints, is becoming an essential step for the automatic design of mechanical parts. However, existing TO algorithms use the Finite Element Analysis (FEA) that requires massive computational resources. We present a novel TO algorithm that incurs a much lower iterative cost. Unlike conventional methods that require exact inversions of large FEA system matrices at every iteration, we reformulate the problem as a bilevel optimization that can be solved using a first-order algorithm and only inverts the system matrix approximately. As a result, our method incurs a low iterative cost, and users can preview the TO results interactively for fast design updates. Theoretical convergence analysis and numerical experiments are conducted to verify our effectiveness. We further discuss extensions to use high-performance preconditioners and fine-grained parallelism on the Graphics Processing Unit (GPU).',\n",
       "  'Pan2022'),\n",
       " (715,\n",
       "  'YS9ZQ4HQ',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Recent years have witnessed enormous progress of online learning. However, a major challenge on the road to artificial agents is concept drift, that is, the data probability distribution would change where the data instance arrives sequentially in a stream fashion, which would lead to catastrophic forgetting and degrade the performance of the model. In this paper, we proposed a new Bilevel Online Deep Learning (BODL) framework, which combine bilevel optimization strategy and online ensemble classifier. In BODL algorithm, we use an ensemble classifier, which use the output of different hidden layers in deep neural network to build multiple base classifiers, the important weights of the base classifiers are updated according to exponential gradient descent method in an online manner. Besides, we apply the similar constraint to overcome the convergence problem of online ensemble framework. Then an effective concept drift detection mechanism utilizing the error rate of classifier is designed to monitor the change of the data probability distribution. When the concept drift is detected, our BODL algorithm can adaptively update the model parameters via bilevel optimization and then circumvent the large drift and encourage positive transfer. Finally, the extensive experiments and ablation studies are conducted on various datasets and the competitive numerical results illustrate that our BODL algorithm is a promising approach.',\n",
       "  'Han2021'),\n",
       " (717,\n",
       "  '8GI6HS77',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'We propose a new approach to solving bilevel optimization problems, intermediate between solving full-system optimality conditions with a Newton-type approach, and treating the inner problem as an implicit function. The overall idea is to solve the full-system optimality conditions, but to precondition them to alternate between taking steps of simple conventional methods for the inner problem, the adjoint equation, and the outer problem. We prove the convergence of the approach for combinations of gradient descent and forward-backward splitting with exact and inexact solution of the adjoint equation. We demonstrate good performance on learning the regularization parameter for anisotropic total variation image denoising, and the convolution kernel for image deconvolution.',\n",
       "  'Suonpera2022'),\n",
       " (719,\n",
       "  '72DJH6VL',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Bilevel optimization, the problem of minimizing a value function which involves the arg-minimum of another function, appears in many areas of machine learning. In a large scale setting where the number of samples is huge, it is crucial to develop stochastic methods, which only use a few samples at a time to progress. However, computing the gradient of the value function involves solving a linear system, which makes it difficult to derive unbiased stochastic estimates. To overcome this problem we introduce a novel framework, in which the solution of the inner problem, the solution of the linear system, and the main variable evolve at the same time. These directions are written as a sum, making it straightforward to derive unbiased estimates. The simplicity of our approach allows us to develop global variance reduction algorithms, where the dynamics of all variables is subject to variance reduction. We demonstrate that SABA, an adaptation of the celebrated SAGA algorithm in our framework, has $O(\\\\frac1T)$ convergence rate, and that it achieves linear convergence under Polyak-Lojasciewicz assumption. This is the first stochastic algorithm for bilevel optimization that verifies either of these properties. Numerical experiments validate the usefulness of our method.',\n",
       "  'Dagreou2022'),\n",
       " (721,\n",
       "  'V6KYW76N',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Meta-learning approaches enable machine learning systems to adapt to new tasks given few examples by leveraging knowledge from related tasks. However, a large number of meta-training tasks are still required for generalization to unseen tasks during meta-testing, which introduces a critical bottleneck for real-world problems that come with only few tasks, due to various reasons including the difficulty and cost of constructing tasks. Recently, several task augmentation methods have been proposed to tackle this issue using domain-specific knowledge to design augmentation techniques to densify the meta-training task distribution. However, such reliance on domain-specific knowledge renders these methods inapplicable to other domains. While Manifold Mixup based task augmentation methods are domain-agnostic, we empirically find them ineffective on non-image domains. To tackle these limitations, we propose a novel domain-agnostic task augmentation method, Meta-Interpolation, which utilizes expressive neural set functions to densify the meta-training task distribution using bilevel optimization. We empirically validate the efficacy of Meta-Interpolation on eight datasets spanning across various domains such as image classification, molecule property prediction, text classification and speech recognition. Experimentally, we show that Meta-Interpolation consistently outperforms all the relevant baselines. Theoretically, we prove that task interpolation with the set function regularizes the meta-learner to improve generalization.',\n",
       "  'Lee2022'),\n",
       " (723,\n",
       "  'GFD27SSZ',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Bilevel optimization is one of the fundamental problems in machine learning and optimization. Recent theoretical developments in bilevel optimization focus on finding the first-order stationary points for nonconvex-strongly-convex cases. In this paper, we analyze algorithms that can escape saddle points in nonconvex-strongly-convex bilevel optimization. Specifically, we show that the perturbed approximate implicit differentiation (AID) with a warm start strategy finds $\\\\epsilon$-approximate local minimum of bilevel optimization in $\\\\tilde{O}(\\\\epsilon^{-2})$ iterations with high probability. Moreover, we propose an inexact NEgative-curvature-Originated-from-Noise Algorithm (iNEON), a pure first-order algorithm that can escape saddle point and find local minimum of stochastic bilevel optimization. As a by-product, we provide the first nonasymptotic analysis of perturbed multi-step gradient descent ascent (GDmax) algorithm that converges to local minimax point for minimax problems.',\n",
       "  'Huang2022'),\n",
       " (725,\n",
       "  'ZJYHTBDY',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Gradient methods have become mainstream techniques for Bi-Level Optimization (BLO) in learning and vision fields. The validity of existing works heavily relies on solving a series of approximation subproblems with extraordinarily high accuracy. Unfortunately, to achieve the approximation accuracy requires executing a large quantity of time-consuming iterations and computational burden is naturally caused. This paper is thus devoted to address this critical computational issue. In particular, we propose a single-level formulation to uniformly understand existing explicit and implicit Gradient-based BLOs (GBLOs). This together with our designed counter-example can clearly illustrate the fundamental numerical and theoretical issues of GBLOs and their naive accelerations. By introducing the dual multipliers as a new variable, we then establish Bilevel Alternating Gradient with Dual Correction (BAGDC), a general framework, which significantly accelerates different categories of existing methods by taking specific settings. A striking feature of our convergence result is that, compared to those original unaccelerated GBLO versions, the fast BAGDC admits a unified non-asymptotic convergence theory towards stationarity. A variety of numerical experiments have also been conducted to demonstrate the superiority of the proposed algorithmic framework.',\n",
       "  'Liu2022a'),\n",
       " (727,\n",
       "  'E557DDDB',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Bilevel Optimization has witnessed notable progress recently with new emerging efficient algorithms and has been applied to many machine learning tasks such as data cleaning, few-shot learning, and neural architecture search. However, little attention has been paid to solve the bilevel problems under distributed setting. Federated learning (FL) is an emerging paradigm which solves machine learning tasks over distributed-located data. FL problems are challenging to solve due to the heterogeneity and communication bottleneck. However, it is unclear how these challenges will affect the convergence of Bilevel Optimization algorithms. In this paper, we study Federated Bilevel Optimization problems. Specifically, we first propose the FedBiO, a deterministic gradient-based algorithm and we show it requires $O(\\\\epsilon^{-2})$ number of iterations to reach an $\\\\epsilon$-stationary point. Then we propose FedBiOAcc to accelerate FedBiO with the momentum-based variance-reduction technique under the stochastic scenario. We show FedBiOAcc has complexity of $O(\\\\epsilon^{-1.5})$. Finally, we validate our proposed algorithms via the important Fair Federated Learning task. More specifically, we define a bilevel-based group fair FL objective. Our algorithms show superior performances compared to other baselines in numerical experiments.',\n",
       "  'Li2022'),\n",
       " (729,\n",
       "  'MCJ7CNMD',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'In this paper, we exploit the so-called value function reformulation of the bilevel optimization problem to develop duality results for the problem. Our approach builds on Fenchel-Lagrange-type duality to establish suitable results for the bilevel optimization problem. First, we overview some standard duality results to show that they are not applicable to our problem. Secondly, via the concept of partial calmness, we establish weak and strong duality results. In particular, Lagrange, Fenchel-Lagrange, and Toland-Fenchel- Lagrange duality concepts are investigated for this type of problems under some suitable conditions. Thirdly, based on the use of some regularization of our bilevel program, we establish sufficient conditions ensuring strong duality results under a generalized Slater-type condition without convexity assumptions and without the partial calmness condition. Finally, without the Slater condition, a strong duality result is constructed for the bilevel optimization problem with geometric constraint.',\n",
       "  'En-naciri2022'),\n",
       " (731,\n",
       "  'DCNALP8B',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  \"We present a method to solve a special class of parameter identification problems for an elliptic optimal control problem to global optimality. The bilevel problem is reformulated via the optimal-value function of the lower-level problem. The reformulated problem is nonconvex and standard regularity conditions like Robinson's CQ are violated. Via a relaxation of the constraints, the problem can be decomposed into a family of convex problems and this is the basis for a solution algorithm. The convergence properties are analyzed. It is shown that a penalty method can be employed to solve this family of problems while maintaining convergence speed. For an example problem, the use of the identity as penalty function allows for the solution by a semismooth Newton method. Numerical results are presented. Difficulties and limitations of our approach to solve a nonconvex problem to global optimality are discussed.\",\n",
       "  'Friedemann2022'),\n",
       " (733,\n",
       "  'ZTE5RHYL',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  \"In this paper we present BilevelJuMP, a new Julia package to support bilevel optimization within the JuMP framework. The package is a Julia library that enables the user to describe both upper and lower-level optimization problems using the JuMP algebraic syntax. Due to the generality and flexibility our library inherits from JuMP's syntax, our package allows users to model bilevel optimization problems with conic constraints in the lower level and all JuMP supported constraints in the upper level (Conic, Quadratic, Non-Linear, Integer, etc.). Moreover, the user-defined problem can be subsequently solved by various techniques relying on mathematical program with equilibrium constraints (MPEC) reformulations. Manipulations on the original problem data are possible due to MathOptInterface.jl's structures and Dualization.jl features. Hence, the proposed package allows quickly model, deploy, and thereby experiment bilevel models based on off-the-shelf mixed integer linear programming and nonlinear solvers.\",\n",
       "  'Garcia2022'),\n",
       " (735,\n",
       "  'IZJ6SK2J',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'In this work we derive a second-order approach to bilevel optimization, a type of mathematical programming in which the solution to a parameterized optimization problem (the \"lower\" problem) is itself to be optimized (in the \"upper\" problem) as a function of the parameters. Many existing approaches to bilevel optimization employ first-order sensitivity analysis, based on the implicit function theorem (IFT), for the lower problem to derive a gradient of the lower problem solution with respect to its parameters; this IFT gradient is then used in a first-order optimization method for the upper problem. This paper extends this sensitivity analysis to provide second-order derivative information of the lower problem (which we call the IFT Hessian), enabling the usage of faster-converging second-order optimization methods at the upper level. Our analysis shows that (i) much of the computation already used to produce the IFT gradient can be reused for the IFT Hessian, (ii) errors bounds derived for the IFT gradient readily apply to the IFT Hessian, (iii) computing IFT Hessians can significantly reduce overall computation by extracting more information from each lower level solve. We corroborate our findings and demonstrate the broad range of applications of our method by applying it to problem instances of least squares hyperparameter auto-tuning, multi-class SVM auto-tuning, and inverse optimal control.',\n",
       "  'Dyro2022'),\n",
       " (737,\n",
       "  'FTX5EYAJ',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'This review examines gradient-based techniques to solve bilevel optimization problems. Bilevel optimization extends the loss minimization framework underlying statistical learning to systems that are implicitly defined through a quantity they minimize. This characterization can be applied to neural networks, optimizers, algorithmic solvers, and even physical systems and allows for greater modeling flexibility compared to the usual explicit definition of such systems. We focus on solving learning problems of this kind through gradient descent, leveraging the toolbox of implicit differentiation and, for the first time applied to this setting, the equilibrium propagation theorem. We present the mathematical foun-dations behind such methods, introduce the gradient estimation algorithms in detail, and compare the competitive advantages of the different approaches.',\n",
       "  'Zucchet2022'),\n",
       " (739,\n",
       "  'U4FTS88Y',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Bilevel optimization recently has attracted increased interest in machine learning due to its many applications such as hyper-parameter optimization and policy optimization. Although some methods recently have been proposed to solve the bilevel problems, these methods do not consider using adaptive learning rates. To fill this gap, in the paper, we propose a class of fast and effective adaptive methods for solving bilevel optimization problems that the outer problem is possibly nonconvex and the inner problem is strongly-convex. Specifically, we propose a fast single-loop BiAdam algorithm based on the basic momentum technique, which achieves a sample complexity of $\\\\tilde{O}(\\\\epsilon^{-4})$ for finding an $\\\\epsilon$-stationary point. At the same time, we propose an accelerated version of BiAdam algorithm (VR-BiAdam) by using variance reduced technique, which reaches the best known sample complexity of $\\\\tilde{O}(\\\\epsilon^{-3})$. To further reduce computation in estimating derivatives, we propose a fast single-loop stochastic approximated BiAdam algorithm (saBiAdam) by avoiding the Hessian inverse, which still achieves a sample complexity of $\\\\tilde{O}(\\\\epsilon^{-4})$ without large batches. We further present an accelerated version of saBiAdam algorithm (VR-saBiAdam), which also reaches the best known sample complexity of $\\\\tilde{O}(\\\\epsilon^{-3})$. We apply the unified adaptive matrices to our methods as the SUPER-ADAM \\\\citep{huang2021super}, which including many types of adaptive learning rates. Moreover, our framework can flexibly use the momentum and variance reduced techniques. In particular, we provide a useful convergence analysis framework for both the constrained and unconstrained bilevel optimization. To the best of our knowledge, we first study the adaptive bilevel optimization methods with adaptive learning rates.',\n",
       "  'Huang2021'),\n",
       " (741,\n",
       "  'BWLLQG63',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Stochastic nested optimization, including stochastic compositional, min-max and bilevel optimization, is gaining popularity in many machine learning applications. While the three problems share the nested structure, existing works often treat them separately, and thus develop problem-specific algorithms and their analyses. Among various exciting developments, simple SGD-type updates (potentially on multiple variables) are still prevalent in solving this class of nested problems, but they are believed to have slower convergence rate compared to that of the non-nested problems. This paper unifies several SGD-type updates for stochastic nested problems into a single SGD approach that we term ALternating Stochastic gradient dEscenT (ALSET) method. By leveraging the hidden smoothness of the problem, this paper presents a tighter analysis of ALSET for stochastic nested problems. Under the new analysis, to achieve an $\\\\epsilon$-stationary point of the nested problem, it requires ${\\\\cal O}(\\\\epsilon^{-2})$ samples. Under certain regularity conditions, applying our results to stochastic compositional, min-max and reinforcement learning problems either improves or matches the best-known sample complexity in the respective cases. Our results explain why simple SGD-type algorithms in stochastic nested problems all work very well in practice without the need for further modifications.',\n",
       "  'Chen2021'),\n",
       " (743,\n",
       "  'ZHVKVS69',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Bilevel optimization has been widely applied many machine learning problems such as hyperparameter optimization, policy optimization and meta learning. Although many bilevel optimization methods more recently have been proposed to solve the bilevel optimization problems, they still suffer from high computational complexities and do not consider the more general bilevel problems with nonsmooth regularization. In the paper, thus, we propose a class of efficient bilevel optimization methods based on Bregman distance. In our methods, we use the mirror decent iteration to solve the outer subproblem of the bilevel problem by using strongly-convex Bregman functions. Specifically, we propose a bilevel optimization method based on Bregman distance (BiO-BreD) for solving deterministic bilevel problems, which reaches the lower computational complexities than the best known results. We also propose a stochastic bilevel optimization method (SBiO-BreD) for solving stochastic bilevel problems based on the stochastic approximated gradients and Bregman distance. Further, we propose an accelerated version of SBiO-BreD method (ASBiO-BreD) by using the variance-reduced technique. Moreover, we prove that the ASBiO-BreD outperforms the best known computational complexities with respect to the condition number $\\\\kappa$ and the target accuracy $\\\\epsilon$ for finding an $\\\\epsilon$-stationary point of nonconvex-strongly-convex bilevel problems. In particular, our methods can solve the bilevel optimization problems with nonsmooth regularization with a lower computational complexity.',\n",
       "  'Huang2022a'),\n",
       " (746,\n",
       "  '5K3AE3BT',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Although application examples of multilevel optimization have already been discussed since the 1990s, the development of solution methods was almost limited to bilevel cases due to the difficulty of the problem. In recent years, in machine learning, Franceschi et al. have proposed a method for solving bilevel optimization problems by replacing their lower-level problems with the T steepest descent update equations with some prechosen iteration number T . In this paper, we have developed a gradient-based algorithm for multilevel optimization with n levels based on their idea and proved that our reformulation asymptotically converges to the original multilevel problem. As far as we know, this is one of the first algorithms with some theoretical guarantee for multilevel optimization. Numerical experiments show that a trilevel hyperparameter learning model considering data poisoning produces more stable prediction results than an existing bilevel hyperparameter learning model in noisy data settings.',\n",
       "  'Sato2021'),\n",
       " (748,\n",
       "  'KYW7LMT4',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Iteratively reweighted least square (IRLS) is a popular approach to solve sparsity-enforcing regression problems in machine learning. State of the art approaches are more efficient but typically rely on specific coordinate pruning schemes. In this work, we show how a surprisingly simple re-parametrization of IRLS, coupled with a bilevel resolution (instead of an alternating scheme) is able to achieve top performances on a wide range of sparsity (such as Lasso, group Lasso and trace norm regularizations), regularization strength (including hard constraints), and design matrices (ranging from correlated designs to differential operators). Similarly to IRLS, our method only involves linear systems resolutions, but in sharp contrast, corresponds to the minimization of a smooth function. Despite being non-convex, we show that there are no spurious minima and that saddle points are “ridable”, so that there always exists a descent direction. We thus advocate for the use of a BFGS quasi-Newton solver, which makes our approach simple, robust and efficient. We perform a numerical benchmark of the convergence speed of our algorithm against state of the art solvers for Lasso, group Lasso, trace norm and linearly constrained problems. These results highlight the versatility of our approach, removing the need to use different solvers depending on the specificity of the ML problem under study.',\n",
       "  'Poon2021'),\n",
       " (750,\n",
       "  '2M38SR4F',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'We study the network pricing problem where the leader maximizes revenue by determining the optimal amounts of tolls to charge on a set of arcs, under the assumption that the followers will react rationally and choose the shortest paths to travel. Many distinct single-level reformulations of this bilevel optimization program have been proposed; however, their relationship has not been established. In this paper, we aim to build a connection between those reformulations and explore the combination of the path representation with various modeling options, allowing us to generate 12 different reformulations of the problem. Moreover, we propose a new path enumeration scheme, path-based preprocessing, and hybrid framework to further improve performance and robustness when solving the final model. We provide numerical results, comparing all the derived reformulations and confirming the efficiency of the novel dimensionality reduction procedures.',\n",
       "  'Bui2022'),\n",
       " (752,\n",
       "  'JLKZL69I',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'The (gradient-based) bilevel programming framework is widely used in hyperparameter optimization and has achieved excellent performance empirically. Previous theoretical work mainly focuses on its optimization properties, while leaving the analysis on generalization largely open. This paper attempts to address the issue by presenting an expectation bound w.r.t. the validation set based on uniform stability. Our results can explain some mysterious behaviours of the bilevel programming in practice, for instance, overfitting to the validation set. We also present an expectation bound for the classical cross-validation algorithm. Our results suggest that gradient-based algorithms can be better than cross-validation under certain conditions in a theoretical perspective. Furthermore, we prove that regularization terms in both the outer and inner levels can relieve the overfitting problem in gradient-based algorithms. In experiments on feature learning and data reweighting for noisy labels, we corroborate our theoretical findings.',\n",
       "  'Bao2021'),\n",
       " (754,\n",
       "  'RCTEHWMF',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'We propose a new computationally-efficient first-order algorithm for Model-Agnostic Meta-Learning (MAML). The key enabling technique is to interpret MAML as a bilevel optimization (BLO) problem and leverage the sign-based SGD(signSGD) as a lower-level optimizer of BLO. We show that MAML, through the lens of signSGD-oriented BLO, naturally yields an alternating optimization scheme that just requires first-order gradients of a learned meta-model. We term the resulting MAML algorithm Sign-MAML. Compared to the conventional first-order MAML (FO-MAML) algorithm, Sign-MAML is theoretically-grounded as it does not impose any assumption on the absence of second-order derivatives during meta training. In practice, we show that Sign-MAML outperforms FO-MAML in various few-shot image classification tasks, and compared to MAML, it achieves a much more graceful tradeoff between classification accuracy and computation efficiency.',\n",
       "  'Fan2021'),\n",
       " (756,\n",
       "  'EFARBJCI',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'This review discusses methods for learning parameters for image reconstruction problems using bilevel formulations. Image reconstruction typically involves optimizing a cost function to recover a vector of unknown variables that agrees with collected measurements and prior assumptions. Stateof- the-art image reconstruction methods learn these prior assumptions from training data using various machine learning techniques, such as bilevel methods. One can view the bilevel problem as formalizing hyperparameter optimization, as bridging machine learning and cost function based optimization methods, or as a method to learn variables best suited to a specific task. More formally, bilevel problems attempt to minimize an upper-level loss function, where variables in the upper-level loss function are themselves minimizers of a lower-level cost function. This review contains a running example problem of learning tuning parameters and the coefficients for sparsifying filters used in a regularizer. Such filters generalize the popular total variation regularization method, and learned filters are closely related to convolutional neural networks approaches that are rapidly gaining in popularity. Here, the lower-level problem is to reconstruct an image using a regularizer with learned sparsifying filters; the corresponding upper-level optimization problem involves a measure of reconstructed image quality based on training data. This review discusses multiple perspectives to motivate the use of bilevel methods and to make them more easily accessible to different audiences. We then turn to ways to optimize the bilevel problem, providing pros and cons of the variety of proposed approaches. Finally we overview bilevel applications in image reconstruction.',\n",
       "  'Crockett2022'),\n",
       " (760,\n",
       "  'DPGI86W5',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'This paper presents novel formulations and algorithms for <formula><tex>$N-k$</tex></formula> interdiction problem in transmission networks. In particular, it formulates spatial and topological resource constraints on attackers for <formula><tex>$N-k$</tex></formula> interdiction problems and illustrates the formulation with two new classes of <formula><tex>$N-k$</tex></formula> attacks: (i) Spatial <formula><tex>$N-k$</tex></formula> attacks where the attack is constrained by geographic distance of a bus chosen by an attacker and (ii) Topological <formula><tex>$N-k$</tex></formula> attacks where the attack is constrained to connected components. These two specific types of <formula><tex>$N-k$</tex></formula> attacks compute interdiction plans designed to better model localized attacks, such as those induced by natural disasters or physical attacks. We then formulate these two resource-constrained interdiction problems as bilevel, max-min optimization problems and present a novel constraint generation algorithm to solve these formulations. Detailed case studies analyzing the behavior of spatially and topologically resource-constrained problems and comparing them to the traditional <formula><tex>$N-k$</tex></formula> interdiction problem are also presented.',\n",
       "  'Sundar2021'),\n",
       " (762,\n",
       "  'XCLF7C4H',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  \"The partial calmness for the bilevel programming problem (BLPP) is an important condition which ensures that a local optimal solution of BLPP is a local optimal solution of a partially penalized problem where the lower-level optimality constraint is moved to the objective function and hence a weaker constraint qualification can be applied. In this paper, we propose a sufficient condition in the form of a partial error bound condition which guarantees the partial calmness condition. We analyze the partial calmness for the combined program based on the Bouligand (B) and the Fritz John (FJ) stationary conditions from a generic point of view. Our main result states that the partial error bound condition for the combined programs based on B and FJ conditions is generic for an important setting with applications in economics, and hence the partial calmness for the combined program is not a particularly stringent assumption. Moreover, we derive optimality conditions for the combined program for the generic case without any extra constraint qualifications and show the exact equivalence between our optimality condition and the one by Jongen and Shikhman [Math. Program., 136 (2012), pp. 65-89] given in implicit form. Our arguments are based on Jongen, Jonker, and Twilt's [Math. Program., 34 (1986), pp. 333-353] generic (five type) classification of the so-called generalized critical points for one-dimensional parametric optimization problems and Jongen and Shikhman's generic local reductions of BLPPs.\",\n",
       "  'Ke2022'),\n",
       " (764,\n",
       "  'I5CUKMSD',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'In this paper, we propose a combined approach with second-order optimality conditions of the lower level problem to study constraint qualifications and optimality conditions for bilevel programming problems. The new method is inspired by the combined approach developed by Ye and Zhu in 2010, where the authors combined the classical first-order and the value function approaches to derive new necessary optimality conditions under weaker conditions. In our approach, we add the second-order optimality condition to the combined program as a new constraint. We show that when all known approaches fail, adding the second-order optimality condition as a constraint makes the corresponding partial calmness condition easier to hold. We also give some discussions on optimality conditions and advantages and disadvantages of the combined approaches with the first-order and the second-order information.',\n",
       "  'Ma2021'),\n",
       " (766,\n",
       "  '2G9MIUPZ',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Bilevel optimization has become a powerful framework in various machine learning applications including meta-learning, hyperparameter optimization, and network architecture search. There are generally two classes of bilevel optimization formulations for machine learning: 1) problem-based bilevel optimization, whose inner-level problem is formulated as finding a minimizer of a given loss function; and 2) algorithm-based bilevel optimization, whose inner-level solution is an output of a fixed algorithm. For the first class, two popular types of gradient-based algorithms have been proposed for hypergradient estimation via approximate implicit differentiation (AID) and iterative differentiation (ITD). Algorithms for the second class include the popular model-agnostic meta-learning (MAML) and almost no inner loop (ANIL). However, the convergence rate and fundamental limitations of bilevel optimization algorithms have not been well explored. This thesis provides a comprehensive convergence rate analysis for bilevel algorithms in the aforementioned two classes. We further propose principled algorithm designs for bilevel optimization with higher efficiency and scalability. For the problem-based formulation, we provide a convergence rate analysis for AID- and ITD-based bilevel algorithms. We then develop acceleration bilevel algorithms, for which we provide shaper convergence analysis with relaxed assumptions. We also provide the first lower bounds for bilevel optimization, and establish the optimality by providing matching upper bounds under certain conditions. We finally propose new stochastic bilevel optimization algorithms with lower complexity and higher efficiency in practice. For the algorithm-based formulation, we develop a theoretical convergence for general multi-step MAML and ANIL, and characterize the impact of parameter selections and loss geometries on the their complexities.',\n",
       "  'Ji2021'),\n",
       " (768,\n",
       "  'E64CWC84',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'We consider a bilevel continuous knapsack problem where the leader controls the capacity of the knapsack, while the follower chooses a feasible packing maximizing his own profit. The leader’s aim is to optimize a linear objective function in the capacity and in the follower’s solution, but with respect to different item values. We address a stochastic version of this problem where the follower’s profits are uncertain from the leader’s perspective, and only a probability distribution is known. Assuming that the leader aims at optimizing the expected value of her objective function, we first observe that the stochastic problem is tractable as long as the possible scenarios are given explicitly as part of the input, which also allows to deal with general distributions using a sample average approximation. For the case of independently and uniformly distributed item values, we show that the problem is #P-hard in general, and the same is true even for evaluating the leader’s objective function. Nevertheless, we present pseudo-polynomial time algorithms for this case, running in time linear in the total size of the items. Based on this, we derive an additive approximation scheme for the general case of independently distributed item values, which runs in pseudo-polynomial time.',\n",
       "  'Buchheim2022'),\n",
       " (770,\n",
       "  '4FGKJDIE',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'This paper is concerned with the derivation of first- and second-order sufficient optimality conditions for optimistic bilevel optimization problems involving smooth functions. First-order sufficient optimality conditions are obtained by estimating the tangent cone to the feasible set of the bilevel program in terms of initial problem data. This is done by exploiting several different reformulations of the hierarchical model as a single-level problem. To obtain second-order sufficient optimality conditions, we exploit the so-called value function reformulation of the bilevel optimization problem, which is then tackled with the aid of second-order directional derivatives. The resulting conditions can be stated in terms of initial problem data in several interesting situations comprising the settings where the lower level is linear or possesses strongly stable solutions.',\n",
       "  'Mehlitz2021'),\n",
       " (772,\n",
       "  'JLYLC8QJ',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'This study explores the design of an On-Demand Multimodal Transit System (ODMTS) that includes segmented mode switching models that decide whether potential riders adopt the new ODMTS or stay with their personal vehicles. It is motivated by the desire of transit agencies to design their network by taking into account both existing and latent demand, as quality of service improves. The paper presents a bilevel optimization where the leader problem designs the network and each rider has a follower problem to decide her best route through the ODMTS. The bilevel model is solved by a decomposition algorithm that combines traditional Benders cuts with combinatorial cuts to ensure the consistency of mode choices by the leader and follower problems. The approach is evaluated on a case study using historical data from Ann Arbor, Michigan, and a user choice model based on the income levels of the potential transit riders.',\n",
       "  'Basciftci2020'),\n",
       " (774,\n",
       "  'WW7N4GQ6',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'This article studies Gauss–Newton-type methods for over-determined systems to find solutions to bilevel programming problems. To proceed, we use the lower-level value function reformulation of bilevel programs and consider necessary optimality conditions under appropriate assumptions. First, under strict complementarity for upper- and lower-level feasibility constraints, we prove the convergence of a Gauss–Newton-type method in computing points satisfying these optimality conditions under additional tractable qualification conditions. Potential approaches to address the shortcomings of the method are then proposed, leading to alternatives such as the pseudo or smoothing Gauss–Newton-type methods for bilevel optimization. Our numerical experiments conducted on 124 examples from the recently released Bilevel Optimization LIBrary (BOLIB) compare the performance of our method under different scenarios and show that it is a tractable approach to solve bilevel optimization problems with continuous variables.',\n",
       "  'Fliege2021'),\n",
       " (776,\n",
       "  'AJWYDT2V',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'The bilevel program is an optimization problem in which the constraint involves solutions to a parametric optimization problem. It is well known that the value function reformulation provides an equivalent single-level optimization problem, but it results in a nonsmooth optimization problem that never satisfies the usual constraint qualification, such as the Mangasarian–Fromovitz constraint qualification (MFCQ). In this paper, we show that even the first order sufficient condition for metric subregularity (which is, in general, weaker than MFCQ) fails at each feasible point of the bilevel program. We introduce the concept of a directional calmness condition and show that, under the directional calmness condition, the directional necessary optimality condition holds. Although the directional optimality condition is, in general, sharper than the nondirectional one, the directional calmness condition is, in general, weaker than the classical calmness condition and, hence, is more likely to hold. We perform the directional sensitivity analysis of the value function and propose the directional quasi-normality as a sufficient condition for the directional calmness. An example is given to show that the directional quasi-normality condition may hold for the bilevel program.',\n",
       "  'Bai2022a'),\n",
       " (778,\n",
       "  '7M5M5AFA',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'In this paper, we consider an extension of the well-known Influence Maximization Problem in a social network which deals with finding a set of k nodes to initiate a diffusion process so that the total number of influenced nodes at the end of the process is maximized. The extension focuses on a competitive variant where two decision makers are involved. The first one, the leader, tries to maximize the total influence spread by selecting the most influential nodes and the second one, the follower, tries to minimize it by deactivating some of these nodes. The formulated bilevel model is solved by complete enumeration for small-sized instances and by a matheuristic for large-sized instances. In both cases, the lower level problem, which is a stochastic optimization problem, is approximated via the Sample Average Approximation method.',\n",
       "  'Taninmis2019'),\n",
       " (780,\n",
       "  'R4XU57TF',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'The mathematical modeling of numerous real-world applications results in hierarchical optimization problems with two decision makers where at least one of them has to solve an optimal control problem of ordinary or partial differential equations. Such models are referred to as bilevel optimal control problems. Here, we first review some different features of bilevel optimal control including important applications, existence results, solution approaches, and optimality conditions. Afterwards, we focus on a specific problem class where parameters appearing in the objective functional of an optimal control problem of partial differential equations have to be reconstructed. After verifying the existence of solutions, necessary optimality conditions are derived by exploiting the optimal value function of the underlying parametric optimal control problem in the context of a relaxation approach.',\n",
       "  'Mehlitz2020'),\n",
       " (782,\n",
       "  'KX88B488',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'This chapter presents a self-contained approach of variational analysis and generalized differentiation to deriving necessary optimality conditions in bilevel optimization with Lipschitzian data. We mainly concentrate on optimistic models, although the developed machinery also applies to pessimistic versions. Some open problems are posed and discussed.',\n",
       "  'Mordukhovich2020'),\n",
       " (784,\n",
       "  'MAVUFTDA',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'In this paper we study constraint qualifications and optimality conditions for bilevel programming problems. We strive to derive checkable constraint qualifications in terms of problem data and applicable optimality conditions. For the bilevel program with convex lower level program we discuss drawbacks of reformulating a bilevel programming problem by the mathematical program with complementarity constraints and present a new sharp necessary optimality condition for the reformulation by the mathematical program with a generalized equation constraint. For the bilevel program with a nonconvex lower level program we propose a relaxed constant positive linear dependence (RCPLD) condition for the combined program.',\n",
       "  'Ye2020'),\n",
       " (786,\n",
       "  'WTDSXS2A',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'We consider a repeated sequential game between a learner, who plays first, and an opponent who responds to the chosen action. We seek to design strategies for the learner to successfully interact with the opponent. While most previous approaches consider known opponent models, we focus on the setting in which the opponent’s model is unknown. To this end, we use kernel-based regularity assumptions to capture and exploit the structure in the opponent’s response. We propose a novel algorithm for the learner when playing against an adversarial sequence of opponents. The algorithm combines ideas from bilevel optimization and online learning to effectively balance between exploration (learning about the opponent’s model) and exploitation (selecting highly rewarding actions for the learner). Our results include algorithm’s regret guarantees that depend on the regularity of the opponent’s response and scale sublinearly with the number of game rounds. Moreover, we specialize our approach to repeated Stackelberg games, and empirically demonstrate its effectiveness in a traffic routing and wildlife conservation task.',\n",
       "  'Sessa2020'),\n",
       " (788,\n",
       "  'GL3T9J78',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Hyperparameter tuning is an active area of research in machine learning, where the aim is to identify the optimal hyperparameters that provide the best performance on the validation set. Hyperparameter tuning is often achieved using naive techniques, such as random search and grid search. However, most of these methods seldom lead to an optimal set of hyperparameters and often get very expensive. In this paper, we propose a bilevel solution method for solving the hyperparameter optimization problem that does not suffer from the drawbacks of the earlier studies. The proposed method is general and can be easily applied to any class of machine learning algorithms. The idea is based on the approximation of the lower level optimal value function mapping, which is an important mapping in bilevel optimization and helps in reducing the bilevel problem to a single level constrained optimization task. The single-level constrained optimization problem is solved using the augmented Lagrangian method. We discuss the theory behind the proposed algorithm and perform extensive computational study on two datasets that confirm the efficiency of the proposed method. We perform a comparative study against grid search, random search and Bayesian optimization techniques that shows that the proposed algorithm is multiple times faster on problems with one or two hyperparameters. The computational gain is expected to be significantly higher as the number of hyperparameters increase. Corresponding to a given hyperparameter most of the techniques in the literature often assume a unique optimal parameter set that minimizes loss on the training set. Such an assumption is often violated by deep learning architectures and the proposed method does not require any such assumption.',\n",
       "  'Sinha2020'),\n",
       " (790,\n",
       "  'JBN9BIKY',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Implicit variables of a mathematical program are variables which do not need to be optimized but are used to model feasibility conditions. They frequently appear in several different problem classes of optimization theory comprising bilevel programming, evaluated multiobjective optimization, or nonlinear optimization problems with slack variables. In order to deal with implicit variables, they are often interpreted as explicit ones. Here, we first point out that this is a light-headed approach which induces artificial locally optimal solutions. Afterwards, we derive various Mordukhovich-stationarity-type necessary optimality conditions which correspond to treating the implicit variables as explicit ones on the one hand, or using them only implicitly to model the constraints on the other. A detailed comparison of the obtained stationarity conditions as well as the associated underlying constraint qualifications will be provided. Overall, we proceed in a fairly general setting relying on modern tools of variational analysis. Finally, we apply our findings to different well-known problem classes of mathematical optimization in order to visualize the obtained theory.',\n",
       "  'Benko2021'),\n",
       " (792,\n",
       "  'BNVSNCSI',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Motivated by emerging applications in wireless sensor networks and large-scale data processing, we consider distributed optimization over directed networks where the agents communicate their information locally to their neighbors to cooperatively minimize a global cost function. We introduce a new unifying distributed constrained optimization model that is characterized as a bilevel optimization problem. This model captures a wide range of existing problems over directed networks including: (i) Distributed optimization with linear constraints; (ii) Distributed unconstrained nonstrongly convex optimization over directed networks. Employing a novel regularization-based relaxation approach and gradient-tracking schemes, we develop an iteratively regularized push-pull gradient algorithm. We establish the consensus and derive new convergence rate statements for suboptimality and infeasibility of the generated iterates for solving the bilevel model. The proposed algorithm and the complexity analysis obtained in this work appear to be new for addressing the bilevel model and also for the two sub-classes of problems. The numerical performance of the proposed algorithm is presented.',\n",
       "  'Yousefian2021'),\n",
       " (794,\n",
       "  'J3F734QF',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Model-agnostic meta-learning (MAML) formulates meta-learning as a bilevel optimization problem, where the inner level solves each subtask based on a shared prior, while the outer level searches for the optimal shared prior by optimizing its aggregated performance over all the subtasks. Despite its empirical success, MAML remains less understood in theory, especially in terms of its global optimality, due to the nonconvexity of the meta-objective (the outer-level objective). To bridge such a gap between theory and practice, we characterize the optimality gap of the stationary points attained by MAML for both reinforcement learning and supervised learning, where the inner-level and outer-level problems are solved via first-order optimization methods. In particular, our characterization connects the optimality gap of such stationary points with (i) the functional geometry of inner-level objectives and (ii) the representation power of function approximators, including linear models and neural networks. To the best of our knowledge, our analysis establishes the global optimality of MAML with nonconvex meta-objectives for the first time.',\n",
       "  'Wang2020'),\n",
       " (796,\n",
       "  '2Z7RQCNR',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  \"In E-commerce, advertising is essential for merchants to reach their target users. The typical objective is to maximize the advertiser's cumulative revenue over a period of time under a budget constraint. In real applications, an advertisement (ad) usually needs to be exposed to the same user multiple times until the user finally contributes revenue (e.g., places an order). However, existing advertising systems mainly focus on the immediate revenue with single ad exposures, ignoring the contribution of each exposure to the final conversion, thus usually falls into suboptimal solutions. In this paper, we formulate the sequential advertising strategy optimization as a dynamic knapsack problem. We propose a theoretically guaranteed bilevel optimization framework, which significantly reduces the solution space of the original optimization space while ensuring the solution quality. To improve the exploration efficiency of reinforcement learning, we also devise an effective action space reduction approach. Extensive offline and online experiments show the superior performance of our approaches over state-of-theart baselines in terms of cumulative revenue.\",\n",
       "  'Hao2020'),\n",
       " (798,\n",
       "  'LXJZFP6M',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'By searching for shared inductive biases across tasks, meta-learning promises to accelerate learning on novel tasks, but with the cost of solving a complex bilevel optimization problem. We introduce and rigorously define the trade-off between accurate modeling and optimization ease in meta-learning. At one end, classic meta-learning algorithms account for the structure of meta-learning but solve a complex optimization problem, while at the other end domain randomized search (otherwise known as joint training) ignores the structure of meta-learning and solves a single level optimization problem. Taking MAML as the representative meta-learning algorithm, we theoretically characterize the trade-off for general nonconvex risk functions as well as linear regression, for which we are able to provide explicit bounds on the errors associated with modeling and optimization. We also empirically study this trade-off for meta-reinforcement learning benchmarks.',\n",
       "  'Gao2020'),\n",
       " (800,\n",
       "  '4QQ8SMFX',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Hyperparameter optimization of neural networks can be elegantly formulated as a bilevel optimization problem. While research on bilevel optimization of neural networks has been dominated by implicit differentiation and unrolling, hypernetworks such as Self-Tuning Networks (STNs) have recently gained traction due to their ability to amortize the optimization of the inner objective. In this paper, we diagnose several subtle pathologies in the training of STNs. Based on these observations, we propose the ?-STN, an improved hypernetwork architecture which stabilizes training and optimizes hyperparameters much more efficiently than STNs. The key idea is to focus on accurately approximating the best-response Jacobian rather than the full best-response function; we achieve this by reparameterizing the hypernetwork and linearizing the network around the current parameters. We demonstrate empirically that our ?-STN can tune regularization hyperparameters (e.g. weight decay, dropout, number of cutout holes) with higher accuracy, faster convergence, and improved stability compared to existing approaches.',\n",
       "  'Bae2020'),\n",
       " (802,\n",
       "  'PCAWQ2X3',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Continual Learning (CL) studies the problem of learning a sequence of tasks, one at a time, such that the learning of each new task does not lead to the deterioration in performance on the previously seen ones while exploiting previously learned features. This paper presents Bilevel Continual Learning (BiCL), a general framework for continual learning that fuses bilevel optimization and recent advances in meta-learning for deep neural networks. BiCL is able to train both deep discriminative and generative models under the conservative setting of the online continual learning. Experimental results show that BiCL provides competitive performance in terms of accuracy for the current task while reducing the effect of catastrophic forgetting.',\n",
       "  'Shaker2021'),\n",
       " (804,\n",
       "  'C7M2VSLM',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Chance constraints provide a principled framework to mitigate the risk of high-impact extreme events by modifying the controllable properties of a system. The low probability and rare occurrence of such events, however, impose severe sampling and computational requirements on classical solution methods that render them impractical. This work proposes a novel sampling-free method for solving rare chance constrained optimization problems affected by uncertainties that follow general Gaussian mixture distributions. By integrating modern developments in large deviation theory with tools from convex analysis and bilevel optimization, we propose tractable formulations that can be solved by off-the-shelf solvers. Our formulations enjoy several advantages compared to classical methods: their size and complexity is independent of event rarity, they do not require linearity or convexity assumptions on system constraints, and under easily verifiable conditions, serve as safe conservative approximations or asymptotically exact reformulations of the true problem. Computational experiments on linear, nonlinear, and PDE-constrained problems from applications in portfolio management, structural engineering, and fluid dynamics illustrate the broad applicability of our method and its advantages over classical sampling-based approaches in terms of both accuracy and efficiency.',\n",
       "  'Tong2022'),\n",
       " (806,\n",
       "  '8TDTCJW6',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Due to the hierarchical structure of many machine learning problems, bilevel programming is becoming more and more important recently, however, the complicated correlation between the inner and outer problem makes it extremely challenging to solve. Although several intuitive algorithms based on the automatic differentiation have been proposed and obtained success in some applications, not much attention has been paid to finding the optimal formulation of the bilevel model. Whether there exists a better formulation is still an open problem. In this paper, we propose an improved bilevel model which converges faster and better compared to the current formulation. We provide theoretical guarantee and evaluation results over two tasks: Data Hyper-Cleaning and Hyper Representation Learning. The empirical results show that our model outperforms the current bilevel model with a great margin. \\\\emph{This is a concurrent work with \\\\citet{liu2020generic} and we submitted to ICML 2020. Now we put it on the arxiv for record.}',\n",
       "  'Li2020'),\n",
       " (808,\n",
       "  'FCM6R2BF',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'This paper presents an application of the energy shaping methodology to control a flexible, elastic Cosserat rod model of a single octopus arm. The novel contributions of this work are two-fold: (i) a control-oriented modeling of the anatomically realistic internal muscular architecture of an octopus arm; and (ii) the integration of these muscle models into the energy shaping control methodology. The control-oriented modeling takes inspiration in equal parts from theories of nonlinear elasticity and energy shaping control. By introducing a stored energy function for muscles, the difficulties associated with explicitly solving the matching conditions of the energy shaping methodology are avoided. The overall control design problem is posed as a bilevel optimization problem. Its solution is obtained through iterative algorithms. The methodology is numerically implemented and demonstrated in a full-scale dynamic simulation environment Elastica. Two bio-inspired numerical experiments involving the control of octopus arms are reported.',\n",
       "  'Chang2021'),\n",
       " (810,\n",
       "  'KW42Z4UD',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Bilevel optimization has arisen as a powerful tool for many machine learning problems such as meta-learning, hyperparameter optimization, and reinforcement learning. In this paper, we investigate the nonconvex-strongly-convex bilevel optimization problem. For deterministic bilevel optimization, we provide a comprehensive convergence rate analysis for two popular algorithms respectively based on approximate implicit differentiation (AID) and iterative differentiation (ITD). For the AID-based method, we orderwisely improve the previous convergence rate analysis due to a more practical parameter selection as well as a warm start strategy, and for the ITD-based method we establish the first theoretical convergence rate. Our analysis also provides a quantitative comparison between ITD and AID based approaches. For stochastic bilevel optimization, we propose a novel algorithm named stocBiO, which features a sample-efficient hypergradient estimator using efficient Jacobian- and Hessian-vector product computations. We provide the convergence rate guarantee for stocBiO, and show that stocBiO outperforms the best known computational complexities orderwisely with respect to the condition number $\\\\kappa$ and the target accuracy $\\\\epsilon$. We further validate our theoretical results and demonstrate the efficiency of bilevel optimization algorithms by the experiments on meta-learning and hyperparameter optimization.',\n",
       "  'Ji2020'),\n",
       " (812,\n",
       "  'V296MRRP',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Active learning is an effective technique for reducing the labeling cost by improving data efficiency. In this work, we propose a novel batch acquisition strategy for active learning in the setting where the model training is performed in a semi-supervised manner. We formulate our approach as a data summarization problem via bilevel optimization, where the queried batch consists of the points that best summarize the unlabeled data pool. We show that our method is highly effective in keyword detection tasks in the regime when only few labeled samples are available.',\n",
       "  'Borsos2021'),\n",
       " (814,\n",
       "  'SUTM5Q9U',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'A classical approach to design controllers for interconnected systems is to assume that the different subsystems operate at different time scales, then design simpler controllers within each time scale, and finally certify stability of the interconnected system via singular perturbation analysis. In this work, we propose an alternative approach that also allows to design the controllers of the individual subsystems separately. However, instead of requiring a sufficiently large time-scale separation, our approach consists of adding a feed-forward term to modify the dynamics of faster systems in order to anticipate the dynamics of slower ones. We present several examples in bilevel optimization and cascade control design, where our approach improves the performance of currently available methods.',\n",
       "  'Picallo2022'),\n",
       " (816,\n",
       "  'IZG9TLMD',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Hyperparameter optimization has remained a central topic within the machine learning community due to its ability to produce state-of-the-art results. With the recent interest growing in the usage of CNNs for time series prediction, we propose the notion of optimizing Hyperparameters in CNNs for the purpose of time series prediction. In this position paper, we give away the idea of modeling the concerned hyperparameter optimization problem using bilevel programming.',\n",
       "  'Seth2021'),\n",
       " (818,\n",
       "  'QTV3M7T9',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Bilevel optimization has recently attracted growing interests due to its wide applications in modern machine learning problems. Although recent studies have characterized the convergence rate for several such popular algorithms, it is still unclear how much further these convergence rates can be improved. In this paper, we address this fundamental question from two perspectives. First, we provide the first-known lower complexity bounds of $\\\\widetilde{\\\\Omega}(\\\\frac{1}{\\\\sqrt{\\\\mu_x}\\\\mu_y})$ and $\\\\widetilde \\\\Omega\\\\big(\\\\frac{1}{\\\\sqrt{\\\\epsilon}}\\\\min\\\\{\\\\frac{1}{\\\\mu_y},\\\\frac{1}{\\\\sqrt{\\\\epsilon^{3}}}\\\\}\\\\big)$ respectively for strongly-convex-strongly-convex and convex-strongly-convex bilevel optimizations. Second, we propose an accelerated bilevel optimizer named AccBiO, for which we provide the first-known complexity bounds without the gradient boundedness assumption (which was made in existing analyses) under the two aforementioned geometries. We also provide significantly tighter upper bounds than the existing complexity when the bounded gradient assumption does hold. We show that AccBiO achieves the optimal results (i.e., the upper and lower bounds match up to logarithmic factors) when the inner-level problem takes a quadratic form with a constant-level condition number. Interestingly, our lower bounds under both geometries are larger than the corresponding optimal complexities of minimax optimization, establishing that bilevel optimization is provably more challenging than minimax optimization.',\n",
       "  'Ji2021a'),\n",
       " (822,\n",
       "  'T23ECC4V',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'We consider a new class of max flow network interdiction problems, where the defender is able to introduce new arcs to the network after the attacker has made their interdiction decisions. We provide an example of when interdiction can result in an increase to the maximum flow, and prove properties of when this restructuring will not increase the value of the minimum cut, which has important practical interpretations for problems of disrupting drug or human trafficking networks. In particular, it demonstrates that disrupting lower levels of these networks will not impact their operations when replacing the disrupted participants is easy. For the bilevel mixed integer linear programming formulation of this problem, we devise a column-and-constraint generation (C&CG) algorithm to solve it. Our approach uses partial information on the feasibility of restructuring plans and is shown to be orders of magnitude faster than previous C&CG methods. We apply this algorithm to the application of disrupting drug trafficking networks. We demonstrate that applying decisions from standard max flow network interdiction problems can result in significantly higher flows than interdictions that account for the restructuring. We also show that interdicting lower level participants is less impactful when recruitment of new participants is allowed.',\n",
       "  'Kosmas2022'),\n",
       " (824,\n",
       "  '74QDVUN8',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Bilevel optimization problems are receiving increasing attention in machine learning as they provide a natural framework for hyperparameter optimization and meta-learning. A key step to tackle these problems is the efficient computation of the gradient of the upper-level objective (hypergradient). In this work, we study stochastic approximation schemes for the hypergradient, which are important when the lower-level problem is empirical risk minimization on a large dataset. The method that we propose is a stochastic variant of the approximate implicit differentiation approach in (Pedregosa, 2016). We provide bounds for the mean square error of the hypergradient approximation, under the assumption that the lower-level problem is accessible only through a stochastic mapping which is a contraction in expectation. In particular, our main bound is agnostic to the choice of the two stochastic solvers employed by the procedure. We provide numerical experiments to support our theoretical analysis and to show the advantage of using stochastic hypergradients in practice.',\n",
       "  'Grazzi2020'),\n",
       " (826,\n",
       "  'QLGS3FMX',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'We provide theoretical convergence guarantees on training Generative Adversarial Networks (GANs) via SGD. We consider learning a target distribution modeled by a 1-layer Generator network with a non-linear activation function $\\\\phi(\\\\cdot)$ parametrized by a $d \\\\times d$ weight matrix $\\\\mathbf W_*$, i.e., $f_*(\\\\mathbf x) = \\\\phi(\\\\mathbf W_* \\\\mathbf x)$. Our main result is that by training the Generator together with a Discriminator according to the Stochastic Gradient Descent-Ascent iteration proposed by Goodfellow et al. yields a Generator distribution that approaches the target distribution of $f_*$. Specifically, we can learn the target distribution within total-variation distance $\\\\epsilon$ using $\\\\tilde O(d^2/\\\\epsilon^2)$ samples which is (near-)information theoretically optimal. Our results apply to a broad class of non-linear activation functions $\\\\phi$, including ReLUs and is enabled by a connection with truncated statistics and an appropriate design of the Discriminator network. Our approach relies on a bilevel optimization framework to show that vanilla SGDA works.',\n",
       "  'Kontonis2020'),\n",
       " (828,\n",
       "  'WSU6GPVE',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Training a fair machine learning model is essential to prevent demographic disparity. Existing techniques for improving model fairness require broad changes in either data preprocessing or model training, rendering themselves difficult-to-adopt for potentially already complex machine learning systems. We address this problem via the lens of bilevel optimization. While keeping the standard training algorithm as an inner optimizer, we incorporate an outer optimizer so as to equip the inner problem with an additional functionality: Adaptively selecting minibatch sizes for the purpose of improving model fairness. Our batch selection algorithm, which we call FairBatch, implements this optimization and supports prominent fairness measures: equal opportunity, equalized odds, and demographic parity. FairBatch comes with a significant implementation benefit -- it does not require any modification to data preprocessing or model training. For instance, a single-line change of PyTorch code for replacing batch selection part of model training suffices to employ FairBatch. Our experiments conducted both on synthetic and benchmark real data demonstrate that FairBatch can provide such functionalities while achieving comparable (or even greater) performances against the state of the arts. Furthermore, FairBatch can readily improve fairness of any pre-trained model simply via fine-tuning. It is also compatible with existing batch selection techniques intended for different purposes, such as faster convergence, thus gracefully achieving multiple purposes.',\n",
       "  'Roh2020'),\n",
       " (830,\n",
       "  '2CW4LMYK',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'We present a regularization method to approach a solution of the pessimistic formulation of ill-posed bilevel problems. This allows to overcome the difficulty arising from the non uniqueness of the lower level problems solutions and responses. We prove existence of approximated solutions, give convergence result using Hoffman-like assumptions. We end with objective value error estimates. © EDP Sciences 2006.',\n",
       "  'Bergounioux2006'),\n",
       " (832,\n",
       "  'RZVEM7E9',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Shortest path network interdiction is a combinatorial optimization problem on an activity network arising in a number of important security-related applications. It is classically formulated as a bilevel maximin problem representing an \"interdictor\" and an \"evader\". The evader tries to move from a source node to the target node along a path of the least cost while the interdictor attempts to frustrate this motion by cutting edges or nodes. The interdiction objective is to find the optimal set of edges to cut given that there is a finite interdiction budget and the interdictor must move first. We reformulate the interdiction problem for stochastic evaders by introducing a model in which the evader follows a Markovian random walk guided by the least-cost path to the target. This model can represent incomplete knowledge about the evader, and the resulting model is a nonlinear 0-1 optimization problem. We then introduce an optimization heuristic based on betweenness centrality that can rapidly find high-quality interdiction solutions by providing a global view of the network.',\n",
       "  'Gutfraind2011'),\n",
       " (834,\n",
       "  'YYCFLDCQ',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'We propose techniques for approximating bilevel optimization problems with non-smooth lower level problems that can have a non-unique solution. To this end, we substitute the expression of a minimizer of the lower level minimization problem with an iterative algorithm that is guaranteed to converge to a minimizer of the problem. Using suitable non-linear proximal distance functions, the update mappings of such an iterative algorithm can be differentiable, notwithstanding the fact that the minimization problem is non-smooth.',\n",
       "  'Ochs2016'),\n",
       " (836,\n",
       "  'VLEB98Z3',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Hyperparameter optimization can be formulated as a bilevel optimization problem, where the optimal parameters on the training set depend on the hyperparameters. We aim to adapt regularization hyperparameters for neural networks by fitting compact approximations to the best-response function, which maps hyperparameters to optimal weights and biases. We show how to construct scalable best-response approximations for neural networks by modeling the best-response as a single network whose hidden units are gated conditionally on the regularizer. We justify this approximation by showing the exact best-response for a shallow linear network with L2-regularized Jacobian can be represented by a similar gating mechanism. We fit this model using a gradient-based hyperparameter optimization algorithm which alternates between approximating the best-response around the current hyperparameters and optimizing the hyperparameters using the approximate best-response function. Unlike other gradient-based approaches, we do not require differentiating the training loss with respect to the hyperparameters, allowing us to tune discrete hyperparameters, data augmentation hyperparameters, and dropout probabilities. Because the hyperparameters are adapted online, our approach discovers hyperparameter schedules that can outperform fixed hyperparameter values. Empirically, our approach outperforms competing hyperparameter optimization methods on large-scale deep learning problems. We call our networks, which update their own hyperparameters online during training, Self-Tuning Networks (STNs).',\n",
       "  'Mackay2019'),\n",
       " (838,\n",
       "  'USKLZNPB',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'This work proposes a new algorithm – the Single-timescale Double-momentum Stochastic Approximation (SUSTAIN) – for tackling stochastic unconstrained bilevel optimization problems. We focus on bilevel problems where the lower level subproblem is strongly-convex and the upper level objective function is smooth. Unlike prior works which rely on two-timescale or double loop techniques, we design a stochastic momentum-assisted gradient estimator for both the upper and lower level updates. The latter allows us to control the error in the stochastic gradient updates due to inaccurate solution to both subproblems. If the upper objective function is smooth but possibly non-convex, we show that SUSTAIN requires O(ε-3/2) iterations (each using O(1) samples) to find an ε-stationary solution. The ε-stationary solution is defined as the point whose squared norm of the gradient of the outer function is less than or equal to ε. The total number of stochastic gradient samples required for the upper and lower level objective functions match the best-known complexity for single-level stochastic gradient algorithms. We also analyze the case when the upper level objective function is strongly-convex.',\n",
       "  'Khanduri2021'),\n",
       " (840,\n",
       "  'IQUPQ2Y2',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  \"Model extraction attacks have become serious issues for service providers using machine learning. We consider an adversarial setting to prevent model extraction under the assumption that attackers will make their best guess on the service provider's model using query accesses, and propose to build a surrogate model that significantly keeps away the predictions of the attacker's model from those of the true model. We formulate the problem as a non-convex constrained bilevel optimization problem and show that for kernel models, it can be transformed into a non-convex 1-quadratically constrained quadratic program with a polynomial-time algorithm to find the global optimum. Moreover, we give a tractable transformation and an algorithm for more complicated models that are learned by using stochastic gradient descent-based algorithms. Numerical experiments show that the surrogate model performs well compared with existing defense models when the difference between the attacker's and service provider's distributions is large. We also empirically confirm the generalization ability of the surrogate model.\",\n",
       "  'Mori2021'),\n",
       " (842,\n",
       "  'T5IYSD49',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  \"This article concerns an optimal crowd motion control problem in which the crowd features a structure given by its organization into $N$ groups (participants) each one spatially confined in a set. The overall optimal control problem consists in driving the ensemble of sets as close as possible to a given point (the 'exit') while the population in each set minimizes its control effort subject to its sweeping dynamics with a controlled state dependent velocity drift. In order to capture the conflict between the goal of the overall population and those of the various groups, the problem is cast as a bilevel optimization framework. A key challenge of this problem consists in bringing together two quite different paradigms: bilevel programming and sweeping dynamics with a controlled drift. Necessary conditions of optimality in the form of a Maximum Principle of Pontryagin in the Gamkrelidze framework are derived. These conditions are then used to solve a simple illustrative example with two participants, emphasizing the interaction between them.\",\n",
       "  'Cao2022'),\n",
       " (844,\n",
       "  'JFMRWU7H',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  \"There has been a growing interest in developing data-driven, and in particular deep neural network (DNN) based methods for modern communication tasks. These methods achieve state-of-the-art performance for a few popular wireless resource allocation problems, while requiring less computational efforts, less resources for acquiring channel state information (CSI), etc. However, it is often challenging for these approaches to learn in a dynamic environment. This work develops a new approach that enables data-driven methods to continuously learn and optimize wireless resource allocation in a dynamic environment. Specifically, we consider an 'episodically dynamic' setting where the environment statistics change in 'episodes,' and in each episode the environment is stationary. We propose to build the notion of continual learning (CL) into wireless system design, so that the learning model can incrementally adapt to the new episodes, without forgetting knowledge learned from the previous episodes. We demonstrate the effectiveness of the CL approach by integrating it with three popular DNN based models for power control, beamforming and multi-user MIMO, respectively, and testing using both synthetic and ray-tracing based data sets. These numerical results show that the proposed CL approach is not only able to adapt to the new scenarios quickly and seamlessly, but importantly, it also maintains high performance over the previously encountered scenarios as well.\",\n",
       "  'Sun2022'),\n",
       " (848,\n",
       "  'UQ3ZP9WV',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'A large number of application problems involve two levels of optimization, where one optimization task is nested inside the other. These problems are known as bilevel optimization problems and have been studied by both classical optimization community and evolutionary optimization community. Most of the solution procedures proposed until now are either computationally very expensive or applicable to only small classes of bilevel optimization problems adhering to mathematically simplifying assumptions. In this paper, we propose an evolutionary optimization method that tries to reduce the computational expense by iteratively approximating two important mappings in bilevel optimization; namely, the lower level rational reaction mapping and the lower level optimal value function mapping. The algorithm has been tested on a large number of test problems and comparisons have been performed with other algorithms. The results show the performance gain to be quite significant. To the best knowledge of the authors, a combined theory-based and population-based solution procedure utilizing mappings has not been suggested yet for bilevel problems.',\n",
       "  'Sinha2020a'),\n",
       " (850,\n",
       "  'U4WGC3PK',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Bilevel programs are optimization problems where some variables are solutions to optimization problems themselves, and they arise in a variety of control applications, including: control of vehicle traffic networks, inverse reinforcement learning and inverse optimization, and robust control for human-automation systems. This paper develops a duality-based approach to solving bilevel programs where the lower level problem is convex. Our approach is to use partial dualization to construct a new dual function that is differentiable, unlike the Lagrangian dual that is only directionally differentiable. We use our dual to define a duality-based reformulation of bilevel programs, prove equivalence of our reformulation with the original bilevel program, and then introduce regularization to ensure constraint qualification holds. These technical results about our new dual and regularized duality-based reformulation are used to provide theoretical justification for an algorithm we construct for solving bilevel programs with a convex lower level, and we conclude by demonstrating the efficacy of our algorithm by solving two practical instances of bilevel programs.',\n",
       "  'Ouattara2018'),\n",
       " (852,\n",
       "  'QD2ZTDNF',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'We consider a bilevel optimatisation method for inverse linear atmospheric dispersion problems where both linear and non-linear model parameters are to be determined. We propose that a smooth weighted Mahalanobis distance function is used and derive sufficient conditions for when the follower problem has local strict convexity. A few toy-models are presented where local strict convexity and ill-posedness of the inverse problem are explored, indeed the smooth distance function is compared and contrasted to linear and piecewise linear ones. The bilevel optimisation method is then applied to sensor data collected in wind tunnel experiments of a neutral gas release in urban environments (MODITIC).',\n",
       "  'Brannstrom2016'),\n",
       " (855,\n",
       "  'BIBXG6VL',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Bilevel optimization has been widely used in decision-making process. However, there still lacks an efficient algorithm to determine an optimal solution of a bilevel optimization problem, especially for a large-size problem. To bridge the gap, this paper proposes an efficient decomposition algorithm for a general bilevel linear programming(GBLP). The simulation results on large-size testing system demonstrate its correctness and efficiency.',\n",
       "  'Liu2016'),\n",
       " (857,\n",
       "  'TIMTYM6M',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'A learning approach for selecting regularization parameters in multi-penalty Tikhonov regularization is investigated. It leads to a bilevel optimization problem, where the lower level problem is a Tikhonov regularized problem parameterized in the regularization parameters. Conditions which ensure the existence of solutions to the bilevel optimization problem are derived, and these conditions are verified for two relevant examples. Difficulties arising from the possible lack of convexity of the lower level problems are discussed. Optimality conditions are given provided that a reasonable constraint qualification holds. Finally, results from numerical experiments used to test the developed theory are presented.',\n",
       "  'Holler2018'),\n",
       " (859,\n",
       "  'R22I3PLQ',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'The generalized bilevel programming problem (GBLP) is a bilevel mathematical program where the lower level is a variational inequality. In this paper we prove that if the objective function of a GBLP is uniformly Lipschitz continuous in the lower level decision variable with respect to the upper level decision variable, then using certain uniform parametric error bounds as penalty functions gives single level problems equivalent to the GBLP. Several local and global uniform parametric error bounds are presented, and assumptions guaranteeing that they apply are discussed. We then derive Kuhn-Tucker-type necessary optimality conditions by using exact penalty formulations and nonsmooth analysis.',\n",
       "  'Ye1997'),\n",
       " (870,\n",
       "  'KX96NT9Y',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'In this paper, we consider a simple bilevel program where the lower level program is a nonconvex minimization problem with a convex set constraint and the upper level program has a convex set constraint. By using the value function of the lower level program, we reformulate the bilevel program as a single level optimization problem with a nonsmooth inequality constraint and a convex set constraint. To deal with such a nonsmooth and nonconvex optimization problem, we design a smoothing projected gradient algorithm for a general optimization problem with a nonsmooth inequality constraint and a convex set constraint. We show that, if the sequence of penalty parameters is bounded then any accumulation point is a stationary point of the nonsmooth optimization problem and, if the generated sequence is convergent and the extended Mangasarian-Fromovitz constraint qualification holds at the limit then the limit point is a stationary point of the nonsmooth optimization problem. We apply the smoothing projected gradient algorithm to the bilevel program if a calmness condition holds and to an approximate bilevel program otherwise. Preliminary numerical experiments show that the algorithm is efficient for solving the simple bilevel program. © 2013 Springer-Verlag Berlin Heidelberg and Mathematical Optimization Society.',\n",
       "  'Lin2014'),\n",
       " (872,\n",
       "  '8VNM7A2I',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Bilevel programming problems are often reformulated using the Karush-Kuhn-Tucker conditions for the lower level problem resulting in a mathematical program with complementarity constraints(MPCC). Clearly, both problems are closely related. But the answer to the question posed is \"No\" even in the case when the lower level programming problem is a parametric convex optimization problem. This is not obvious and concerns local optimal solutions. We show that global optimal solutions of the MPCC correspond to global optimal solutions of the bilevel problem provided the lower-level problem satisfies the Slater\\'s constraint qualification. We also show by examples that this correspondence can fail if the Slater\\'s constraint qualification fails to hold at lower-level. When we consider the local solutions, the relationship between the bilevel problem and its corresponding MPCC is more complicated. We also demonstrate the issues relating to a local minimum through examples.',\n",
       "  'Dempe2012'),\n",
       " (881,\n",
       "  '9Y76YMZI',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Lagrange multipliers used to be viewed as auxiliary variables introduced in a problem of constrained minimization in order to write first-order optimality conditions formally as a system of equations. Modern applications, with their emphasis on numerical methods and more complicated side conditions than equations, have demanded deeper understanding of the concept and how it fits into a larger theoretical picture. A major line of research has been the nonsmooth geometry of one-sided tangent and normal vectors to the set of points satisfying the given constraints. Another has been the game-theoretic role of multiplier vectors as solutions to a dual problem. Interpretations as generalized derivatives of the optimal value with respect to problem parameters have also been explored. Lagrange multipliers are now being seen as arising from a general rule for the subdifferentiation of a nonsmooth objective function which allows black-and-white constraints to be replaced by penalty expressions. This paper traces such themes in the current theory of Lagrange multipliers, providing along the way a free-standing exposition of basic nonsmooth analysis as motivated by and applied to this subject.',\n",
       "  'Rockafellar1993'),\n",
       " (883,\n",
       "  'WBVL9TSH',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'We discuss Lagrange multiplier rules from a variational perspective. This allows us to highlight many of the issues involved and also to illustrate how broadly an abstract version can be applied.',\n",
       "  'Borwein2016'),\n",
       " (887,\n",
       "  'HYPCVZJM',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'We present a short elementary proof of the Lagrange multiplier theorem for equality-constrained optimization. Most proofs in the literature rely on advanced analysis concepts such as the implicit function theorem, whereas elementary proofs tend to be long and involved. By contrast, our proof uses only basic facts from linear algebra, the definition of differentiability, the critical-point condition for unconstrained minima, and the fact that a continuous function attains its minimum over a closed ball. © 2011 Springer-Verlag.',\n",
       "  'Brezhneva2012'),\n",
       " (889,\n",
       "  'UX3MGI8B',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Optimality Conditions in Convex Optimizationexplores an important and central issue in the field of convex optimization: optimality conditions. It brings together the most important and recent results in this area that have been scattered in the literature-notably in the area of convex analysis-essential in developing many of the important results in this book, and not usually found in conventional texts. Unlike other books on convex optimization, which usually discuss algorithms along with some basic theory, the sole focus of this book is on fundamental and advanced convex optimization theory. Although many results presented in the book can also be proved in infinite dimensions, the authors focus on finite dimensions to allow for much deeper results and a better understanding of the structures involved in a convex optimization problem. They address semi-infinite optimization problems; approximate solution concepts of convex optimization problems; and some classes of non-convex problems which can be studied using the tools of convex analysis. They include examples wherever needed, provide details of major results, and discuss proofs of the main results.',\n",
       "  'Dutta2011'),\n",
       " (891,\n",
       "  'ULQGKA8I',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'We show that a semi-infinite quasi-convex program with certain regularity conditions possesses finitely constrained subprograms with the same optimal value. This result is applied to various problems. © 1981 The Mathematical Programming Society.',\n",
       "  'Borwein1981'),\n",
       " (893,\n",
       "  'DYHMYXDI',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Thismonograph is about a class of optimization algorithms called prox- imal algorithms.Much like Newton’smethod is a standard tool for solv- ing unconstrained smooth optimization problems of modest size, proxi- mal algorithms can be viewed as an analogous tool for nonsmooth, con- strained, large-scale, or distributed versions of these problems. They are very generally applicable, but are especially well-suited to problems of substantial recent interest involving large or high-dimensional datasets. Proximal methods sit at a higher level of abstraction than classical al- gorithms like Newton’s method: the base operation is evaluating the proximal operator of a function, which itself involves solving a small convex optimization problem. These subproblems, which generalize the problem of projecting a point onto a convex set, often admit closed- form solutions or can be solved very quickly with standard or simple specialized methods. Here, we discuss the many different interpreta- tions of proximal operators and algorithms, describe their connections to many other topics in optimization and applied mathematics, survey some popular algorithms, and provide a large number of examples of proximal operators that commonly arise in practice.',\n",
       "  'Parikh2014'),\n",
       " (896,\n",
       "  'GX8DJPYF',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'We propose an approach to find the optimal value of a convex semi-infinite program (SIP) that involves identifying a finite set of relevant constraints by solving a finite-dimensional global maximization problem. One of the major advantages of our approach is that it admits a plug-and-play module where any suitable global optimization algorithm can be employed to obtain the optimal value of the SIP. As an example, we propose a simulated annealing based algorithm which is useful especially when the constraint index set is high-dimensional. A proof of convergence of the algorithm is included, and the performance and accuracy of the algorithm itself are illustrated on several benchmark SIPs lifted from the literature.',\n",
       "  'Das2022'),\n",
       " (915,\n",
       "  'WRNQAA7T',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Variational regularization techniques are dominant in the field of mathematical imaging. A drawback of these techniques is that they are dependent on a number of parameters which have to be set by the user. A by-now common strategy to resolve this issue is to learn these parameters from data. While mathematically appealing, this strategy leads to a nested optimization problem (known as bilevel optimization) which is computationally very difficult to handle. It is common when solving the upper-level problem to assume access to exact solutions of the lower-level problem, which is practically infeasible. In this work we propose to solve these problems using inexact derivative-free optimization algorithms which never require exact lower-level problem solutions, but instead assume access to approximate solutions with controllable accuracy, which is achievable in practice. We prove global convergence and a worst-case complexity bound for our approach. We test our proposed framework on ROF denoising and learning MRI sampling patterns. Dynamically adjusting the lower-level accuracy yields learned parameters with similar reconstruction quality as high-accuracy evaluations but with dramatic reductions in computational work (up to 100 times faster in some cases).',\n",
       "  'Ehrhardt2021'),\n",
       " (928,\n",
       "  'J6UB5VSV',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Abstract A simple theory is developed to assess quantitatively the mechanism of fatigue crack propagation in metals. The basic laws governing fatigue are derived theoretically for failure in both the high and low stress regions, and the material parameters controlling crack propagation determined. The theory is compared with that developed in recent years using linear fracture mechanics.',\n",
       "  'Tomkins1968'),\n",
       " (930,\n",
       "  '88L5JPY3',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'In this paper we propose an online learning algorithm, a general randomized sparse Kaczmarz method, for generating sparse approximate solutions to linear systems and present learning theory analysis for its convergence. Under a mild assumption covering the case of noisy random measurements in the sampling process or nonlinear regression function, we show that the algorithm converges in expectation if and only if the step size sequence {ηt}tεℕ satisfies limt→∞ηt= 0 and ∑∞t=1ηt = ∞. Convergence rates are also obtained and linear convergence is shown to be impossible under the assumption of positive variance of the sampling process. A sufficient condition for almost sure convergence is derived with an additional restriction ∑∞t=1η2t<∞. Our novel analysis is performed by interpreting the randomized sparse Kaczmarz method as a special online mirror descent algorithm with a nondifferentiable mirror map and using the Bregman distance. The sufficient and necessary conditions are derived by establishing a restricted variant of strong convexity for the involved generalization error and using the special structures of the soft-thresholding operator.',\n",
       "  'Lei2018'),\n",
       " (932,\n",
       "  'CZL7RGSJ',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'The slip band formed in a grain on the material surface is a preferential site for crack initiation during low strain fatigue of polycrystalline metals. The forward and reverse plastic flow within the slip band is modeled in the present study by dislocations with different signs moving on two closely located layers, and it is assumed that their movement is irreversible. Based on the model, the monotonic buildup of dislocation dipoles piled up at the grain boundary is systematically derived using the theory of continuously distributed dislocations. This buildup is associated with the progress of extrusion or intrusion. The number of stress cycles up to the initiation of a crack of the grain size order is defined as the cycle when the stored strain energy of accumulated dislocations reaches a critical value. The relation between the initiation life and the plastic strain range derived theoretically is in agreement with a Coffin-Manson type law, and that between the fatigue strength and the grain size is expressed in an equation of the Petch type.',\n",
       "  'Tanaka1981'),\n",
       " (935,\n",
       "  'R69KWBZB',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Roughly speaking, the main result of this work states that the limit (2) exists even if A is nonlinear (and multivalued) provided the nonlinear analogue of (1) holds for A',\n",
       "  'Crandall1971'),\n",
       " (937,\n",
       "  'N44RWBNR',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'This letter treats a novel maximum hands-off control problem in which two types of sparsity are considered. Our optimization problem is mathematically formulated as an L0 optimal control problem with an ℓ 0 constraint. This optimal control has the minimum length of support and a small number of activated components among all control inputs that steer the system to a target state within a fixed time duration. Since the formulated problem is combinatorial, we introduce a convex relaxation problem for its computational tractability. We show a sufficient condition under which our sparse optimization problem boils down to the convex optimization problem and give an existence theorem of the optimal controls. The proposed control is illustrated through a numerical example.',\n",
       "  'Ikeda2021'),\n",
       " (941,\n",
       "  'GVSL54Z6',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  \"In the present paper, the dislocation-based antiplane fracture mechanics is employed for the analysis of Mode III crack within nonlocal and (strain) gradient elasticity of bi-Helmholtz type. These frameworks are appropriate candidates of generalized continua for regularization of classical singularities of defects such as dislocations. Within nonlocal elasticity of bi-Helmholtz type, nonlocal stress is regularized, while the strain field remain singular. Interestingly, gradient elasticity of bi-Helmholtz type (second strain gradient elasticity) eliminates all physical singularities of discrete dislocation including stress and strain fields and dislocation density while the so-called total stress tensor still contains singularity at the dislocation core. Based on the distribution of dislocations, a fracture theory with nonsingular stress field is formulated in these nonlocal and gradient theories. Strain and displacement fields within nonlocal fracture theory are identical to the classical ones. In contrast, gradient elasticity of bi-Helmholtz type leads to a full nonsingular fracture theory in which stress, strain and dislocation density are regularized. However, the singular total stress of a discrete dislocation results in singular total stress of the plane weakened by a crack. Within classical fracture mechanics, Barenblatt's cohesive fracture theory assumes that cohesive forces is distributed ahead of the crack tip to model crack tip plasticity and remove the stress singularity. Here, considering the dislocations as the carriers of plasticity, the crack tip plasticity is captured without any assumption. Once the crack is modeled by distributing the dislocations along its surface, due to the gradient theory, the distribution function gives rise to a non-zero plastic distortion ahead of the crack. Consequently, regularized solutions of crack are developed incorporating crack tip plasticity.\",\n",
       "  'Mousavi2016'),\n",
       " (947,\n",
       "  'VXNQ6MRC',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'The Kaczmarz algorithm is popular for iteratively solving an over determined system of linear equations. Randomized version of the Kaczmarz algorithm can converge exponentially and independent of number of equations. Recently an algorithm for finding sparse solution to a linear system of equations has been proposed based on weighted randomized Kaczmarz algorithm. These algorithms solves single measurement vector problem, however there are applications where multiple-measurements are available. In this work, the objective is to solve a multiple measurement vector problem with common sparse support by modifying the sparse randomized Kaczmarz algorithm. We have also modeled the problem of face recognition from video as the multiple measurement vector problem and solved using our proposed technique. We have compared the proposed algorithm with state-of-art spectral projected gradient algorithm for multiple measurement vectors on both real and synthetic datasets. The Monte Carlo simulations confirms that our proposed algorithm has better recovery and convergence rate than the MMV version of spectral projected gradient algorithm under fairness constraints.',\n",
       "  'Aggarwal2014'),\n",
       " (949,\n",
       "  '4KFX7LWU',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'The bilevel programming problem (BLPP) is a sequence of two optimization problems where the constraint region of the first is determined implicitly by the solution to the second. In this article it is first shown that the linear BLPP is equivalent to maximizing a linear function over a feasible region comprised of connected faces and edges of the original polyhedral constraint set. The solution is shown to occur at a vertex of that set. Next, under assumptions of differentiability, first-order necessary optimality conditions are developed for the more general BLPP, and a potentially equivalent mathematical program is formulated. Finally, the relationship between the solution to this problem and Pareto optimality is discussed and a number of examples given.',\n",
       "  'Bard1984'),\n",
       " (956,\n",
       "  'TV93ENB2',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'The randomized version of the Kaczmarz method for the solution of consistent linear systems is known to converge linearly in expectation. And even in the possibly inconsistent case, when only noisy data is given, the iterates are expected to reach an error threshold in the order of the noise-level with the same rate as in the noiseless case. In this work we show that the same also holds for the iterates of the recently proposed randomized sparse Kaczmarz method for recovery of sparse solutions. Furthermore we consider the more general setting of convex feasibility problems and their solution by the method of randomized Bregman projections. This is motivated by the observation that, similarly to the Kaczmarz method, the Sparse Kaczmarz method can also be interpreted as an iterative Bregman projection method to solve a convex feasibility problem. We obtain expected sublinear rates for Bregman projections with respect to a general strongly convex function. Moreover, even linear rates are expected for Bregman projections with respect to smooth or piecewise linear-quadratic functions, and also the regularized nuclear norm, which is used in the area of low rank matrix problems.',\n",
       "  'Schopfer2019'),\n",
       " (959,\n",
       "  'NDN3GLWN',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'The purpose of the paper is threefold: (1) To develop a useful error bound for the method of alternating projections which is relatively easy to compute and remember; (2) To exhibit a counterexample to a conjecture of Kayalar and Weinert; (3) To show that (in the case of at least three subspaces) any error bound which only depends on the angles between the various subspaces involved canneverbe sharp. © 1997 Academic Press.',\n",
       "  'Deutsch1997'),\n",
       " (961,\n",
       "  'H6ZP6LB5',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  \"We consider linear equations y = Φx where y is a given vector in ℝ n and Φ is a given n × m matrix with n < m ≤ τ n, and we wish to solve for x ε ℝ m. We suppose that the columns of Φ are normalized to the unit ℓ 2-norm, and we place uniform measure on such Φ. We prove the existence of ρ = ρ(τ) > 0 so that for large n and for all Φ's except a negligible fraction, the following property holds: For every y having a representation y = Φx 0 by a coefficient vector x 0 ∈ ℝ m with fewer than p · n nonzeros, the solution x 1 of the ℓ-minimization problem min ||x|| 1 subject to Φx = y is unique and equal to x 0. In contrast, heuristic attempts to sparsely solve such systems - greedy algorithms and thresholding - perform poorly in this challenging setting. The techniques include the use of random proportional embeddings and almost-spherical sections in Banach space theory, and deviation bounds for the eigenvalues of random Wishart matrices. © 2006 Wiley Periodicals, Inc.\",\n",
       "  'Donoho2006'),\n",
       " (963,\n",
       "  'GUWQGCPC',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'The Kaczmarz algorithm is a popular solver for overdetermined linear systems due to its simplicity and speed. In this paper, we propose a modification that speeds up the convergence of the randomized Kaczmarz algorithm for systems of linear equations with sparse solutions. The speedup is achieved by projecting every iterate onto a weighted row of the linear system while maintaining the random row selection criteria of Strohmer and Vershynin. The weights are chosen to attenuate the contribution of row elements that lie outside of the estimated support of the sparse solution. While the Kaczmarz algorithm and its variants can only find solutions to overdetermined linear systems, our algorithm surprisingly succeeds in finding sparse solutions to underdetermined linear systems as well. We present empirical studies which demonstrate the acceleration in convergence to the sparse solution using this modified approach in the overdetermined case. We also demonstrate the sparse recovery capabilities of our approach in the underdetermined case and compare the performance with that of $\\\\ell_1$ minimization.',\n",
       "  'Mansour2013'),\n",
       " (965,\n",
       "  'U9D53TQ3',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  '1 Problem Definition • Input: we are given m equalities a T i x = b i , where each b i ∈ R and each a i ∈ R n for i = 1, 2,. .. , m. • Output: a point x * ∈ R n that satisfies all m inequalities, a T i x * = b i (we assume such a point exists). Written in matrix form, we want solve a linear system Ax = b. In this notation, element i of b is given by b i (so b ∈ R m) and each row i of A is given by a T i (so A ∈ R m×n).',\n",
       "  'Schmidt2015'),\n",
       " (967,\n",
       "  '4KF9M5N8',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'This paper gives a review of the features of Bilevel Linear Programming (BLP) by presenting prior results as well as providing new results, including the capability of the problem to formulate any piecewise linear function and its connection to other optimization problems. The paper also surveys the applications and the algorithms of BLP; the NP-hardness of BLP did not prevent the success of several applications of the model to real world problems. Certain confusing representations in the literature are clarified. © 1993.',\n",
       "  'Ben-ayed1993'),\n",
       " (969,\n",
       "  'TLT6W8WR',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'We present an ordinary differential equation approach to the analysis of algorithms for constructing l 1 minimizing solutions to underdetermined linear systems of full rank. It involves a relaxed minimization problem whose minimum is independent of the relaxation parameter. An advantage of using the ordinary differential equations is that energy methods can be used to prove convergence. The connection to the discrete algorithms is provided by the Crandall-Liggett theory of monotone nonlinear semigroups. We illustrate the effectiveness of the discrete optimization algorithm in some sparse array imaging problems. © 2012 IOP Publishing Ltd.',\n",
       "  'Moscoso2012'),\n",
       " (971,\n",
       "  'AEKQM9AP',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'The Kaczmarz algorithm is an iterative method for solving systems of linear equations. We introduce a modified Kaczmarz algorithm for solving systems of linear equations in a distributed environment, i.e. the equations within the system are distributed over multiple nodes within a network. The modification we introduce is designed for a network with a tree structure that allows for passage of solution estimates between the nodes in the network. We prove that the modified algorithm converges under no additional assumptions on the equations. We demonstrate that the algorithm converges to the solution, or the solution of minimal norm, when the system is consistent. We also demonstrate that in the case of an inconsistent system of equations, the modified relaxed Kaczmarz algorithm converges to a weighted least squares solution as the relaxation parameter approaches 0. MSC Codes 65F10, 15A06, 68W15, 41A65',\n",
       "  'Hegde2019'),\n",
       " (972,\n",
       "  'XP2LVELR',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'In this work we present a new greedy algorithm for sparse approximation called LocOMP. LocOMP is meant to be run on local dictionaries made of atoms with much shorter supports than the signal length. This notably encompasses shift-invariant dictionaries and timefrequency dictionaries, be they monoscale or multiscale. In this case, very fast implementations of Matching Pursuit are already available. LocOMP is almost as fast as Matching Pursuit while approaching the signal almost as well as the much slower Orthogonal Matching Pursuit. © 2011 Elsevier B.V. All rights reserved.',\n",
       "  'Mailhe2011'),\n",
       " (974,\n",
       "  '9VZSGX4V',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Randomized iterative algorithms have recently been proposed to solve large-scale linear systems. In this paper, we present a simple randomized extended average block Kaczmarz algorithm that exponentially converges in the mean square to the unique minimum norm least squares solution of a given linear system of equations. The proposed algorithm is pseudoinverse-free and therefore different from the projection-based randomized double block Kaczmarz algorithm of Needell, Zhao, and Zouzias [Linear Algebra Appl., 484 (2015), pp. 322-343]. We emphasize that our method works for all types of linear systems (consistent or inconsistent, overdetermined or underdetermined, full-rank or rank-deficient). Moreover, our approach can be implemented for parallel computation, yielding remarkable improvements in computational time. Numerical examples are given to show the efficiency of the new algorithm.',\n",
       "  'Du2020'),\n",
       " (976,\n",
       "  'ACY2Z6BX',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'A vector with at most k nonzeros is called k-sparse. Given a linear system Ax=b (the rows of A are r-sparse), it has been shown by Damaschke [1] that enumerating nonnegative k-sparse solutions is fixed-parameter tractable. They present an algorithm reaching an O*(r2k) time bound. Since this problem is closely related to the hitting set problem, they raise the question whether it is possible to improve this time bound to O*(rk). This paper investigates this problem and discovers that a refined analysis of a modified version of their algorithm leads to a smaller time bound O*((4r)k).',\n",
       "  'Yuan2014'),\n",
       " (978,\n",
       "  'SYLW8VD7',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'In this paper we explore the duality relations that characterize least norm problems. The paper starts by presenting a new Minimum Norm Duality (MND) theorem, one that considers the distance between two convex sets. Roughly speaking the new theorem says that the shortest distance between the two sets is equal to the maximal \"separation\" between the sets, where the term \"separation\" refers to the distance between a pair of parallel hyperplanes that separates the two sets. The second part of the paper brings several examples of applications. The examples teach valuable lessons about the role of duality in least norm problems, and reveal new features of these problems. One lesson exposes the polar decomposition which characterizes the \"solution\" of an inconsistent system of linear inequalities. Another lesson reveals the close links between the MND theorem, theorems of the alternatives, steepest descent directions, and constructive optimality conditions. © 2006 Elsevier Inc. All rights reserved.',\n",
       "  'Dax2006'),\n",
       " (980,\n",
       "  'KNYR4QI4',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'First-order optimization algorithms are widely used today. Two standard building blocks in these algorithms are proximal operators (proximals) and gradients. Although gradients can be computed for a wide array of functions, explicit proximal formulas are known for only limited classes of functions. We provide an algorithm, HJ-Prox, for accurately approximating such proximals. This is derived from a collection of relations between proximals, Moreau envelopes, Hamilton–Jacobi (HJ) equations, heat equations, and Monte Carlo sampling. In particular, HJ-Prox smoothly approximates the Moreau envelope and its gradient. The smoothness can be adjusted to act as a denoiser. Our approach applies even when functions are accessible only by (possibly noisy) black box samples. We show that HJ-Prox is effective numerically via several examples.',\n",
       "  'Osher2023'),\n",
       " (983,\n",
       "  '2PLBN8GG',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Proximal mappings, which generalize projection mappings, were introduced by Moreau and shown to be valuable in understanding the subgradient properties of convex functions. Proximal mappings subsequently turned out to be important also in numerical methods of optimization and the solution of nonlinear partial differential equations and variational inequalities. Here it is shown that, when a convex function is propagated through time by a generalized Hamilton-Jacobi partial differential equation with a Hamiltonian that concave in the state and convex in the co-state, the associated proximal mapping exhibits locally Lipschitz dependence on time. Furthermore, the subgradient mapping associated of the value function associated with this mapping is graphically Lipschitzian.',\n",
       "  'Rockafellar2006'),\n",
       " (985,\n",
       "  'LMIT9F5F',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'The efficient point algorithm proposed by J. F. Bard for the computation of the solution of the Linear Two-Stage Optimization Problem does not always converge to the desired solution. A counterexample is provided and the reasons for this lack of convergence are discussed.',\n",
       "  'Haurie1990'),\n",
       " (987,\n",
       "  'BXQFVNZG',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  \"The method of alternating projections involves orthogonally projecting an element of a Hilbert space onto a collection of closed subspaces. It is known that the resulting sequence always converges in norm if the projections are taken periodically, or even quasiperiodically. We present proofs of such well known results, and offer an original proof for the case of two closed subspaces, known as von Neumann's theorem. Additionally, it is known that this sequence always converges with respect to the weak topology, regardless of the order projections are taken in. By focusing on projections directly, rather than the more general case of contractions considered previously in the literature, we are able to give a simpler proof of this result. We end by presenting a technical construction taken from a recent paper, of a sequence for which we do not have convergence in norm.\",\n",
       "  'Deutsch2001'),\n",
       " (989,\n",
       "  'G83MTYHE',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'We propose an algorithm for mcta-lcaming that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.',\n",
       "  'Finn2017'),\n",
       " (991,\n",
       "  'BQAQ6S6P',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  '\"Energy\" models for continuous domains can be applied to many problems, but often suffer from high computational expense in training, due to the need to repeatedly minimize the energy function to high accuracy. This paper considers a modified setting, where the model is trained in terms of results after optimization is truncated to a fixed number of iterations. We derive \"backpropagating\" versions of gradient descent, heavy-ball and LBFGS. These are simple to use, as they require as input only routines to compute the gradient of the energy with respect to the domain and parameters. Experimental results on denoising and image labeling problems show that learning with truncated optimization greatly reduces computational expense compared to \"full\" fitting.',\n",
       "  'Domke2012'),\n",
       " (996,\n",
       "  'Y9MKWRDB',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Optimization problems involving stochastic models occur in almost all areas of science and engineering, such as telecommunications, medicine, and finance. Their existence compels a need for rigorous ways of formulating, analyzing, and solving such problems. This book focuses on optimization problems involving uncertain parameters and covers the theoretical foundations and recent advances in areas where stochastic models are available. Readers will find coverage of the basic concepts of modeling these problems, including recourse actions and the nonanticipativity principle. The book also includes the theory of two-stage and multistage stochastic programming problems; the current state of the theory on chance (probabilistic) constraints, including the structure of the problems, optimality theory, and duality; and statistical inference in and risk-averse approaches to stochastic programming.',\n",
       "  'Shapiro2021'),\n",
       " (1000,\n",
       "  'AVTWNRQ4',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'This paper analyzes a two-timescale stochastic algorithm framework for bilevel optimization. Bilevel optimization is a class of problems which exhibit a two-level structure, and its goal is to minimize an outer objective function with variables which are constrained to be the optimal solution to an (inner) optimization problem. We consider the case when the inner problem is unconstrained and strongly convex, while the outer problem is constrained and has a smooth objective function. We propose a two-timescale stochastic approximation (TTSA) algorithm for tackling such a bilevel problem. In the algorithm, a stochastic gradient update with a larger step size is used for the inner problem, while a projected stochastic gradient update with a smaller step size is used for the outer problem. We analyze the convergence rates for the TTSA algorithm under various settings: when the outer problem is strongly convex (resp.~weakly convex), the TTSA algorithm finds an $\\\\mathcal{O}(K^{-2/3})$-optimal (resp.~$\\\\mathcal{O}(K^{-2/5})$-stationary) solution, where $K$ is the total iteration number. As an application, we show that a two-timescale natural actor-critic proximal policy optimization algorithm can be viewed as a special case of our TTSA framework. Importantly, the natural actor-critic algorithm is shown to converge at a rate of $\\\\mathcal{O}(K^{-1/4})$ in terms of the gap in expected discounted reward compared to a global optimal policy.',\n",
       "  'Hong2023'),\n",
       " (1002,\n",
       "  '32XPMGIN',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Bilevel optimization has been widely applied in many important machine learning applications such as hyperparameter optimization and meta-learning. Recently, several momentum-based algorithms have been proposed to solve bilevel optimization problems faster. However, those momentum-based algorithms do not achieve provably better computational complexity than Oe(ǫ-2) of the SGD-based algorithm. In this paper, we propose two new algorithms for bilevel optimization, where the first algorithm adopts momentum-based recursive iterations, and the second algorithm adopts recursive gradient estimations in nested loops to decrease the variance. We show that both algorithms achieve the complexity of Oe(ǫ-1.5), which outperforms all existing algorithms by the order of magnitude. Our experiments validate our theoretical results and demonstrate the superior empirical performance of our algorithms in hyperparameter applications.',\n",
       "  'Yang2021'),\n",
       " (1004,\n",
       "  'LK4LNP5M',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'In this paper, we study a class of bilevel programming problem where the inner objective function is strongly convex. More specifically, under some mile assumptions on the partial derivatives of both inner and outer objective functions, we present an approximation algorithm for solving this class of problem and provide its finite-time convergence analysis under different convexity assumption on the outer objective function. We also present an accelerated variant of this method which improves the rate of convergence under convexity assumption. Furthermore, we generalize our results under stochastic setting where only noisy information of both objective functions is available. To the best of our knowledge, this is the first time that such (stochastic) approximation algorithms with established iteration complexity (sample complexity) are provided for bilevel programming.',\n",
       "  'Ghadimi2018'),\n",
       " (1012,\n",
       "  'K3B6252W',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Linear Programming and Network Flows, now in its third edition, addresses the problem of minimizing or maximizing a linear function in the presence of linear equality or inequility constraints. This book: Provides methods for modeling complex problems via effective algorithms on modern computers. Presents the general theory and characteristics of optimization problems, along with effective solution algorithms. Explores linear programming (LP) and network flows, employing polynomial-time algorithms and various specializations of the simplex method.',\n",
       "  'Bazaraa2009'),\n",
       " (1022,\n",
       "  'FZ2J6UW9',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'This paper is devoted to bilevel optimization, a branch of mathematical programming of both practical and theoretical interest. Starting with a simple example, we proceed towards a general formulation. We then present fields of application, focus on solution approaches, and make the connection with MPECs (Mathematical Programs with Equilibrium Constraints). © Springer Science+Business Media, LLC 2007.',\n",
       "  'Colson2007'),\n",
       " (1024,\n",
       "  '6RXCS6CX',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Asymptotic behaviour of a two time scale stochastic approximation algorithm is analysed in terms of a related singular ordinary differential equation.',\n",
       "  'Borkar1997'),\n",
       " (1026,\n",
       "  'DSBFU4IC',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'A search engine maintains local copies of different web pages to provide quick search results. This local cache is kept up-to-date by a web crawler that frequently visits these different pages to track changes in them. Ideally, the local copy should be updated as soon as a page changes on the web. However, finite bandwidth availability and server restrictions limit how frequently different pages can be crawled. This brings forth the following optimization problem: maximize the freshness of the local cache subject to the crawling frequencies being within prescribed bounds. While tractable algorithms do exist to solve this problem, these either assume the knowledge of exact page change rates or use inefficient methods such as MLE for estimating the same. We address this issue here. We provide three novel schemes for online estimation of page change rates, all of which have extremely low running times per iteration. The first is based on the law of large numbers and the second on stochastic approximation. The third is an extension of the second and includes a heavy-ball momentum term. All these schemes only need partial information about the page change process, i.e., they only need to know if the page has changed or not since the last crawled instance. Our main theoretical results concern asymptotic convergence and convergence rates of these three schemes. In fact, our work is the first to show convergence of the original stochastic heavy-ball method when neither the gradient nor the noise variance is uniformly bounded. We also provide some numerical experiments (based on real and synthetic data) to demonstrate the superiority of our proposed estimators over existing ones such as MLE. Our algorithms are readily applicable to the synchronization of databases and network inventory management.',\n",
       "  'Avrachenkov2022'),\n",
       " (1029,\n",
       "  'HG63K2JX',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Stochastic bilevel optimization generalizes the classic stochastic optimization from the minimization of a single objective to the minimization of an objective function that depends on the solution of another optimization problem. Recently, bilevel optimization is regaining popularity in emerging machine learning applications such as hyper-parameter optimization and model-agnostic meta learning. To solve this class of optimization problems, existing methods require either double-loop or two-timescale updates, which are sometimes less efficient. This paper develops a new optimization method for a class of stochas-tic bilevel problems that we term Single-Timescale stochAstic BiLevEl optimization (STABLE) method. STABLE runs in a single loop fashion, and uses a single-timescale update with a fixed batch size. To achieve an-stationary point of the bilevel problem, STABLE requires O(−2) samples in total; and to achieve an-optimal solution in the strongly convex case, STABLE requires O(−1) samples. To the best of our knowledge, when STABLE was proposed, it is the first bilevel optimization algorithm achieving the same order of sample complexity as SGD for single-level stochastic optimization.',\n",
       "  'Chen2022'),\n",
       " (1031,\n",
       "  'NF9328RX',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'In this paper, we introduce a new stochastic approximation type algorithm, namely, the randomized stochastic gradient (RSG) method, for solving an important class of nonlinear (possibly nonconvex) stochastic programming problems. We establish the complexity of this method for computing an approximate stationary point of a nonlinear programming problem. We also show that this method possesses a nearly optimal rate of convergence if the problem is convex. We discuss a variant of the algorithm which consists of applying a postoptimization phase to evaluate a short list of solutions generated by several independent runs of the RSG method, and we show that such modification allows us to improve significantly the large-deviation properties of the algorithm. These methods are then specialized for solving a class of simulation-based optimization problems in which only stochastic zeroth-order information is available. © 2013 Society for Industrial and Applied Mathematics.',\n",
       "  'Ghadimi2013'),\n",
       " (1556,\n",
       "  'SX9HFZV6',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Given a set V of n elements we wish to linearly order them given pairwise preference labels which may be non-transitive (due to irrationality or arbitrary noise). The goal is to linearly order the elements while disagreeing with as few pairwise preference labels as possible. Our performance is measured by two parameters: The number of disagreements (loss) and the query complexity (number of pairwise preference labels). Our algorithm adaptively queries at most O(ε -6nlog 5 n) preference labels for a regret of ε times the optimal loss. As a function of n, this is asymptotically better than standard (non-adaptive) learning bounds achievable for the same problem. Our main result takes us a step closer toward settling an open problem posed by learning-torank (from pairwise information) theoreticians and practitioners: What is a provably correct way to sample preference labels? To further show the power and practicality of our solution, we analyze a typical test case in which a large margin linear relaxation is used for efficiently solving the simpler learning problems in our decomposition. © 2012 Nir Ailon.',\n",
       "  'Ailon2012'),\n",
       " (1574,\n",
       "  'AZSERNWI',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'We address optimization problems in which we are given contradictory pieces of input information and the goal is to find a globally consistent solution that minimizes the extent of disagreement with the respective inputs. Specifically, the problems we address are rank aggregation, the feedback arc set problem on tournaments, and correlation and consensus clustering. We show that for all these problems (and various weighted versions of them), we can obtain improved approximation factors using essentially the same remarkably simple algorithm. Additionally, we almost settle a long-standing conjecture of Bang-Jensen and Thomassen and show that unless NPBPP, there is no polynomial time algorithm for the problem of minimum feedback arc set in tournaments.',\n",
       "  'Ailon2008'),\n",
       " (1610,\n",
       "  '5VAKT6FZ',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'We examine what is an efficient and scalable nonlinear solver, with low work and memory complexity, for many classes of discretized partial differential equations (PDEs) - matrix-free Full multigrid (FMG) with a Full Approximation Storage (FAS) - in the context of current trends in computer architectures. Brandt proposed an extremely low memory FMG-FAS algorithm over 25 years ago that has several attractive properties for reducing costs on modern - memory centric -- machines and has not been developed to our knowledge. This method, segmental refinement (SR), has very low memory requirements because the finest grids need not be held in memory at any one time but can be \"swept\" through, computing coarse grid correction and any quantities of interest, allowing for orders of magnitude reduction in memory usage. This algorithm has two useful ideas for effectively exploiting future architectures: improved data locality and reuse via \"vertical\" processing of the multigrid algorithms and the method of $\\\\tau$-corrections, which allows for not storing the entire fine grids at any one time. This report develops this algorithm for a model problem and a parallel generalization of the original sweeping technique. We show that FMG-FAS-SR can work as originally predicted, solving systems accurately enough to maintain the convergence rate of the discretization with one FMG iteration, and that the parallel algorithm provides a natural approach to fully exploiting the available parallelism of FMG.',\n",
       "  'Adams2012'),\n",
       " (1648,\n",
       "  '2JZ9EPED',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  \"Stating that they have no argument with the conservative approach in the poor risk patient, the authors report critically on their results in 62 patients treated by radical Wertheim' hysterectomy. In addition, 50 of these had bilateral pelvic lymphadenectomy done. Their patients are quite similar to those in other reports as to age and preliminary diagnosis. All patients had a positive diagnosis made by preliminary biopsy or at the time of operation. Yet their routine consisted of a curettage preliminary to the typical extended radical hysterectomy. Of 10 patients with previous irradiation, 6 showed residual disease in the excised specimen. Thirty-seven of 50 subjects of the extended radical had myometrial invasion and 4 had positive nodes. Nodes were negative in all without myometrial invasion. Preoperative and postoperative care was good, and a Miller-Abbot tube was used preoperatively to insure bowel decompression. Five cm. of vaginal cuff is removed with the 'enbloc' specimen. The cuff is teft open with a Penrose drain. Ten patients died (6 in the first year after operation of whom 2 in the first month) and on 2 of these (death 26 days and 8 months after operation) autopsies were done. The first had residual disease and the latter had recurrent disease. There were 3 known recurrences, 2 died, 1 remained alive and well following colpectomy. Morbidity was similar to other reports with 8.1% urinary fistulae, 11.3% haematomata or thrombophlebitides. There were 4.8% late urinary complications (not including mild bladder atony) and one late small bowel obstruction. Thirty-three patients were suitable for 5-year study, 26 survived without recurrence, and 2 are alive at 10 yr. Salvage was 92% when the lesion was confined to the endometrium but only 71% when the myometrium was involved. Of the patients with positive nodes, 2 are alive at 2 yr. and 1 died of disease at 3 yr. Excerpts from the authors' conclusions bear repeating: 'The end results in this study compare favourably with those reported in the current literature and justify further evaluation of the radical approach.,.,. The high incidence of postoperative complications, however, impose heavy obligations on those who treat and care for these patients.,.,'. This word of warning comes from one of this country's most skilled groups: one with excellent team work and with all modern facilities for the care of any patient.\\\\n\",\n",
       "  'Parlett1974'),\n",
       " (427,\n",
       "  '34J9TKNK',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'An algorithmic framework to compute sparse or minimal-TV solutions of linear systems is proposed. The framework includes both the Kaczmarz method and the linearized Bregman method as special cases and also several new methods such as a sparse Kaczmarz solver. The algorithmic framework has a variety of applications and is especially useful for problems in which the linear measurements are slow and expensive to obtain. We present examples for online compressed sensing, TV tomographic reconstruction and radio interferometry.',\n",
       "  'Lorenz2014a'),\n",
       " (993,\n",
       "  'BGFRXZZU',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'The essential difficulty of gradient-based bilevel optimization using implicit differentiation is to estimate the inverse Hessian vector product with respect to neural network parameters. This paper proposes to tackle this problem by the Nystrom method and the Woodbury matrix identity, exploiting the low-rankness of the Hessian. Compared to existing methods using iterative approximation, such as conjugate gradient and the Neumann series approximation, the proposed method avoids numerical instability and can be efficiently computed in matrix operations without iterations. As a result, the proposed method works stably in various tasks and is faster than iterative approximations. Throughout experiments including large-scale hyperparameter optimization and meta learning, we demonstrate that the Nystrom method consistently achieves comparable or even superior performance to other approaches. The source code is available from https://github.com/moskomule/hypergrad.',\n",
       "  'Hataya2023'),\n",
       " (489,\n",
       "  'TRNCG7SV',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Signal transduction at cell-cell junctions is critical for many biological processes, such as the development of multicellular organisms and the recognition of damaged or infected cells. Such interfaces can be reconstituted in vitro by synthetically coupling cell surface ligands to supported membranes, which can be interfaced directly with live cells. The lateral fluidity of these membranes allows ligand receptor complexes to assemble into oligomers and higher order clusters. This type of higher order clustering has been shown to play a role in the regulation and function of various cell membrane receptors. In order to study complex, multicomponent signaling assemblies, I have extended the use of supported membranes and DNA-based protein assembly to form heterodimers of signaling molecules. Characterization of these structures was performed by fluorescence cross-correlation spectroscopy, which confirmed their lateral mobility and the formation of specific heterodimers. I have additionally demonstrated the interaction of these structures with live cells and the modulation of signaling cluster content in these cells. DNA based assembly was also used for the precise positioning of fluorophores at a fixed distance from a gold nanoparticle encased in a viral capsid. These fluorophores were protected from contact quenching, and their fluorescence was enhanced by their proximity to the gold nanoparticle. Together these studies demonstrate the use of DNA hybridization in directing the formation of functional nanoscale assemblies.',\n",
       "  'Wilson2018'),\n",
       " (1892,\n",
       "  'G9GAP343',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'A core capability of intelligent systems is the ability to quickly learn new tasks by drawing on prior experience. Gradient (or optimization) based meta-learning has recently emerged as an effective approach for few-shot learning. In this formulation, meta-parameters are learned in the outer loop, while task-specific models are learned in the inner-loop, by using only a small amount of data from the current task. A key challenge in scaling these approaches is the need to differentiate through the inner loop learning process, which can impose considerable computational and memory burdens. By drawing upon implicit differentiation, we develop the implicit MAML algorithm, which depends only on the solution to the inner level optimization and not the path taken by the inner loop optimizer. This effectively decouples the meta-gradient computation from the choice of inner loop optimizer. As a result, our approach is agnostic to the choice of inner loop optimizer and can gracefully handle many gradient steps without vanishing gradients or memory constraints. Theoretically, we prove that implicit MAML can compute accurate meta-gradients with a memory footprint that is, up to small constant factors, no more than that which is required to compute a single inner loop gradient and at no overall increase in the total computational cost. Experimentally, we show that these benefits of implicit MAML translate into empirical gains on few-shot image recognition benchmarks.',\n",
       "  'Rajeswaran2019'),\n",
       " (1904,\n",
       "  '4TCHH3DV',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Meta-learning allows an intelligent agent to leverage prior learning episodes as a basis for quickly improving performance on a novel task. Bayesian hierarchical modeling provides a theoretical framework for formalizing meta-learning as inference for a set of parameters that are shared across tasks. Here, we reformulate the model-agnostic meta-learning algorithm (MAML) of Finn et al. (2017) as a method for probabilistic inference in a hierarchical Bayesian model. In contrast to prior methods for meta-learning via hierarchical Bayes, MAML is naturally applicable to complex function approximators through its use of a scalable gradient descent procedure for posterior inference. Furthermore, the identiﬁcation of MAML as hierarchical Bayes provides a way to understand the algorithm’s operation as a meta-learning procedure, as well as an opportunity to make use of computational strategies for efﬁcient inference. We use this opportunity to propose an improvement to the MAML algorithm that makes use of techniques from approximate inference and curvature estimation.',\n",
       "  'Grant2018'),\n",
       " (1930,\n",
       "  'NDAXQPTT',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'A new Monte Carlo estimator for solving the matrix equation x = Hx+b is presented, and theoretical results comparing this estimator with the traditional terminal and collision estimators are given. We then make a detailed investigation of the average complexity of the Monte Carlo estimators when several popular random variable generation techniques are used, and we show that the average complexity can be as low as C z 1n + N , where n is the number of random walks generated, N is the size of the matrix H, z is the solution of an associated matrix equation, and C is a small constant. As a consequence of the complexity results, we observe how scaling of matrices, a wellknown technique in deterministic methods, can increase the eﬃciency of the Monte Carlo method. One advantage of the Monte Carlo method is the eﬃciency at which it can be parallelized. The algorithms we discuss can provide fast and approximate solutions to systems of linear equations in massively parallel computing environments. Surprisingly, sequential (or adaptive) Monte Carlo methods can even be competitive in single-processor computing environments. We present numerical results and compare our Monte Carlo algorithms with Krylov subspace methods for some test matrices.',\n",
       "  'okten2005'),\n",
       " (1931,\n",
       "  'WKEIB3P2',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'We study a novel variation on the Ulam–von Neumann Monte Carlo method for solving a linear system. This is an old randomized procedure that results from using a random walk to stochastically evaluate terms in the Neumann series. In order to apply this procedure, the variance of the stochastic estimator needs to be bounded. The best known suﬃcient condition for bounding the variance is that the inﬁnity norm of the matrix in the Neumann series is smaller than one, which greatly limits the usability of this method. We improve this condition by proposing a new stochastic estimator based on a diﬀerent type of random walk. Our multiway walk and estimator is based on a time-inhomogeneous Markov process that iterates through a sequence of transition matrices built from the original linear system. For our new method, we prove that a necessary and suﬃcient condition for convergence is that the spectral radius of the elementwise absolute value of the matrix underlying the Neumann series is smaller than one. This is a strictly weaker condition than currently exists. In addition, our new method is often faster than the standard algorithm. Through experiments, we demonstrate the potential for our method to reduce the time needed to solve linear equations by incorporating it into an outer iterative method.',\n",
       "  'Wu2019'),\n",
       " (1932,\n",
       "  'SI7RCS5P',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'We describe a new Monte Carlo method based on a multilevel method for computing the action of the resolvent matrix over a vector. The method is based on the numerical evaluation of the Laplace transform of the matrix exponential, which is computed efﬁciently using a multilevel Monte Carlo method. Essentially, it requires generating suitable random paths which evolve through the indices of the matrix according to the probability law of a continuous-time Markov chain governed by the associated Laplacian matrix. The convergence of the proposed multilevel method has been discussed, and several numerical examples were run to test the performance of the algorithm. These examples concern the computation of some metrics of interest in the analysis of complex networks, and the numerical solution of a boundaryvalue problem for an elliptic partial differential equation. In addition, the algorithm was conveniently parallelized, and the scalability analyzed and compared with the results of other existing Monte Carlo method for solving linear algebra systems.',\n",
       "  'Acebron2020'),\n",
       " (290,\n",
       "  'UN4UEVVP',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  '\"This is the second edition of the now definitive text on partial differential equations (PDE). It offers a comprehensive survey of modern techniques in the theoretical study of PDE with particular emphasis on nonlinear equations. Its wide scope and clear exposition make it a great text for a graduate course in PDE. For this edition, the author has made numerous changes, including: a new chapter on nonlinear wave equations, more than 80 new exercises, several new sections, and a significantly expanded bibliography.\"--Publisher\\'s description',\n",
       "  'Evans2010'),\n",
       " (442,\n",
       "  'NW34PSF3',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'The split feasibility problem arises in many fields in the real world, such as signal processing, image reconstruction, and medical care. In this paper, we present a solution algorithm called memory gradient projection method for solving the split feasibility problem, which employs a parameter and two previous iterations to get the next iteration, and its step size can be calculated directly. It not only improves the flexibility of the algorithm, but also avoids computing the largest eigenvalue of the related matrix or estimating the Lipschitz constant in each iteration. Theoretical convergence results are established under some suitable conditions.',\n",
       "  'Qu2018'),\n",
       " (281,\n",
       "  'ZWI5BE2X',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Real Analysis, Fourth Edition, covers the basic material that every reader should know in the classical theory of functions of a real variable, measure and integration theory, and some of the more important and elementary topics in general topology and normed linear space theory. This text assumes a general background in mathematics and familiarity with the fundamental concepts of analysis. Classical theory of functions, including the classical Banach spaces; General topology and the theory of general Banach spaces; Abstract treatment of measure and integration. For all readers interested in real analysis',\n",
       "  'Royden2010'),\n",
       " (279, 'FQAEDEPB', 2, 'abstractNote', 'Latex Reference', 'Kumar2020'),\n",
       " (39,\n",
       "  'C24R4FL4',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  \"We consider a Bayesian approach to to nonlinear inverse problems in which the unknown quantity is a random field (spatial or temporal). The Bayesian approach contains a natural mechanism for regularization in the form of prior information, can incorporate information from from heterogeneous sources and provide a quantitative assessment of uncertainty in the inverse solution. The Bayesian setting casts the inverse solution as a posterior probability distribution over the model parameters. Karhunen-Lo\\\\'eve expansion is used for dimension reduction of the random field. Furthermore, we use a hierarchical Bayes model to inject multiscale data in the modeling framework. In this Bayesian framework, we have shown that this inverse problem is well-posed by proving that the posterior measure is Lipschitz continuous with respect to the data in total variation norm. Computation challenges in this construction arise from the need for repeated evaluations of the forward model (e.g. in the context of MCMC) and are compounded by high dimensionality of the posterior. We develop two-stage reversible jump MCMC which has the ability to screen the bad proposals in the first inexpensive stage. Numerical results are presented by analyzing simulated as well as real data.\",\n",
       "  'Mallick2011'),\n",
       " (1961,\n",
       "  'RGL5BVFW',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'These notes aim to give a gentle introduction to some important topics in continuous optimization. The focus is on methods that arise in machine learning and modern data analysis, highlighting concerns about complexity, robustness, and implementation in these domains.',\n",
       "  'Hardt2018'),\n",
       " (1968,\n",
       "  'RNYA5SAG',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Targeted to advanced undergraduate and graduate students, this textbook develops the concepts of convex analysis and optimization.',\n",
       "  'Borkar2023'),\n",
       " (1985,\n",
       "  'MVNAP3IH',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'A new proof of the Liapunov convexity theorem is presented.',\n",
       "  'Tardella1990'),\n",
       " (1988,\n",
       "  'RPTLPDGS',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'The Shapley-Folkman theorem shows that Minkowski averages of uniformly bounded sets tend to be convex when the number of terms in the sum becomes much larger than the ambient dimension. In optimization, Aubin and Ekeland [1976] show that this produces an a priori bound on the duality gap of separable nonconvex optimization problems involving ﬁnite sums. This bound is highly conservative and depends on unstable quantities, and we relax it in several directions to show that non convexity can have a much milder impact on ﬁnite sum minimization problems such as empirical risk minimization and multi-task classiﬁcation. As a byproduct, we show a new version of Maurey’s classical approximate Carathe´odory lemma where we sample a signiﬁcant fraction of the coefﬁcients, without replacement.',\n",
       "  'Kerdreux2018'),\n",
       " (1999,\n",
       "  'XCXC32JT',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Advancing research. Creating connections.',\n",
       "  'Khan1974'),\n",
       " (2001,\n",
       "  'J6T5QPNQ',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'Nonconvex duality properties for multiobjective optimization problems are obtained by using a characterization of Pareto optima by means of generalized Tchebycheff norms.\\n            Bounds for the corresponding duality gap are given, and approximate Pareto multipliers are constructed. A generalized notion of Pareto multipliers for quasi-convex multiobjective problems is introduced.',\n",
       "  'DiGuglielmo1977'),\n",
       " (2002,\n",
       "  '68KXJH92',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'The Frank-Wolfe algorithms, a.k.a. conditional gradient algorithms, solve constrained optimization problems. They break down a non-linear problem into a series of linear minimization on the constraint set. This contributes to their recent revival in many applied domains, in particular those involving large-scale optimization problems. In this dissertation, we design or analyze versions of the Frank-Wolfe algorithms. We notably show that, contrary to other types of algorithms, this family is adaptive to a broad spectrum of structural assumptions, without the need to know and specify the parameters controlling these hypotheses.',\n",
       "  'Kerdreux2020'),\n",
       " (2023, '2RCYD5AE', 2, 'abstractNote', 'All collected notes', 'Kumar2023'),\n",
       " (2049,\n",
       "  '733KMJE5',\n",
       "  2,\n",
       "  'abstractNote',\n",
       "  'A continuation of EE263. Optimal control and dynamic programming; linear quadratic regulator. Lyapunov theory and methods. Time-varying and periodic systems. Realization theory. Linear estimation and the Kalman filter. Examples and applications from digital filters, circuits, signal processing, and control systems.\\n\\n\\nIf you’d like to consult some books, we listed some below.\\n\\n- LQR and Kalman filtering are covered in many books on linear systems, optimal control, and optimization. One good one is Dynamic Programming and Optimal Control, vol. 1, Bertsekas, Athena Scientific. Another two are Optimal Filtering and Optimal Control: Linear Quadratic Methods, both Anderson & Moore, Dover.\\n\\n- Lyapunov theory is covered in many texts on linear systems, e.g., Linear Systems, Antsaklis & Michel, McGraw-Hill.\\n\\n- Nonlinear Lyapunov theory is covered in most texts on nonlinear system analysis, e.g., Nonlinear systems: Analysis, Stability, and Control, Sastry, Springer, or Nonlinear Systems Analysis (2nd edition), Vidyasagar, SIAM.\\n\\n- Lots of material on LMIs can be found in Boyd, El Ghaoui, Feron, and Balakrishnan, Linear Matrix Inequalities in System and Control Theory, but this is not a book for casual browsing.',\n",
       "  'Boyd2009'),\n",
       " ...]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur.execute(query).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b3189bc9",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1),\n",
       " (1, 2),\n",
       " (2, 3),\n",
       " (6, 4),\n",
       " (13, 5),\n",
       " (14, 6),\n",
       " (19, 8),\n",
       " (32, 9),\n",
       " (38, 10),\n",
       " (59, 11),\n",
       " (79, 12),\n",
       " (1, 13),\n",
       " (1, 14),\n",
       " (2, 15),\n",
       " (6, 16),\n",
       " (14, 6),\n",
       " (19, 18),\n",
       " (32, 19),\n",
       " (38, 20),\n",
       " (59, 21),\n",
       " (79, 22),\n",
       " (1, 13),\n",
       " (1, 23),\n",
       " (2, 24),\n",
       " (6, 25),\n",
       " (14, 6),\n",
       " (19, 27),\n",
       " (32, 28),\n",
       " (38, 20),\n",
       " (59, 29),\n",
       " (79, 22),\n",
       " (1, 13),\n",
       " (1, 30),\n",
       " (2, 31),\n",
       " (6, 32),\n",
       " (14, 6),\n",
       " (19, 34),\n",
       " (32, 35),\n",
       " (38, 36),\n",
       " (59, 37),\n",
       " (79, 38),\n",
       " (1, 13),\n",
       " (1, 39),\n",
       " (2, 40),\n",
       " (6, 41),\n",
       " (14, 6),\n",
       " (19, 43),\n",
       " (32, 44),\n",
       " (38, 20),\n",
       " (59, 45),\n",
       " (79, 22),\n",
       " (1, 13),\n",
       " (1, 46),\n",
       " (2, 47),\n",
       " (6, 48),\n",
       " (19, 50),\n",
       " (32, 51),\n",
       " (38, 52),\n",
       " (59, 53),\n",
       " (76, 54),\n",
       " (79, 55),\n",
       " (1, 13),\n",
       " (1, 13),\n",
       " (1, 58),\n",
       " (2, 59),\n",
       " (6, 60),\n",
       " (13, 61),\n",
       " (1, 13),\n",
       " (1, 63),\n",
       " (2, 64),\n",
       " (6, 65),\n",
       " (13, 66),\n",
       " (19, 68),\n",
       " (32, 69),\n",
       " (38, 70),\n",
       " (59, 71),\n",
       " (76, 54),\n",
       " (79, 72),\n",
       " (1, 13),\n",
       " (1, 73),\n",
       " (2, 74),\n",
       " (6, 75),\n",
       " (13, 76),\n",
       " (19, 78),\n",
       " (32, 79),\n",
       " (38, 80),\n",
       " (76, 81),\n",
       " (79, 82),\n",
       " (1, 13),\n",
       " (1, 83),\n",
       " (2, 84),\n",
       " (6, 85),\n",
       " (19, 87),\n",
       " (32, 88),\n",
       " (38, 89),\n",
       " (59, 90),\n",
       " (76, 91),\n",
       " (79, 92),\n",
       " (1, 13),\n",
       " (1, 93),\n",
       " (2, 94),\n",
       " (6, 95),\n",
       " (19, 97),\n",
       " (25, 98),\n",
       " (44, 99),\n",
       " (1, 13),\n",
       " (1, 100),\n",
       " (2, 101),\n",
       " (6, 102),\n",
       " (19, 104),\n",
       " (32, 105),\n",
       " (38, 106),\n",
       " (59, 107),\n",
       " (76, 81),\n",
       " (79, 108),\n",
       " (1, 13),\n",
       " (1, 109),\n",
       " (6, 110),\n",
       " (25, 112),\n",
       " (1, 13),\n",
       " (1, 113),\n",
       " (2, 114),\n",
       " (6, 115),\n",
       " (14, 116),\n",
       " (19, 118),\n",
       " (38, 119),\n",
       " (59, 120),\n",
       " (76, 121),\n",
       " (79, 122),\n",
       " (1, 123),\n",
       " (2, 124),\n",
       " (6, 125),\n",
       " (13, 126),\n",
       " (14, 116),\n",
       " (19, 128),\n",
       " (32, 129),\n",
       " (38, 130),\n",
       " (59, 131),\n",
       " (76, 132),\n",
       " (79, 133),\n",
       " (1, 134),\n",
       " (2, 135),\n",
       " (6, 136),\n",
       " (14, 116),\n",
       " (19, 138),\n",
       " (32, 139),\n",
       " (38, 140),\n",
       " (59, 141),\n",
       " (76, 142),\n",
       " (79, 143),\n",
       " (1, 144),\n",
       " (2, 145),\n",
       " (6, 136),\n",
       " (19, 147),\n",
       " (32, 148),\n",
       " (38, 149),\n",
       " (59, 150),\n",
       " (76, 151),\n",
       " (79, 152),\n",
       " (1, 13),\n",
       " (1, 153),\n",
       " (6, 154),\n",
       " (19, 156),\n",
       " (38, 157),\n",
       " (1, 13),\n",
       " (1, 158),\n",
       " (6, 159),\n",
       " (19, 161),\n",
       " (32, 162),\n",
       " (38, 163),\n",
       " (59, 164),\n",
       " (76, 132),\n",
       " (79, 165),\n",
       " (1, 13),\n",
       " (1, 13),\n",
       " (1, 168),\n",
       " (2, 169),\n",
       " (6, 110),\n",
       " (19, 171),\n",
       " (32, 172),\n",
       " (38, 173),\n",
       " (59, 174),\n",
       " (76, 54),\n",
       " (79, 175),\n",
       " (1, 13),\n",
       " (1, 176),\n",
       " (6, 177),\n",
       " (19, 179),\n",
       " (32, 180),\n",
       " (38, 181),\n",
       " (59, 182),\n",
       " (76, 132),\n",
       " (79, 183),\n",
       " (1, 13),\n",
       " (1, 184),\n",
       " (2, 185),\n",
       " (6, 186),\n",
       " (19, 188),\n",
       " (32, 189),\n",
       " (38, 190),\n",
       " (59, 191),\n",
       " (76, 132),\n",
       " (79, 192),\n",
       " (1, 13),\n",
       " (1, 193),\n",
       " (2, 194),\n",
       " (6, 195),\n",
       " (19, 197),\n",
       " (38, 198),\n",
       " (59, 199),\n",
       " (76, 200),\n",
       " (79, 201),\n",
       " (1, 13),\n",
       " (1, 202),\n",
       " (6, 203),\n",
       " (21, 205),\n",
       " (23, 206),\n",
       " (25, 207),\n",
       " (32, 208),\n",
       " (1, 13),\n",
       " (1, 209),\n",
       " (2, 210),\n",
       " (6, 211),\n",
       " (19, 27),\n",
       " (32, 213),\n",
       " (38, 214),\n",
       " (59, 215),\n",
       " (76, 142),\n",
       " (79, 216),\n",
       " (1, 13),\n",
       " (1, 217),\n",
       " (2, 218),\n",
       " (6, 65),\n",
       " (19, 220),\n",
       " (32, 221),\n",
       " (57, 222),\n",
       " (59, 223),\n",
       " (1, 13),\n",
       " (1, 224),\n",
       " (2, 225),\n",
       " (6, 95),\n",
       " (19, 147),\n",
       " (32, 227),\n",
       " (38, 228),\n",
       " (59, 229),\n",
       " (76, 230),\n",
       " (79, 231),\n",
       " (1, 13),\n",
       " (1, 232),\n",
       " (2, 233),\n",
       " (6, 16),\n",
       " (1, 13),\n",
       " (1, 235),\n",
       " (6, 115),\n",
       " (13, 236),\n",
       " (19, 238),\n",
       " (32, 239),\n",
       " (38, 240),\n",
       " (59, 241),\n",
       " (76, 54),\n",
       " (79, 242),\n",
       " (1, 13),\n",
       " (1, 243),\n",
       " (6, 60),\n",
       " (1, 13),\n",
       " (1, 245),\n",
       " (6, 95),\n",
       " (19, 91),\n",
       " (25, 247),\n",
       " (44, 248),\n",
       " (1, 13),\n",
       " (1, 249),\n",
       " (2, 250),\n",
       " (6, 16),\n",
       " (19, 252),\n",
       " (25, 253),\n",
       " (32, 254),\n",
       " (57, 255),\n",
       " (59, 256),\n",
       " (1, 13),\n",
       " (1, 257),\n",
       " (6, 258),\n",
       " (23, 260),\n",
       " (25, 261),\n",
       " (44, 262),\n",
       " (1, 13),\n",
       " (1, 263),\n",
       " (6, 264),\n",
       " (13, 265),\n",
       " (19, 128),\n",
       " (25, 267),\n",
       " (44, 268),\n",
       " (1, 13),\n",
       " (1, 13),\n",
       " (1, 273),\n",
       " (6, 25),\n",
       " (25, 275),\n",
       " (32, 276),\n",
       " (57, 277),\n",
       " (59, 278),\n",
       " (1, 13),\n",
       " (1, 279),\n",
       " (2, 280),\n",
       " (6, 264),\n",
       " (1, 13),\n",
       " (1, 282),\n",
       " (2, 283),\n",
       " (6, 75),\n",
       " (13, 284),\n",
       " (14, 285),\n",
       " (32, 287),\n",
       " (104, 288),\n",
       " (1, 13),\n",
       " (1, 289),\n",
       " (2, 290),\n",
       " (6, 16),\n",
       " (14, 285),\n",
       " (19, 142),\n",
       " (23, 292),\n",
       " (25, 293),\n",
       " (32, 294),\n",
       " (57, 222),\n",
       " (59, 295),\n",
       " (1, 13),\n",
       " (1, 296),\n",
       " (2, 297),\n",
       " (6, 115),\n",
       " (32, 299),\n",
       " (1, 13),\n",
       " (1, 300),\n",
       " (2, 301),\n",
       " (6, 48),\n",
       " (25, 303),\n",
       " (44, 304),\n",
       " (1, 13),\n",
       " (1, 305),\n",
       " (2, 306),\n",
       " (6, 307),\n",
       " (19, 309),\n",
       " (25, 310),\n",
       " (44, 311),\n",
       " (1, 13),\n",
       " (1, 312),\n",
       " (2, 313),\n",
       " (6, 48),\n",
       " (32, 315),\n",
       " (38, 316),\n",
       " (59, 317),\n",
       " (1, 13),\n",
       " (1, 318),\n",
       " (2, 319),\n",
       " (6, 48),\n",
       " (19, 142),\n",
       " (25, 321),\n",
       " (44, 322),\n",
       " (1, 13),\n",
       " (1, 323),\n",
       " (2, 324),\n",
       " (6, 16),\n",
       " (13, 325),\n",
       " (76, 327),\n",
       " (1, 13),\n",
       " (1, 328),\n",
       " (2, 329),\n",
       " (6, 48),\n",
       " (13, 330),\n",
       " (19, 332),\n",
       " (32, 333),\n",
       " (38, 334),\n",
       " (59, 335),\n",
       " (76, 336),\n",
       " (79, 337),\n",
       " (1, 13),\n",
       " (1, 338),\n",
       " (1, 13),\n",
       " (1, 340),\n",
       " (1, 13),\n",
       " (1, 13),\n",
       " (1, 345),\n",
       " (2, 346),\n",
       " (6, 347),\n",
       " (19, 104),\n",
       " (32, 349),\n",
       " (38, 149),\n",
       " (59, 350),\n",
       " (79, 351),\n",
       " (1, 358),\n",
       " (2, 359),\n",
       " (6, 186),\n",
       " (19, 361),\n",
       " (32, 362),\n",
       " (38, 363),\n",
       " (59, 364),\n",
       " (76, 365),\n",
       " (79, 366),\n",
       " (1, 13),\n",
       " (1, 367),\n",
       " (2, 368),\n",
       " (6, 369),\n",
       " (19, 132),\n",
       " (32, 371),\n",
       " (38, 372),\n",
       " (59, 373),\n",
       " (76, 374),\n",
       " (79, 375),\n",
       " (1, 13),\n",
       " (1, 383),\n",
       " (2, 384),\n",
       " (6, 385),\n",
       " (19, 374),\n",
       " (32, 387),\n",
       " (38, 388),\n",
       " (59, 389),\n",
       " (76, 142),\n",
       " (79, 390),\n",
       " (1, 391),\n",
       " (2, 392),\n",
       " (6, 110),\n",
       " (13, 393),\n",
       " (25, 395),\n",
       " (44, 396),\n",
       " (1, 13),\n",
       " (1, 397),\n",
       " (2, 398),\n",
       " (6, 115),\n",
       " (57, 400),\n",
       " (1, 13),\n",
       " (1, 401),\n",
       " (6, 65),\n",
       " (13, 402),\n",
       " (32, 404),\n",
       " (76, 405),\n",
       " (1, 13),\n",
       " (1, 406),\n",
       " (2, 407),\n",
       " (6, 65),\n",
       " (19, 188),\n",
       " (32, 409),\n",
       " (38, 410),\n",
       " (79, 411),\n",
       " (1, 13),\n",
       " (1, 412),\n",
       " (2, 413),\n",
       " (6, 41),\n",
       " (19, 415),\n",
       " (32, 416),\n",
       " (38, 410),\n",
       " (79, 411),\n",
       " (1, 13),\n",
       " (1, 417),\n",
       " (2, 418),\n",
       " (6, 65),\n",
       " (19, 420),\n",
       " (32, 421),\n",
       " (38, 422),\n",
       " (59, 423),\n",
       " (76, 230),\n",
       " (79, 424),\n",
       " (1, 13),\n",
       " (1, 13),\n",
       " (1, 434),\n",
       " (2, 435),\n",
       " (6, 436),\n",
       " (19, 438),\n",
       " (32, 439),\n",
       " (38, 440),\n",
       " (59, 441),\n",
       " (76, 142),\n",
       " (79, 442),\n",
       " (1, 443),\n",
       " (2, 444),\n",
       " (6, 445),\n",
       " (13, 446),\n",
       " (19, 448),\n",
       " (32, 449),\n",
       " (38, 450),\n",
       " (59, 451),\n",
       " (76, 142),\n",
       " (79, 452),\n",
       " (1, 13),\n",
       " (1, 453),\n",
       " (2, 454),\n",
       " (6, 195),\n",
       " (19, 456),\n",
       " (32, 457),\n",
       " (38, 458),\n",
       " (59, 459),\n",
       " (76, 374),\n",
       " (79, 460),\n",
       " (1, 13),\n",
       " (1, 461),\n",
       " (2, 462),\n",
       " (6, 48),\n",
       " (19, 456),\n",
       " (32, 464),\n",
       " (38, 465),\n",
       " (76, 54),\n",
       " (1, 13),\n",
       " (1, 466),\n",
       " (2, 467),\n",
       " (6, 48),\n",
       " (19, 68),\n",
       " (32, 469),\n",
       " (38, 458),\n",
       " (59, 470),\n",
       " (76, 374),\n",
       " (79, 471),\n",
       " (1, 13),\n",
       " (1, 472),\n",
       " (6, 16),\n",
       " (19, 87),\n",
       " (32, 474),\n",
       " (38, 475),\n",
       " (59, 476),\n",
       " (76, 142),\n",
       " (79, 477),\n",
       " (1, 13),\n",
       " (1, 478),\n",
       " (2, 479),\n",
       " (6, 48),\n",
       " (19, 68),\n",
       " (32, 481),\n",
       " (38, 458),\n",
       " (59, 482),\n",
       " (76, 374),\n",
       " (79, 471),\n",
       " (1, 483),\n",
       " (2, 484),\n",
       " (6, 203),\n",
       " (13, 485),\n",
       " (19, 361),\n",
       " (32, 487),\n",
       " (38, 488),\n",
       " (59, 489),\n",
       " (76, 374),\n",
       " (79, 490),\n",
       " (1, 13),\n",
       " (1, 491),\n",
       " (2, 492),\n",
       " (6, 195),\n",
       " (13, 493),\n",
       " (19, 495),\n",
       " (32, 496),\n",
       " (38, 497),\n",
       " (59, 498),\n",
       " (76, 54),\n",
       " (79, 499),\n",
       " (1, 13),\n",
       " (1, 500),\n",
       " (2, 501),\n",
       " (6, 186),\n",
       " (19, 361),\n",
       " (32, 503),\n",
       " (38, 363),\n",
       " (59, 504),\n",
       " (76, 365),\n",
       " (79, 366),\n",
       " (1, 13),\n",
       " (1, 505),\n",
       " (2, 506),\n",
       " (6, 203),\n",
       " (38, 465),\n",
       " (1, 13),\n",
       " (1, 508),\n",
       " (6, 60),\n",
       " (19, 510),\n",
       " (32, 511),\n",
       " (59, 512),\n",
       " (79, 513),\n",
       " (1, 13),\n",
       " (1, 514),\n",
       " (2, 515),\n",
       " (6, 115),\n",
       " (19, 517),\n",
       " (32, 518),\n",
       " (38, 363),\n",
       " (59, 519),\n",
       " (76, 54),\n",
       " (79, 366),\n",
       " (1, 13),\n",
       " (1, 520),\n",
       " (2, 521),\n",
       " (6, 195),\n",
       " (19, 523),\n",
       " (32, 524),\n",
       " (38, 465),\n",
       " (76, 91),\n",
       " (1, 13),\n",
       " (1, 13),\n",
       " (1, 532),\n",
       " (2, 533),\n",
       " (6, 60),\n",
       " (13, 534),\n",
       " (19, 536),\n",
       " (32, 537),\n",
       " (38, 497),\n",
       " (59, 538),\n",
       " (76, 132),\n",
       " (79, 499),\n",
       " (1, 13),\n",
       " (1, 539),\n",
       " (2, 540),\n",
       " (6, 541),\n",
       " (19, 543),\n",
       " (32, 544),\n",
       " (38, 545),\n",
       " (59, 546),\n",
       " (76, 142),\n",
       " (79, 547),\n",
       " (1, 13),\n",
       " (1, 548),\n",
       " (2, 549),\n",
       " (6, 41),\n",
       " (19, 551),\n",
       " (32, 552),\n",
       " (38, 553),\n",
       " (59, 554),\n",
       " (76, 54),\n",
       " (79, 555),\n",
       " (1, 13),\n",
       " (1, 556),\n",
       " (2, 557),\n",
       " (6, 203),\n",
       " (19, 238),\n",
       " (32, 559),\n",
       " (38, 560),\n",
       " (59, 561),\n",
       " (76, 81),\n",
       " (79, 562),\n",
       " (1, 13),\n",
       " (1, 563),\n",
       " (1, 13),\n",
       " (1, 565),\n",
       " (2, 566),\n",
       " (6, 16),\n",
       " (19, 568),\n",
       " (32, 569),\n",
       " (38, 570),\n",
       " (59, 571),\n",
       " (76, 91),\n",
       " (79, 572),\n",
       " (1, 13),\n",
       " (1, 573),\n",
       " (2, 574),\n",
       " (6, 575),\n",
       " (19, 577),\n",
       " (32, 578),\n",
       " (38, 579),\n",
       " (59, 580),\n",
       " (76, 54),\n",
       " (79, 581),\n",
       " (1, 13),\n",
       " (1, 582),\n",
       " (2, 583),\n",
       " (6, 95),\n",
       " (19, 523),\n",
       " (32, 585),\n",
       " (38, 363),\n",
       " (59, 586),\n",
       " (76, 179),\n",
       " (79, 366),\n",
       " (1, 13),\n",
       " (1, 587),\n",
       " (2, 588),\n",
       " (6, 85),\n",
       " (19, 91),\n",
       " (32, 590),\n",
       " (38, 591),\n",
       " (59, 592),\n",
       " (79, 593),\n",
       " (1, 13),\n",
       " (1, 594),\n",
       " (2, 595),\n",
       " (6, 65),\n",
       " (19, 456),\n",
       " (38, 597),\n",
       " (59, 598),\n",
       " (76, 54),\n",
       " (79, 599),\n",
       " (1, 13),\n",
       " (1, 600),\n",
       " (6, 601),\n",
       " (19, 415),\n",
       " (32, 603),\n",
       " (38, 604),\n",
       " (1, 13),\n",
       " (1, 605),\n",
       " (2, 606),\n",
       " (6, 203),\n",
       " (32, 608),\n",
       " (38, 609),\n",
       " (59, 610),\n",
       " (79, 611),\n",
       " (1, 13),\n",
       " (1, 612),\n",
       " (2, 613),\n",
       " (6, 614),\n",
       " (19, 616),\n",
       " (32, 617),\n",
       " (38, 618),\n",
       " (59, 619),\n",
       " (76, 91),\n",
       " (79, 620),\n",
       " (1, 13),\n",
       " (1, 621),\n",
       " (1, 13),\n",
       " (1, 623),\n",
       " (2, 624),\n",
       " (6, 203),\n",
       " (19, 626),\n",
       " (32, 627),\n",
       " (38, 628),\n",
       " (59, 629),\n",
       " (76, 54),\n",
       " (79, 630),\n",
       " (1, 13),\n",
       " (1, 631),\n",
       " (2, 632),\n",
       " (6, 60),\n",
       " (38, 465),\n",
       " (79, 634),\n",
       " (1, 13),\n",
       " (1, 635),\n",
       " (2, 636),\n",
       " (6, 203),\n",
       " (32, 638),\n",
       " (38, 465),\n",
       " (79, 634),\n",
       " (1, 13),\n",
       " (1, 639),\n",
       " (2, 640),\n",
       " (6, 307),\n",
       " (19, 551),\n",
       " (32, 642),\n",
       " (38, 643),\n",
       " (59, 644),\n",
       " (76, 179),\n",
       " (79, 645),\n",
       " (1, 13),\n",
       " (1, 646),\n",
       " (2, 647),\n",
       " (6, 648),\n",
       " (19, 415),\n",
       " (32, 650),\n",
       " (38, 618),\n",
       " (59, 651),\n",
       " (76, 142),\n",
       " (79, 620),\n",
       " (1, 13),\n",
       " (1, 652),\n",
       " (2, 653),\n",
       " (6, 654),\n",
       " (19, 656),\n",
       " (32, 657),\n",
       " (38, 545),\n",
       " (59, 658),\n",
       " (76, 132),\n",
       " (79, 659),\n",
       " (1, 13),\n",
       " (1, 660),\n",
       " (2, 661),\n",
       " (6, 369),\n",
       " (19, 128),\n",
       " (32, 663),\n",
       " (38, 664),\n",
       " (59, 665),\n",
       " (76, 142),\n",
       " (79, 666),\n",
       " (1, 13),\n",
       " (1, 667),\n",
       " (6, 668),\n",
       " (19, 34),\n",
       " (32, 670),\n",
       " (38, 671),\n",
       " (1, 13),\n",
       " (1, 672),\n",
       " (6, 541),\n",
       " (25, 674),\n",
       " (1, 13),\n",
       " (1, 675),\n",
       " (1, 677),\n",
       " (2, 678),\n",
       " (6, 679),\n",
       " (13, 680),\n",
       " (14, 681),\n",
       " (1, 13),\n",
       " (1, 683),\n",
       " (2, 684),\n",
       " (6, 60),\n",
       " (19, 309),\n",
       " (32, 686),\n",
       " (38, 458),\n",
       " (59, 687),\n",
       " (76, 142),\n",
       " (79, 460),\n",
       " (1, 688),\n",
       " (2, 689),\n",
       " (6, 60),\n",
       " (19, 691),\n",
       " (38, 692),\n",
       " (59, 693),\n",
       " (79, 694),\n",
       " (1, 695),\n",
       " (2, 696),\n",
       " (6, 697),\n",
       " (14, 681),\n",
       " (19, 699),\n",
       " (32, 700),\n",
       " (38, 609),\n",
       " (59, 701),\n",
       " (79, 611),\n",
       " (1, 13),\n",
       " (1, 702),\n",
       " (2, 703),\n",
       " (6, 541),\n",
       " (19, 104),\n",
       " (38, 705),\n",
       " (59, 706),\n",
       " (76, 179),\n",
       " (79, 707),\n",
       " (1, 708),\n",
       " (2, 709),\n",
       " (6, 541),\n",
       " (19, 711),\n",
       " (38, 712),\n",
       " (59, 713),\n",
       " (76, 230),\n",
       " (79, 714),\n",
       " (1, 715),\n",
       " (2, 716),\n",
       " (6, 25),\n",
       " (57, 591),\n",
       " (59, 718),\n",
       " (1, 719),\n",
       " (2, 720),\n",
       " (6, 195),\n",
       " (19, 616),\n",
       " (32, 254),\n",
       " (45, 722),\n",
       " (1, 723),\n",
       " (2, 724),\n",
       " (6, 725),\n",
       " (13, 726),\n",
       " (14, 681),\n",
       " (38, 728),\n",
       " (1, 13),\n",
       " (1, 729),\n",
       " (2, 730),\n",
       " (6, 60),\n",
       " (19, 732),\n",
       " (25, 733),\n",
       " (32, 734),\n",
       " (57, 735),\n",
       " (59, 736),\n",
       " (1, 13),\n",
       " (1, 739),\n",
       " (2, 740),\n",
       " (6, 41),\n",
       " (32, 742),\n",
       " (38, 743),\n",
       " (59, 744),\n",
       " (79, 745),\n",
       " (1, 13),\n",
       " (1, 746),\n",
       " (6, 747),\n",
       " (13, 748),\n",
       " (19, 568),\n",
       " (32, 750),\n",
       " (38, 497),\n",
       " (59, 751),\n",
       " (76, 142),\n",
       " (79, 752),\n",
       " (1, 13),\n",
       " (1, 753),\n",
       " (6, 60),\n",
       " (19, 121),\n",
       " (32, 755),\n",
       " (38, 553),\n",
       " (59, 756),\n",
       " (76, 142),\n",
       " (1, 13),\n",
       " (1, 757),\n",
       " (6, 25),\n",
       " (14, 758),\n",
       " (32, 760),\n",
       " (1, 13),\n",
       " (1, 762),\n",
       " (1, 13),\n",
       " (1, 764),\n",
       " (2, 765),\n",
       " (6, 766),\n",
       " (13, 767),\n",
       " (14, 768),\n",
       " (38, 388),\n",
       " (59, 770),\n",
       " (79, 771),\n",
       " (1, 13),\n",
       " (1, 772),\n",
       " (2, 773),\n",
       " (6, 25),\n",
       " (14, 768),\n",
       " (19, 775),\n",
       " (32, 776),\n",
       " (38, 388),\n",
       " (59, 777),\n",
       " (76, 142),\n",
       " (79, 390),\n",
       " (1, 13),\n",
       " (1, 778),\n",
       " (2, 779),\n",
       " (6, 154),\n",
       " (14, 780),\n",
       " (1, 13),\n",
       " (1, 782),\n",
       " (6, 195),\n",
       " (32, 784),\n",
       " (38, 785),\n",
       " (79, 786),\n",
       " (1, 13),\n",
       " (1, 787),\n",
       " (2, 788),\n",
       " (6, 203),\n",
       " (13, 789),\n",
       " (19, 27),\n",
       " (32, 791),\n",
       " (38, 792),\n",
       " (59, 793),\n",
       " (76, 91),\n",
       " (79, 794),\n",
       " (1, 13),\n",
       " (1, 795),\n",
       " (6, 203),\n",
       " (32, 650),\n",
       " (59, 797),\n",
       " (1, 13),\n",
       " (1, 798),\n",
       " (2, 799),\n",
       " (6, 16),\n",
       " (32, 801),\n",
       " (38, 802),\n",
       " (1, 13),\n",
       " (1, 803),\n",
       " (2, 804),\n",
       " (6, 65),\n",
       " (19, 806),\n",
       " (32, 807),\n",
       " (38, 609),\n",
       " (59, 808),\n",
       " (79, 611),\n",
       " (1, 13),\n",
       " (1, 809),\n",
       " (2, 810),\n",
       " (6, 60),\n",
       " (19, 812),\n",
       " (32, 813),\n",
       " (38, 628),\n",
       " (59, 814),\n",
       " (76, 142),\n",
       " (79, 630),\n",
       " (1, 13),\n",
       " (2, 816),\n",
       " (6, 115),\n",
       " (19, 54),\n",
       " (32, 818),\n",
       " (38, 819),\n",
       " (76, 142),\n",
       " (79, 820),\n",
       " (1, 13),\n",
       " (1, 821),\n",
       " (6, 25),\n",
       " (25, 823),\n",
       " (44, 824),\n",
       " (1, 13),\n",
       " (1, 825),\n",
       " (6, 826),\n",
       " (32, 828),\n",
       " (38, 829),\n",
       " (59, 830),\n",
       " (1, 13),\n",
       " (1, 835),\n",
       " (6, 60),\n",
       " (13, 836),\n",
       " (19, 838),\n",
       " (21, 839),\n",
       " (23, 840),\n",
       " (25, 841),\n",
       " (44, 842),\n",
       " (1, 13),\n",
       " (1, 843),\n",
       " (2, 844),\n",
       " (6, 95),\n",
       " (25, 846),\n",
       " (44, 847),\n",
       " (1, 13),\n",
       " (1, 848),\n",
       " (2, 849),\n",
       " (6, 186),\n",
       " (19, 851),\n",
       " (32, 852),\n",
       " (38, 853),\n",
       " ...]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itemData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92017903",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'title'),\n",
       " (2, 'abstractNote'),\n",
       " (3, 'artworkMedium'),\n",
       " (4, 'medium'),\n",
       " (5, 'artworkSize'),\n",
       " (6, 'date'),\n",
       " (7, 'language'),\n",
       " (8, 'shortTitle'),\n",
       " (9, 'archive'),\n",
       " (10, 'archiveLocation'),\n",
       " (11, 'libraryCatalog'),\n",
       " (12, 'callNumber'),\n",
       " (13, 'url'),\n",
       " (14, 'accessDate'),\n",
       " (15, 'rights'),\n",
       " (16, 'extra'),\n",
       " (17, 'audioRecordingFormat'),\n",
       " (18, 'seriesTitle'),\n",
       " (19, 'volume'),\n",
       " (20, 'numberOfVolumes'),\n",
       " (21, 'place'),\n",
       " (22, 'label'),\n",
       " (23, 'publisher'),\n",
       " (24, 'runningTime'),\n",
       " (25, 'ISBN'),\n",
       " (26, 'billNumber'),\n",
       " (27, 'number'),\n",
       " (28, 'code'),\n",
       " (29, 'codeVolume'),\n",
       " (30, 'section'),\n",
       " (31, 'codePages'),\n",
       " (32, 'pages'),\n",
       " (33, 'legislativeBody'),\n",
       " (34, 'authority'),\n",
       " (35, 'session'),\n",
       " (36, 'history'),\n",
       " (37, 'blogTitle'),\n",
       " (38, 'publicationTitle'),\n",
       " (39, 'websiteType'),\n",
       " (40, 'type'),\n",
       " (41, 'series'),\n",
       " (42, 'seriesNumber'),\n",
       " (43, 'edition'),\n",
       " (44, 'numPages'),\n",
       " (45, 'bookTitle'),\n",
       " (46, 'caseName'),\n",
       " (47, 'court'),\n",
       " (48, 'dateDecided'),\n",
       " (49, 'docketNumber'),\n",
       " (50, 'reporter'),\n",
       " (51, 'reporterVolume'),\n",
       " (52, 'firstPage'),\n",
       " (53, 'versionNumber'),\n",
       " (54, 'system'),\n",
       " (55, 'company'),\n",
       " (56, 'programmingLanguage'),\n",
       " (57, 'proceedingsTitle'),\n",
       " (58, 'conferenceName'),\n",
       " (59, 'DOI'),\n",
       " (60, 'identifier'),\n",
       " (61, 'repository'),\n",
       " (62, 'repositoryLocation'),\n",
       " (63, 'format'),\n",
       " (64, 'citationKey'),\n",
       " (65, 'dictionaryTitle'),\n",
       " (66, 'subject'),\n",
       " (67, 'encyclopediaTitle'),\n",
       " (68, 'distributor'),\n",
       " (69, 'genre'),\n",
       " (70, 'videoRecordingFormat'),\n",
       " (71, 'forumTitle'),\n",
       " (72, 'postType'),\n",
       " (73, 'committee'),\n",
       " (74, 'documentNumber'),\n",
       " (75, 'interviewMedium'),\n",
       " (76, 'issue'),\n",
       " (77, 'seriesText'),\n",
       " (78, 'journalAbbreviation'),\n",
       " (79, 'ISSN'),\n",
       " (80, 'letterType'),\n",
       " (81, 'manuscriptType'),\n",
       " (82, 'mapType'),\n",
       " (83, 'scale'),\n",
       " (84, 'country'),\n",
       " (85, 'assignee'),\n",
       " (86, 'issuingAuthority'),\n",
       " (87, 'patentNumber'),\n",
       " (88, 'filingDate'),\n",
       " (89, 'applicationNumber'),\n",
       " (90, 'priorityNumbers'),\n",
       " (91, 'issueDate'),\n",
       " (92, 'references'),\n",
       " (93, 'legalStatus'),\n",
       " (94, 'status'),\n",
       " (95, 'episodeNumber'),\n",
       " (96, 'audioFileType'),\n",
       " (97, 'archiveID'),\n",
       " (98, 'presentationType'),\n",
       " (99, 'meetingName'),\n",
       " (100, 'programTitle'),\n",
       " (101, 'network'),\n",
       " (102, 'reportNumber'),\n",
       " (103, 'reportType'),\n",
       " (104, 'institution'),\n",
       " (105, 'organization'),\n",
       " (106, 'nameOfAct'),\n",
       " (107, 'codeNumber'),\n",
       " (108, 'publicLawNumber'),\n",
       " (109, 'dateEnacted'),\n",
       " (110, 'thesisType'),\n",
       " (111, 'university'),\n",
       " (112, 'studio'),\n",
       " (113, 'websiteTitle')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fields\n",
    "# interesting fields, rest get it from bibliography database.\n",
    "[(1, 'title'),\n",
    " (2, 'abstractNote'),\n",
    " (3, 'artworkMedium'),\n",
    " (4, 'medium'),\n",
    " (5, 'artworkSize'),\n",
    " (6, 'date'),\n",
    " (7, 'language'),\n",
    " (8, 'shortTitle'),\n",
    " (9, 'archive'),\n",
    " (10, 'archiveLocation'),\n",
    " (11, 'libraryCatalog'),\n",
    " (12, 'callNumber'),\n",
    " (13, 'url'),\n",
    " (14, 'accessDate'),\n",
    " (15, 'rights'),\n",
    " (16, 'extra'),\n",
    " (17, 'audioRecordingFormat'),\n",
    " (18, 'seriesTitle'),\n",
    " (19, 'volume'),\n",
    " (20, 'numberOfVolumes'),\n",
    " (21, 'place'),\n",
    " (22, 'label'),\n",
    " (23, 'publisher'),\n",
    " (24, 'runningTime'),\n",
    " (25, 'ISBN'),\n",
    " (26, 'billNumber'),\n",
    " (27, 'number'),\n",
    " (28, 'code'),\n",
    " (29, 'codeVolume'),\n",
    " (30, 'section'),\n",
    " (31, 'codePages'),\n",
    " (32, 'pages'),\n",
    " (33, 'legislativeBody'),\n",
    " (34, 'authority'),\n",
    " (35, 'session'),\n",
    " (36, 'history'),\n",
    " (37, 'blogTitle'),\n",
    " (38, 'publicationTitle'),\n",
    " (39, 'websiteType'),\n",
    " (40, 'type'),\n",
    " (41, 'series'),\n",
    " (42, 'seriesNumber'),\n",
    " (43, 'edition'),\n",
    " (44, 'numPages'),\n",
    " (45, 'bookTitle'),\n",
    " (46, 'caseName'),\n",
    " (47, 'court'),\n",
    " (48, 'dateDecided'),\n",
    " (49, 'docketNumber'),\n",
    " (50, 'reporter'),\n",
    " (51, 'reporterVolume'),\n",
    " (52, 'firstPage'),\n",
    " (53, 'versionNumber'),\n",
    " (54, 'system'),\n",
    " (55, 'company'),\n",
    " (56, 'programmingLanguage'),\n",
    " (57, 'proceedingsTitle'),\n",
    " (58, 'conferenceName'),\n",
    " (59, 'DOI'),\n",
    " (60, 'identifier'),\n",
    " (61, 'repository'),\n",
    " (62, 'repositoryLocation'),\n",
    " (63, 'format'),\n",
    " (64, 'citationKey'),\n",
    " (65, 'dictionaryTitle'),\n",
    " (66, 'subject'),\n",
    " (67, 'encyclopediaTitle'),\n",
    " (68, 'distributor'),\n",
    " (69, 'genre'),\n",
    " (70, 'videoRecordingFormat'),\n",
    " (71, 'forumTitle'),\n",
    " (72, 'postType'),\n",
    " (73, 'committee'),\n",
    " (74, 'documentNumber'),\n",
    " (75, 'interviewMedium'),\n",
    " (76, 'issue'),\n",
    " (77, 'seriesText'),\n",
    " (78, 'journalAbbreviation'),\n",
    " (79, 'ISSN'),\n",
    " (80, 'letterType'),\n",
    " (81, 'manuscriptType'),\n",
    " (82, 'mapType'),\n",
    " (83, 'scale'),\n",
    " (84, 'country'),\n",
    " (85, 'assignee'),\n",
    " (86, 'issuingAuthority'),\n",
    " (87, 'patentNumber'),\n",
    " (88, 'filingDate'),\n",
    " (89, 'applicationNumber'),\n",
    " (90, 'priorityNumbers'),\n",
    " (91, 'issueDate'),\n",
    " (92, 'references'),\n",
    " (93, 'legalStatus'),\n",
    " (94, 'status'),\n",
    " (95, 'episodeNumber'),\n",
    " (96, 'audioFileType'),\n",
    " (97, 'archiveID'),\n",
    " (98, 'presentationType'),\n",
    " (99, 'meetingName'),\n",
    " (100, 'programTitle'),\n",
    " (101, 'network'),\n",
    " (102, 'reportNumber'),\n",
    " (103, 'reportType'),\n",
    " (104, 'institution'),\n",
    " (105, 'organization'),\n",
    " (106, 'nameOfAct'),\n",
    " (107, 'codeNumber'),\n",
    " (108, 'publicLawNumber'),\n",
    " (109, 'dateEnacted'),\n",
    " (110, 'thesisType'),\n",
    " (111, 'university'),\n",
    " (112, 'studio'),\n",
    " (113, 'websiteTitle')][(1, 'title'),\n",
    " (2, 'abstractNote'),\n",
    " (3, 'artworkMedium'),\n",
    " (4, 'medium'),\n",
    " (5, 'artworkSize'),\n",
    " (6, 'date'),\n",
    " (7, 'language'),\n",
    " (8, 'shortTitle'),\n",
    " (9, 'archive'),\n",
    " (10, 'archiveLocation'),\n",
    " (11, 'libraryCatalog'),\n",
    " (12, 'callNumber'),\n",
    " (13, 'url'),\n",
    " (14, 'accessDate'),\n",
    " (15, 'rights'),\n",
    " (16, 'extra'),\n",
    " (17, 'audioRecordingFormat'),\n",
    " (18, 'seriesTitle'),\n",
    " (19, 'volume'),\n",
    " (20, 'numberOfVolumes'),\n",
    " (21, 'place'),\n",
    " (22, 'label'),\n",
    " (23, 'publisher'),\n",
    " (24, 'runningTime'),\n",
    " (25, 'ISBN'),\n",
    " (26, 'billNumber'),\n",
    " (27, 'number'),\n",
    " (28, 'code'),\n",
    " (29, 'codeVolume'),\n",
    " (30, 'section'),\n",
    " (31, 'codePages'),\n",
    " (32, 'pages'),\n",
    " (33, 'legislativeBody'),\n",
    " (34, 'authority'),\n",
    " (35, 'session'),\n",
    " (36, 'history'),\n",
    " (37, 'blogTitle'),\n",
    " (38, 'publicationTitle'),\n",
    " (39, 'websiteType'),\n",
    " (40, 'type'),\n",
    " (41, 'series'),\n",
    " (42, 'seriesNumber'),\n",
    " (43, 'edition'),\n",
    " (44, 'numPages'),\n",
    " (45, 'bookTitle'),\n",
    " (46, 'caseName'),\n",
    " (47, 'court'),\n",
    " (48, 'dateDecided'),\n",
    " (49, 'docketNumber'),\n",
    " (50, 'reporter'),\n",
    " (51, 'reporterVolume'),\n",
    " (52, 'firstPage'),\n",
    " (53, 'versionNumber'),\n",
    " (54, 'system'),\n",
    " (55, 'company'),\n",
    " (56, 'programmingLanguage'),\n",
    " (57, 'proceedingsTitle'),\n",
    " (58, 'conferenceName'),\n",
    " (59, 'DOI'),\n",
    " (60, 'identifier'),\n",
    " (61, 'repository'),\n",
    " (62, 'repositoryLocation'),\n",
    " (63, 'format'),\n",
    " (64, 'citationKey'),\n",
    " (65, 'dictionaryTitle'),\n",
    " (66, 'subject'),\n",
    " (67, 'encyclopediaTitle'),\n",
    " (68, 'distributor'),\n",
    " (69, 'genre'),\n",
    " (70, 'videoRecordingFormat'),\n",
    " (71, 'forumTitle'),\n",
    " (72, 'postType'),\n",
    " (73, 'committee'),\n",
    " (74, 'documentNumber'),\n",
    " (75, 'interviewMedium'),\n",
    " (76, 'issue'),\n",
    " (77, 'seriesText'),\n",
    " (78, 'journalAbbreviation'),\n",
    " (79, 'ISSN'),\n",
    " (80, 'letterType'),\n",
    " (81, 'manuscriptType'),\n",
    " (82, 'mapType'),\n",
    " (83, 'scale'),\n",
    " (84, 'country'),\n",
    " (85, 'assignee'),\n",
    " (86, 'issuingAuthority'),\n",
    " (87, 'patentNumber'),\n",
    " (88, 'filingDate'),\n",
    " (89, 'applicationNumber'),\n",
    " (90, 'priorityNumbers'),\n",
    " (91, 'issueDate'),\n",
    " (92, 'references'),\n",
    " (93, 'legalStatus'),\n",
    " (94, 'status'),\n",
    " (95, 'episodeNumber'),\n",
    " (96, 'audioFileType'),\n",
    " (97, 'archiveID'),\n",
    " (98, 'presentationType'),\n",
    " (99, 'meetingName'),\n",
    " (100, 'programTitle'),\n",
    " (101, 'network'),\n",
    " (102, 'reportNumber'),\n",
    " (103, 'reportType'),\n",
    " (104, 'institution'),\n",
    " (105, 'organization'),\n",
    " (106, 'nameOfAct'),\n",
    " (107, 'codeNumber'),\n",
    " (108, 'publicLawNumber'),\n",
    " (109, 'dateEnacted'),\n",
    " (110, 'thesisType'),\n",
    " (111, 'university'),\n",
    " (112, 'studio'),\n",
    " (113, 'websiteTitle')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ee8687",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "382221b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "queryB = \"\"\"\n",
    "SELECT citationkey.itemID, citationkey.itemKey, citationkey.citationKey\n",
    "FROM citationkey \n",
    "LEFT JOIN items on citationkey.itemID = items.itemID\n",
    "\"\"\"\n",
    "join = cur.execute(queryB).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "23c00fc6",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 'KHW2498F', 'Polak2017'),\n",
       " (4, 'QZ4IPAQV', 'Polak2016'),\n",
       " (6, '6DGRHLYU', 'Sangid2013'),\n",
       " (8, 'YGGUDQCL', 'Polak1987'),\n",
       " (10, '4MFHUNBV', 'Polak2014'),\n",
       " (14, 'KN3GNAXU', 'Acharya'),\n",
       " (18, 'WQ2CIXZK', 'Argyropoulos2015'),\n",
       " (20, '84HAVXWN', 'Mitzenmacher1988'),\n",
       " (22, 'NYN9JAFT', 'Cai2006'),\n",
       " (24, 'XGN74PFB', 'PeterWriggers2012'),\n",
       " (28, 'AH3PKCWJ', 'Press2000'),\n",
       " (30, 'LYZ2SXKL', 'Cotter2009'),\n",
       " (31, 'T57E9LWP', 'Kerfriden2019'),\n",
       " (32, '37AE3RYY', 'Cotter2010'),\n",
       " (33, 'Z3P3I79V', 'Stuart2010'),\n",
       " (35, 'P6YBSNEN', 'Page2007'),\n",
       " (37, 'MHWV4VWA', 'Kac1966'),\n",
       " (39, 'C24R4FL4', 'Mallick2011'),\n",
       " (41, '8T89CPZT', 'Challamel2000'),\n",
       " (43, 'X5NTIP8Q', 'Flanagan1959'),\n",
       " (45, 'QT7I59WW', 'Kabanikhin2008'),\n",
       " (47, '8JHH63I2', 'Billinge2019'),\n",
       " (56, 'JP828P7I', 'Vincent1990'),\n",
       " (58, 'XVP6ESUA', 'Vishnoi2015'),\n",
       " (61, 'ZV3MJCRK', 'Dixit2012'),\n",
       " (91, 'KMHD79T9', 'Hopenhayn2016'),\n",
       " (98, 'KVHB88B3', 'Pitchik2009'),\n",
       " (102, 'TM77ICP7', 'Nov2018'),\n",
       " (105, '2UD7Y8NR', 'Xun2012'),\n",
       " (107, '4J8VQFF3', 'Panageas2016a'),\n",
       " (133, 'A36NLYGH', 'Beilina1390'),\n",
       " (139, 'TLRSTBP4', 'Vishnoi2013'),\n",
       " (141, 'V7IPA6PF', 'Schindler2003'),\n",
       " (143, 'RJQX4CAD', 'Eigen1988'),\n",
       " (145, 'W34DX4WH', 'Panageas2016'),\n",
       " (147, 'RRJJLKRQ', 'Diaconis2009'),\n",
       " (149, '8GHW6KVJ', 'Ghanem2017'),\n",
       " (270, 'HJ2GJUKM', 'Stuart2017'),\n",
       " (275, '7CEDI6HD', 'Clement2016'),\n",
       " (277, 'C85XBEGL', 'Pedregal2017'),\n",
       " (279, 'FQAEDEPB', 'Kumar2020'),\n",
       " (281, 'ZWI5BE2X', 'Royden2010'),\n",
       " (290, 'UN4UEVVP', 'Evans2010'),\n",
       " (292, 'WRHMBIR4', 'Benning2018'),\n",
       " (295, '89QEPBBX', 'Bredies2008'),\n",
       " (297, 'WY8SJYFL', 'Hestenes1969'),\n",
       " (301, 'GIUQPQU6', 'Rockafellar1973'),\n",
       " (302, 'CYTFSGT7', 'Fortin2000'),\n",
       " (304, 'JQV64ZJ9', 'Figueiredo2009'),\n",
       " (306, '5SW9HXKK', 'Rulla2015'),\n",
       " (308, 'EEVMXG2E', 'Lin2015'),\n",
       " (310, 'ZSMZTCTJ', 'Kale2014'),\n",
       " (312, '88HI884G', 'Dai2015'),\n",
       " (314, 'UIKWGIQ8', 'Terra2012'),\n",
       " (320, 'ZIJP72GB', 'Aboud2019'),\n",
       " (322, 'WVB3X3QG', 'Aharoni1984'),\n",
       " (325, 'G9459S3D', 'Zhao2020'),\n",
       " (327, '5R635KPF', 'Haddock2019a'),\n",
       " (329, 'YBSBB3MW', 'Richtarik2017'),\n",
       " (331, '9CXPLTPC', 'Hefny2017'),\n",
       " (333, 'Y3XFX8EP', 'Tropp2016'),\n",
       " (335, 'DIQ7LN5L', 'DeLoera2017'),\n",
       " (337, '79HTE7YU', 'Haddou2020'),\n",
       " (339, 'TX9IQDAE', 'Haddock2019'),\n",
       " (341, 'DH56LMMZ', 'Candes2008a'),\n",
       " (343, '8M6PSFR3', 'Sheriff2020'),\n",
       " (345, 'T6ALGK5Z', 'Chen2018'),\n",
       " (347, 'LVTDCDNJ', 'Strohmer2009'),\n",
       " (353, 'IME9ZZQJ', 'Necoara2019'),\n",
       " (356, 'BUHNFE55', 'Cegielski2020'),\n",
       " (359, 'VPRN5GHF', 'Lorenz2018'),\n",
       " (361, 'BEYVQHE4', 'Chen2001'),\n",
       " (363, 'VUTD2L26', 'Lorenz2014'),\n",
       " (365, 'RF983IJ3', 'Bernstein2020'),\n",
       " (367, '5EDIRAG2', 'Fornasier2015'),\n",
       " (369, 'PSU4NWDX', 'Nagahara2016'),\n",
       " (371, 'Z79VBMX2', 'Natarajan1995'),\n",
       " (374, 'C74YT8SQ', 'Chen2012'),\n",
       " (376, '4T9KPRM5', 'Sra2006'),\n",
       " (378, 'SYH8KZNE', 'Lorenz2015'),\n",
       " (380, 'RLMUI567', 'Kaczmarz1937'),\n",
       " (386, 'VGZGUCNJ', 'Borgard2020'),\n",
       " (388, 'D4MI8XVT', 'Tanabe1971'),\n",
       " (405, 'WDLVFYRL', 'Natterer2001a'),\n",
       " (407, 'MMP9VPRQ', 'Lin2020'),\n",
       " (409, 'EYTHARPP', 'Dey2018'),\n",
       " (411, 'XE8H3Y3H', 'Hazimeh2020'),\n",
       " (413, 'TWW88JF8', 'Gubin1967'),\n",
       " (415, 'AL9X9BHE', 'Elfving1980'),\n",
       " (417, 'MT6UGWKU', 'Censor1981'),\n",
       " (419, 'A6NAGGAQ', 'Tewarson1969'),\n",
       " (423, 'PF5E9JAI', 'Censor1983'),\n",
       " (425, 'QMPN8RRB', 'Natterer2001'),\n",
       " (427, '34J9TKNK', 'Lorenz2014a'),\n",
       " (429, 'MF96NTJF', 'Aly2016'),\n",
       " (431, 'TX46X33P', 'Bai2018'),\n",
       " (432, 'VSS69KW9', 'Bai2018a'),\n",
       " (433, 'HHLDD6VN', 'Bai2019'),\n",
       " (435, 'VZRC3QKE', 'Censor2001a'),\n",
       " (436, '95D9N8YW', 'Censor2001'),\n",
       " (437, '7CZVWJYY', 'Dokmanic2013'),\n",
       " (438, '7NVZXNFN', 'Durgin2019'),\n",
       " (439, 'QB74Q2ZG', 'Lieb2020'),\n",
       " (441, 'LDBE7W2Q', 'Lunglmayr2018'),\n",
       " (442, 'NW34PSF3', 'Qu2018'),\n",
       " (446, 'Z9S8PHGD', 'Crandall2014'),\n",
       " (448, 'SUWRC5Y9', 'Moorman2021'),\n",
       " (450, 'HQUMWS5X', 'Li2018'),\n",
       " (452, 'GI6925RD', 'Vandenberghe2013'),\n",
       " (454, 'YEPXJDM3', 'StefanKidermann'),\n",
       " (456, 'MLJV6IUI', 'Nesterov2021'),\n",
       " (458, 'VSMIFTFG', 'Nesterov2013'),\n",
       " (460, 'J3D4KXAP', 'Nesterov2007'),\n",
       " (462, 'QM8QLV4W', 'HanneKekkonen2019'),\n",
       " (465, 'WETI5VIH', 'Guan2020'),\n",
       " (467, 'VNIYDAD9', 'Huang2020'),\n",
       " (469, 'FUT5TMMB', 'Nutini2016'),\n",
       " (471, '6E5SB8X8', 'Oswald2015'),\n",
       " (473, 'ENVTBAC7', 'Popa2018'),\n",
       " (475, '8P3KFZIB', 'Beck2009'),\n",
       " (477, 'RVDFLBZR', 'Foucart2013'),\n",
       " (479, 'Z5LR3UYK', 'Rektorys1977'),\n",
       " (485, '5FHLXDFU', 'WalterGautschi2012'),\n",
       " (487, '9FRTCCVW', 'Candes2008'),\n",
       " (489, 'TRNCG7SV', 'Wilson2018'),\n",
       " (491, 'E3I2UYZD', 'Combettes2005'),\n",
       " (493, 'BNY6DVKZ', 'Ikeda2019'),\n",
       " (495, 'HG8KKJXW', 'Shah2018a'),\n",
       " (497, 'J3L29JHJ', 'Nagahara2020'),\n",
       " (499, '7FKHG94Q', 'Cho2003'),\n",
       " (501, 'IFISIJDE', 'Zyoud2017'),\n",
       " (503, 'KCGTZC8X', 'Sciences2014'),\n",
       " (505, 'TFA9C9J3', 'Bian2017'),\n",
       " (507, '7GYZYRFR', 'Korhonen2016'),\n",
       " (509, 'E7PULVTQ', 'Saaty2013'),\n",
       " (513, 'V67NRB5K', 'Saaty2003'),\n",
       " (515, '8SLY8AR8', 'Xu2018'),\n",
       " (517, 'SJSRIGG4', 'BanaECosta2008'),\n",
       " (519, 'I323E2P2', 'Emrouznejad2017'),\n",
       " (521, '82ZMDNIX', 'Wang2006'),\n",
       " (523, '6KLGI39Z', 'Perez2006'),\n",
       " (525, 'HM8M7WRJ', 'Wang2017'),\n",
       " (527, 'P8SC88WX', 'Yang2019'),\n",
       " (529, 'K3G2NVWK', 'Moreno-jimenez2016'),\n",
       " (531, 'VFP28VF9', 'Feichtinger1992'),\n",
       " (534, 'W4QW6JRQ', 'Crandall1951'),\n",
       " (538, 'HLCYPHXQ', 'Steinerberger2021a'),\n",
       " (544, 'BXLPZVV6', 'Pelillo2012'),\n",
       " (546, 'HZTU3C3U', 'Jiao2017'),\n",
       " (548, 'TPFVSV5G', 'Jin2019'),\n",
       " (550, '7FCS6PNW', 'Shah2018'),\n",
       " (556, 'ZMN6RFF5', 'Heckel2018'),\n",
       " (558, 'UEQL35AD', 'Pardalos2017'),\n",
       " (560, '8UQYDKBD', 'Mandal2018'),\n",
       " (564, 'FQV6EMP5', 'Dempe1987'),\n",
       " (566, 'U7EYCXPJ', 'Yuille2001'),\n",
       " (568, '57F2NNHD', 'Debnath2020'),\n",
       " (570, 'M5ZNE2YE', 'Sriperumbudur2012'),\n",
       " (572, '25ZYRQ7W', 'Shen2016'),\n",
       " (574, 'UZMJ8VMK', 'Ahmadi2018'),\n",
       " (576, 'W5LFHVT6', 'Sriperumbudur2009'),\n",
       " (578, 'LUQX94JI', 'Carrasco-gutierrez2019'),\n",
       " (580, 'M7S7S55S', 'Lipp2016'),\n",
       " (582, 'CRLGH2XJ', 'Mjolsness1990'),\n",
       " (584, 'R6HPJ6C5', 'Rangarajan1996'),\n",
       " (586, '6KGNZXBY', 'Ralphs2009'),\n",
       " (588, 'F7I8V8EZ', 'WEn1991'),\n",
       " (590, 'RPU6LD4M', 'Yeh1996'),\n",
       " (592, 'EAHRA8M8', 'Colson2005'),\n",
       " (594, 'DM8QYP53', 'Ben-ayed1990'),\n",
       " (596, 'SD7778N4', 'Dempe2020'),\n",
       " (603, '253SZQ85', 'Fliege2006'),\n",
       " (606, 'ZF6XYRPG', 'Bard1983'),\n",
       " (608, 'XSW4NMKH', 'Blair1992'),\n",
       " (610, 'ET3MS7NC', 'Gale2016'),\n",
       " (612, '5U45VJFY', 'Candler1988'),\n",
       " (615, 'XICLQE69', 'Wen1989'),\n",
       " (618, 'TM8EZDRS', 'unlu1987'),\n",
       " (622, 'XGAGTYCV', 'Gallo1977'),\n",
       " (624, '4P8XUHCA', 'Dempe2005'),\n",
       " (628, 'N4F8UE57', 'Chatterjee2005'),\n",
       " (630, '3RZCATAN', 'Gupta2022'),\n",
       " (633, 'F86H99HR', 'Sinha2018'),\n",
       " (635, 'LBNYQCYI', 'Helou2017'),\n",
       " (637, 'YPVY67LG', 'Xu2023'),\n",
       " (639, 'RUGZRURD', 'Dagreou2023'),\n",
       " (641, '6EJKG24B', 'Antil2023'),\n",
       " (643, 'WWPIMYFJ', 'Alesiani2023'),\n",
       " (645, 'K3KMVWJU', 'Li2023a'),\n",
       " (647, 'FA7WG6GX', 'Zhou2022'),\n",
       " (649, 'XDKFE4C6', 'Ji2023'),\n",
       " (651, 'ADGV57P4', 'Lu2023'),\n",
       " (653, 'X9GETDTD', 'Shen2023'),\n",
       " (655, 'YIBEDJ89', 'Li2023'),\n",
       " (657, 'YJRCTNBF', 'Huang2023'),\n",
       " (659, 'MB3PV964', 'Mehlitz2022'),\n",
       " (661, 'UL4U7BAC', 'Doron2022'),\n",
       " (663, 'FQB6LZJ4', 'Cleach2022'),\n",
       " (665, 'DXKQUNMN', 'Cunis2022'),\n",
       " (667, 'CFXI8JAW', 'Guglielmi2023'),\n",
       " (669, 'BMZEGTY7', 'Lu2023a'),\n",
       " (671, 'GDC37DJN', 'Chen2023'),\n",
       " (673, 'KAE6K34D', 'Bai2022'),\n",
       " (675, 'CESDTW8Q', 'Jiang2022'),\n",
       " (677, 'IQ825D6K', 'Shen2022'),\n",
       " (679, '5367LCVU', 'Yang2022'),\n",
       " (681, '5Q6BWBWV', 'Lamontagne2022'),\n",
       " (683, 'NK5ANRRN', 'Chen2022a'),\n",
       " (685, 'S68TSLNF', 'Chen2022b'),\n",
       " (687, 'DT386PQ6', 'Sharrock2022'),\n",
       " (689, 'SXLZI77J', 'Gao2022a'),\n",
       " (691, 'AGQTPT2D', 'Hu2022'),\n",
       " (693, 'XQ492XTK', 'Ding2022'),\n",
       " (695, '6TXXEUBF', 'Liu2022'),\n",
       " (697, '87SPSVBW', 'Chen2022c'),\n",
       " (699, '3QN8LT7X', 'Gao2022'),\n",
       " (701, '7WAJK947', 'Tarzanagh2022'),\n",
       " (703, 'HINCII67', 'Yu2023'),\n",
       " (705, 'TL4JSHM6', 'Sow2022'),\n",
       " (707, 'U7VSMGYS', 'Ghosh2022'),\n",
       " (709, 'EST4982Y', 'Shirai2022'),\n",
       " (711, 'PIINZZSZ', 'Bian2022'),\n",
       " (713, 'T9IPBG26', 'Pan2022'),\n",
       " (715, 'YS9ZQ4HQ', 'Han2021'),\n",
       " (717, '8GI6HS77', 'Suonpera2022'),\n",
       " (719, '72DJH6VL', 'Dagreou2022'),\n",
       " (721, 'V6KYW76N', 'Lee2022'),\n",
       " (723, 'GFD27SSZ', 'Huang2022'),\n",
       " (725, 'ZJYHTBDY', 'Liu2022a'),\n",
       " (727, 'E557DDDB', 'Li2022'),\n",
       " (729, 'MCJ7CNMD', 'En-naciri2022'),\n",
       " (731, 'DCNALP8B', 'Friedemann2022'),\n",
       " (733, 'ZTE5RHYL', 'Garcia2022'),\n",
       " (735, 'IZJ6SK2J', 'Dyro2022'),\n",
       " (737, 'FTX5EYAJ', 'Zucchet2022'),\n",
       " (739, 'U4FTS88Y', 'Huang2021'),\n",
       " (741, 'BWLLQG63', 'Chen2021'),\n",
       " (743, 'ZHVKVS69', 'Huang2022a'),\n",
       " (746, '5K3AE3BT', 'Sato2021'),\n",
       " (748, 'KYW7LMT4', 'Poon2021'),\n",
       " (750, '2M38SR4F', 'Bui2022'),\n",
       " (752, 'JLKZL69I', 'Bao2021'),\n",
       " (754, 'RCTEHWMF', 'Fan2021'),\n",
       " (756, 'EFARBJCI', 'Crockett2022'),\n",
       " (758, 'IWCFKRWW', 'Liu2021'),\n",
       " (760, 'DPGI86W5', 'Sundar2021'),\n",
       " (762, 'XCLF7C4H', 'Ke2022'),\n",
       " (764, 'I5CUKMSD', 'Ma2021'),\n",
       " (766, '2G9MIUPZ', 'Ji2021'),\n",
       " (768, 'E64CWC84', 'Buchheim2022'),\n",
       " (770, '4FGKJDIE', 'Mehlitz2021'),\n",
       " (772, 'JLYLC8QJ', 'Basciftci2020'),\n",
       " (774, 'WW7N4GQ6', 'Fliege2021'),\n",
       " (776, 'AJWYDT2V', 'Bai2022a'),\n",
       " (778, '7M5M5AFA', 'Taninmis2019'),\n",
       " (780, 'R4XU57TF', 'Mehlitz2020'),\n",
       " (782, 'KX88B488', 'Mordukhovich2020'),\n",
       " (784, 'MAVUFTDA', 'Ye2020'),\n",
       " (786, 'WTDSXS2A', 'Sessa2020'),\n",
       " (788, 'GL3T9J78', 'Sinha2020'),\n",
       " (790, 'JBN9BIKY', 'Benko2021'),\n",
       " (792, 'BNVSNCSI', 'Yousefian2021'),\n",
       " (794, 'J3F734QF', 'Wang2020'),\n",
       " (796, '2Z7RQCNR', 'Hao2020'),\n",
       " (798, 'LXJZFP6M', 'Gao2020'),\n",
       " (800, '4QQ8SMFX', 'Bae2020'),\n",
       " (802, 'PCAWQ2X3', 'Shaker2021'),\n",
       " (804, 'C7M2VSLM', 'Tong2022'),\n",
       " (806, '8TDTCJW6', 'Li2020'),\n",
       " (808, 'FCM6R2BF', 'Chang2021'),\n",
       " (810, 'KW42Z4UD', 'Ji2020'),\n",
       " (812, 'V296MRRP', 'Borsos2021'),\n",
       " (814, 'SUTM5Q9U', 'Picallo2022'),\n",
       " (816, 'IZG9TLMD', 'Seth2021'),\n",
       " (818, 'QTV3M7T9', 'Ji2021a'),\n",
       " (822, 'T23ECC4V', 'Kosmas2022'),\n",
       " (824, '74QDVUN8', 'Grazzi2020'),\n",
       " (826, 'QLGS3FMX', 'Kontonis2020'),\n",
       " (828, 'WSU6GPVE', 'Roh2020'),\n",
       " (830, '2CW4LMYK', 'Bergounioux2006'),\n",
       " (832, 'RZVEM7E9', 'Gutfraind2011'),\n",
       " (834, 'YYCFLDCQ', 'Ochs2016'),\n",
       " (836, 'VLEB98Z3', 'Mackay2019'),\n",
       " (838, 'USKLZNPB', 'Khanduri2021'),\n",
       " (840, 'IQUPQ2Y2', 'Mori2021'),\n",
       " (842, 'T5IYSD49', 'Cao2022'),\n",
       " (844, 'JFMRWU7H', 'Sun2022'),\n",
       " (848, 'UQ3ZP9WV', 'Sinha2020a'),\n",
       " (850, 'U4WGC3PK', 'Ouattara2018'),\n",
       " (852, 'QD2ZTDNF', 'Brannstrom2016'),\n",
       " (855, 'BIBXG6VL', 'Liu2016'),\n",
       " (857, 'TIMTYM6M', 'Holler2018'),\n",
       " (859, 'R22I3PLQ', 'Ye1997'),\n",
       " (870, 'KX96NT9Y', 'Lin2014'),\n",
       " (872, '8VNM7A2I', 'Dempe2012'),\n",
       " (879, 'B64TVTKP', 'Iyer'),\n",
       " (881, '9Y76YMZI', 'Rockafellar1993'),\n",
       " (883, 'WBVL9TSH', 'Borwein2016'),\n",
       " (885, '23TVFPBQ', 'Nunemacher2003'),\n",
       " (887, 'HYPCVZJM', 'Brezhneva2012'),\n",
       " (891, 'ULQGKA8I', 'Borwein1981'),\n",
       " (893, 'DYHMYXDI', 'Parikh2014'),\n",
       " (896, 'GX8DJPYF', 'Das2022'),\n",
       " (915, 'WRNQAA7T', 'Ehrhardt2021'),\n",
       " (928, 'J6UB5VSV', 'Tomkins1968'),\n",
       " (930, '88L5JPY3', 'Lei2018'),\n",
       " (932, 'CZL7RGSJ', 'Tanaka1981'),\n",
       " (935, 'R69KWBZB', 'Crandall1971'),\n",
       " (937, 'N44RWBNR', 'Ikeda2021'),\n",
       " (941, 'GVSL54Z6', 'Mousavi2016'),\n",
       " (944, 'TD6I25LG', 'Bubeck2015'),\n",
       " (947, 'VXNQ6MRC', 'Aggarwal2014'),\n",
       " (956, 'TV93ENB2', 'Schopfer2019'),\n",
       " (959, 'NDN3GLWN', 'Deutsch1997'),\n",
       " (961, 'H6ZP6LB5', 'Donoho2006'),\n",
       " (963, 'GUWQGCPC', 'Mansour2013'),\n",
       " (965, 'U9D53TQ3', 'Schmidt2015'),\n",
       " (967, '4KF9M5N8', 'Ben-ayed1993'),\n",
       " (969, 'TLT6W8WR', 'Moscoso2012'),\n",
       " (971, 'AEKQM9AP', 'Hegde2019'),\n",
       " (972, 'XP2LVELR', 'Mailhe2011'),\n",
       " (974, '9VZSGX4V', 'Du2020'),\n",
       " (976, 'ACY2Z6BX', 'Yuan2014'),\n",
       " (978, 'SYLW8VD7', 'Dax2006'),\n",
       " (980, 'KNYR4QI4', 'Osher2023'),\n",
       " (983, '2PLBN8GG', 'Rockafellar2006'),\n",
       " (985, 'LMIT9F5F', 'Haurie1990'),\n",
       " (987, 'BXQFVNZG', 'Deutsch2001'),\n",
       " (989, 'G83MTYHE', 'Finn2017'),\n",
       " (991, 'BQAQ6S6P', 'Domke2012'),\n",
       " (993, 'BGFRXZZU', 'Hataya2023'),\n",
       " (1000, 'AVTWNRQ4', 'Hong2023'),\n",
       " (1002, '32XPMGIN', 'Yang2021'),\n",
       " (1004, 'LK4LNP5M', 'Ghadimi2018'),\n",
       " (1008, 'R2GG9C3C', 'Bazaraa2006'),\n",
       " (1014, 'HZ6RZ4WV', 'Fletcher2000'),\n",
       " (1022, 'FZ2J6UW9', 'Colson2007'),\n",
       " (1024, '6RXCS6CX', 'Borkar1997'),\n",
       " (1026, 'DSBFU4IC', 'Avrachenkov2022'),\n",
       " (1029, 'HG63K2JX', 'Chen2022'),\n",
       " (1031, 'NF9328RX', 'Ghadimi2013'),\n",
       " (1556, 'SX9HFZV6', 'Ailon2012'),\n",
       " (1574, 'AZSERNWI', 'Ailon2008'),\n",
       " (1610, '5VAKT6FZ', 'Adams2012'),\n",
       " (1617, '4BFIEB8W', 'Agaskar2015'),\n",
       " (1648, '2JZ9EPED', 'Parlett1974'),\n",
       " (1841, '8ZG6NSPD', 'Rudin2008'),\n",
       " (1892, 'G9GAP343', 'Rajeswaran2019'),\n",
       " (1904, '4TCHH3DV', 'Grant2018'),\n",
       " (1930, 'NDAXQPTT', 'okten2005'),\n",
       " (1931, 'WKEIB3P2', 'Wu2019'),\n",
       " (1932, 'SI7RCS5P', 'Acebron2020'),\n",
       " (1933, 'PZQ39PPS', 'Wasow1952'),\n",
       " (1934, 'VUTZNP8L', 'Forsythe1950'),\n",
       " (1937, 'M5QUNHC3', 'Strassen1969'),\n",
       " (1940, 'M8742NRY', 'Bertsekas2016'),\n",
       " (1980, 'IXUQD4AS', 'Udell2016'),\n",
       " (1985, 'MVNAP3IH', 'Tardella1990'),\n",
       " (1988, 'RPTLPDGS', 'Kerdreux2018'),\n",
       " (1989, 'UITHCF6S', 'Aubin1976'),\n",
       " (1990, 'R8NYDN9B', 'Cassels1975'),\n",
       " (1999, 'XCXC32JT', 'Khan1974'),\n",
       " (2001, 'J6T5QPNQ', 'DiGuglielmo1977'),\n",
       " (2002, '68KXJH92', 'Kerdreux2020'),\n",
       " (2021, 'RPWK3AG3', 'Starr2008'),\n",
       " (2023, '2RCYD5AE', 'Kumar2023'),\n",
       " (2025, '6G2NIYY2', 'Aubin2009'),\n",
       " (2049, '733KMJE5', 'Boyd2009'),\n",
       " (2050, 'UAMAF28K', 'Blasjo2005'),\n",
       " (2052, 'TYT7VIK5', 'Duchi2021'),\n",
       " (2060, 'TIHSDGWC', 'Bulicek2014'),\n",
       " (2062, 'MJBT3QGE', 'Bressan2014'),\n",
       " (2064, '7YBJL2W4', 'Allen2014'),\n",
       " (2066, 'NIS9MWY6', 'Couellan2016'),\n",
       " (2068, 'H4SILD9W', 'Akhtar2022'),\n",
       " (2071, 'CA5RTJC3', 'Danilova2022'),\n",
       " (2074, 'HHSUQXD6', 'Bush2011'),\n",
       " (2076, 'GLKPGHK9', 'Edelman2005'),\n",
       " (2077, '89X4LYKI', 'Livan2018'),\n",
       " (2081, 'NAZ52QSA', 'Quadrat2006'),\n",
       " (2083, 'GXRUTHIA', 'Quadrat2006a'),\n",
       " (2085, 'Y5ZRIZFC', 'Quadrat2003'),\n",
       " (2088, 'RSZDH8XB', 'Quadrat2003a'),\n",
       " (2091, 'QWUDCTW7', 'Lewis2016'),\n",
       " (2092, 'SSG4LNHE', 'Terrell2018'),\n",
       " (2097, 'AM5T36AN', 'Terrell1999'),\n",
       " (2100, '4AI3NRNX', 'Evans2021'),\n",
       " (2102, 'W53XHJFE', 'Evans2021a'),\n",
       " (2106, 'UVDSIKLJ', 'Netrapalli2014'),\n",
       " (2113, 'NUIVC8XC', 'Kirchheim2009'),\n",
       " (2116, 'CUN9HMSZ', 'Meshulam1996'),\n",
       " (2139, 'MYG5HYEB', 'DeOliveira2020'),\n",
       " (2141, 'L84AZXSF', 'Lasserre2002'),\n",
       " (2142, 'LFHGKH5R', 'zotero-2142'),\n",
       " (2145, 'NEVPSSTE', 'Polyakova'),\n",
       " (2147, 'NDDCVQTC', 'zotero-2147'),\n",
       " (2159, 'QHYBBJVD', 'zotero-2159'),\n",
       " (2162, 'SYGSGKBS', 'Whitesides2004'),\n",
       " (2164, 'C25WS4RA', 'Roscoe'),\n",
       " (2166, 'BGW8YZT4', 'keshav'),\n",
       " (2199, 'ZGBMABRG', 'Diakonikolas2019'),\n",
       " (2206, 'IBGVHNXB', 'Borkar2023a'),\n",
       " (2008, 'QDVZRKW6', 'Fradelizi2017'),\n",
       " (2213, '9R87NLTA', 'Gardner2002'),\n",
       " (2223, 'B3M9YCEK', 'Milenkovic2007'),\n",
       " (2224, '3HUSHI9S', 'Guibas1986'),\n",
       " (2225, '3N8SVK7A', 'sir2006'),\n",
       " (2219, 'QAXRATHA', 'Peternell2007'),\n",
       " (2245, 'XMRSNPK4', 'Oberman2007'),\n",
       " (2246, 'QYXVKXWR', 'Abbasi2018'),\n",
       " (2247, 'V3I97W65', 'Carlier2012'),\n",
       " (2248, 'TAC5CCVG', 'Li2021'),\n",
       " (2249, 'TKXBP9NY', 'Oberman2008'),\n",
       " (2250, 'Y2ZTWYCK', 'Oberman2017'),\n",
       " (2251, 'ZI55TRFC', 'Oberman2011'),\n",
       " (2252, 'ZKNC9EJH', 'Vese1999'),\n",
       " (2263, 'P3Q77HTP', 'Griewank1990'),\n",
       " (2265, 'SZFQTK9W', 'Brighi1994'),\n",
       " (2267, 'RQLTTV5T', 'Lucet1996'),\n",
       " (2271, '94Y24AA9', 'Lucet1997'),\n",
       " (2274, '6WN9AGEY', 'Touchette'),\n",
       " (2280, 'T3XYVZIP', 'Dyke2014'),\n",
       " (2284, 'BPS4WRHB', 'Chizat'),\n",
       " (2289, 'EB7CBIL2', 'Ma2019'),\n",
       " (2290, '9EY452RP', 'Hoeksema2023'),\n",
       " (2293, 'ZHUPKXN2', 'Chizat2020'),\n",
       " (2297, 'DL84AV32', 'Borkar2005'),\n",
       " (2323, 'G5RGAS9N', 'Zhao2023'),\n",
       " (2326, '3ADSLNVF', 'Chung2000'),\n",
       " (2329, 'DPXUVVDG', 'Bietti2021'),\n",
       " (2332, '53RJAB6B', 'Villani2023'),\n",
       " (2409, 'LHNL6N4K', 'Brezis2011'),\n",
       " (2415, 'U8DG8YD2', 'Folland1999'),\n",
       " (2420, 'JHARL7CQ', 'Malliavin1995'),\n",
       " (2423, 'TDBC275P', 'Billingsley1995'),\n",
       " (2424, '8KUMGKB2', 'Hajek2015'),\n",
       " (2425, '7VJIRWUM', 'Grimmett2007'),\n",
       " (2426, 'IWI8WTJT', 'Kumar2012'),\n",
       " (2462, '9Q4PUE3N', 'Thorpe'),\n",
       " (2463, 'E2XJGMA3', 'Bach'),\n",
       " (2464, 'WQHNK93Z', 'Jain2017'),\n",
       " (2475, 'HVV5DMT2', 'Ross2013'),\n",
       " (2478, '4JXH2XM2', 'Sontag1998'),\n",
       " (2480, 'VPXP2AK5', 'Spivak1965'),\n",
       " (2512, 'YVSPZIPB', 'Liberzon2012'),\n",
       " (2514, 'CYF3H42W', 'Zhao2022'),\n",
       " (2520, 'J8RIPNJ2', 'Salsa2016'),\n",
       " (2526, 'ULZP4LQ7', 'Jiang2023'),\n",
       " (2539, 'ASN6GNPX', 'Perko2009'),\n",
       " (2545, 'QME944RQ', 'Santambrogio2015'),\n",
       " (2549, 'BLK28USM', 'Howe2012'),\n",
       " (6494, '9DHKR7F5', 'Ross2013a'),\n",
       " (6502, 'L67A9ZG3', 'oksendal2013'),\n",
       " (6504, '8FISQTEU', 'zotero-6504'),\n",
       " (6512, '3MLSF4GH', 'Dewaskar'),\n",
       " (6513, 'REDSUE33', 'Li2020a'),\n",
       " (6520, '7UWS8YYW', 'Neumann1979'),\n",
       " (6523, 'HUFFMRIC', 'Nisan2007'),\n",
       " (6525, 'G8AXY2EP', 'Jurg1992'),\n",
       " (6526, 'GH7IVJP2', 'Raghavan1970'),\n",
       " (6534, 'JDDMSGAY', 'Axler2024'),\n",
       " (6535, 'F79MNIM5', 'Ekeland2006'),\n",
       " (6537, 'X6U8BEQ8', 'Parthasarathy1971'),\n",
       " (6541, 'XVZDCPUL', 'Rockafellar1996'),\n",
       " (6546, 'MU9HSBIG', 'zotero-6546'),\n",
       " (6564, 'VM8XSFG6', 'Mangasarian1964'),\n",
       " (6550, 'WRSZCDCC', 'Mclennan2010'),\n",
       " (6548, 'FCCGDB8L', 'Lemke1964'),\n",
       " (6566, 'FTG8QBMD', 'Kuhn'),\n",
       " (6569, '987PGFQV', 'zotero-6569'),\n",
       " (6580, 'NHM9EKT4', 'zotero-6580'),\n",
       " (6582, 'LW9DN8ED', '2018'),\n",
       " (6589, 'Z6Q6ESC9', 'Beck2013'),\n",
       " (6590, 'ZK757MRG', 'Chen2020'),\n",
       " (6598, 'DDK2DSG6', 'Kontogiannis2011'),\n",
       " (6596, 'NPG6855A', 'Robbins1951'),\n",
       " (6604, 'FGKUKTI2', 'Schrijver2011'),\n",
       " (6605, 'KK8UI6P3', 'Borkar1995'),\n",
       " (6606, 'UTQJR277', 'Norris1998'),\n",
       " (6640, '8HIJLTYX', 'Avis2010'),\n",
       " (6656, 'WQMZB9Y5', 'Pauwels2016'),\n",
       " (6658, '2JSZ378F', 'Dashti2017'),\n",
       " (6670, 'ZPYIZLPA', 'Bertsimas2020'),\n",
       " (6671, '6TGWFKRI', 'Dey2021'),\n",
       " (6678, 'JP7PBGDM', 'Hermer2022'),\n",
       " (6681, 'PWP6TRJY', 'Joseph2020'),\n",
       " (6683, 'LRHT9IA4', 'Bedi2021'),\n",
       " (6685, 'TGZS2GAG', 'Dillon2016'),\n",
       " (6687, 'CDWNYPIU', 'Wang2011'),\n",
       " (6689, 'XNB9BHCQ', 'Dong2020'),\n",
       " (2535, 'SHUUHMFJ', 'Hirsch2013'),\n",
       " (6820, 'EUTTM3QP', 'Norris'),\n",
       " (7219, 'YE8AL5EC', 'Pang2010'),\n",
       " (7383, 'TN7ALP6S', 'Karimi2016'),\n",
       " (2104, 'L6GK6MFG', 'Evans'),\n",
       " (7463, 'GUT68ITQ', 'Ardizzone2019'),\n",
       " (7559, 'LHLWVIXR', 'Kucukyavuz2022'),\n",
       " (7560, 'VNHXNV7B', 'Vancroonenburg2019'),\n",
       " (7561, 'T2BS2YNW', 'Peng'),\n",
       " (12, 'RK9FV68J', 'Aminikhanghahi2017'),\n",
       " (16, 'XD2QDF6R', 'Truong2018'),\n",
       " (26, 'JRFNKVVT', 'Andrew1998'),\n",
       " (7583, 'WU63C4YN', 'Lampariello2017'),\n",
       " (7586, 'P7YVHZEW', 'Harwood2023'),\n",
       " (7589, '4IYUWGHI', 'Garg2021'),\n",
       " (7592, 'DCD846RR', 'Garg2023'),\n",
       " (7594, 'ULW867T9', 'Baranwal2023'),\n",
       " (7598, 'QGKF53E9', 'Chen2020a'),\n",
       " (7600, 'GAIIR9TL', 'Santambrogio2017'),\n",
       " (7602, '4YZDPSDI', 'Garg2022'),\n",
       " (7604, 'MGA3TLYL', 'Garg2023a'),\n",
       " (7606, '6GVSFAUV', 'Budhraja2022'),\n",
       " (7609, 'P7LFRG9U', 'Polyakov2022'),\n",
       " (7611, 'SVUGDZK5', 'Polyakov2019'),\n",
       " (7613, 'U6NKG6QQ', 'Polyakov2012'),\n",
       " (7615, 'XKAKTMFV', 'Degroot1974'),\n",
       " (7677, 'P4GT2D7Q', 'Ramirez2024'),\n",
       " (7680, 'DJXYCKFC', 'Beznosikov2023'),\n",
       " (7686, 'KLLYDBHK', 'Gorbunov2022'),\n",
       " (7707, 'DCJY5JKH', 'Meng'),\n",
       " (7726, '76TH2GMP', 'Karimi2020'),\n",
       " (7738, '56PZGRND', 'Nesterov2017'),\n",
       " (7742, 'LXBUSJ2D', 'He2023'),\n",
       " (7748, 'ZWLANXSB', 'Chakrabarti2021'),\n",
       " (7750, 'FPQPQFTU', 'Chakrabarti2020'),\n",
       " (7753, 'TR46MDJT', 'Chakrabarti2021a'),\n",
       " (7756, 'VXLLIZZD', 'Chakrabarti2024'),\n",
       " (7759, 'KCAMICM6', 'Pearlmutter1994'),\n",
       " (7761, 'Y236DCLL', 'Bernard2018'),\n",
       " (7763, 'V2YWWR4I', 'Cerone2024'),\n",
       " (7766, 'XJ3JSDG6', 'Lessard2016'),\n",
       " (7770, 'FF4VZCZB', 'Borkar2024'),\n",
       " (7773, '3JG2TTU3', 'Sohrabi2024'),\n",
       " (7776, 'N2QWIIRS', 'Stooke2020'),\n",
       " (7783, '3SKSEPUU', 'Casti2024'),\n",
       " (7785, '9V8V8MKM', 'Recht'),\n",
       " (7787, 'BAXV54PN', 'Platt1987'),\n",
       " (7793, 'LVAI3AJ6', 'Chambolle2015'),\n",
       " (7795, 'HDYUPIKM', 'Muthukumar'),\n",
       " (7797, 'AYX2NTTI', 'Braides1993'),\n",
       " (7799, 'BQLRGK8U', 'Calder'),\n",
       " (7801, 'B47L87RA', 'Attouch1996'),\n",
       " (7811, 'DBSLNPCI', 'Nguyen'),\n",
       " (7813, 'D582IJA7', 'Arsov2019'),\n",
       " (7816, 'UDXME7M7', 'Hinton2002'),\n",
       " (7818, '4BFFT2QW', 'Maaten2009'),\n",
       " (7820, '5CWI3RNN', 'SvenLeyffer2024'),\n",
       " (7822, 'DZDT7HTU', 'Daskalakis'),\n",
       " (7825, '4Q4F4I5D', 'Codenotti2011'),\n",
       " (7834, 'T5QRULZF', 'Harwood2017'),\n",
       " (7837, '9778BF5F', 'Guignard1969'),\n",
       " (7840, 'ZINNGGYJ', 'Izmailov2009'),\n",
       " (7862, 'A9E84PHB', 'Dorsch2012'),\n",
       " (7864, '7H27TX7X', 'Hoheisel2008'),\n",
       " (7866, '5LGH473B', 'Hoheisel2009'),\n",
       " (7876, 'ZXQF4GQP', 'Dussault2019'),\n",
       " (7910, 'C3XLGBTZ', 'Achtziger2008'),\n",
       " (7944, 'ANXWV3IN', 'Mangasarian1993'),\n",
       " (7946, 'VZ7BRXYN', 'Andreani2011'),\n",
       " (7948, 'MNBZI49C', 'Izmailov2012'),\n",
       " (7950, 'XZGSSPUZ', 'Guo2022'),\n",
       " (7952, 'L6UJV5FP', 'Luo2010'),\n",
       " (7954, '48ZT7QX6', 'Andreani2018'),\n",
       " (7958, 'X9ZJMGIT', 'Hoheisel2012'),\n",
       " (7965, '83GLC3UG', 'Steffensen2010'),\n",
       " (7968, '4QIX7V6J', 'Scholtes2001'),\n",
       " (7983, 'TL9T9MBD', 'Fukushima1999'),\n",
       " (7991, '74JC6NWT', 'Jongen1991'),\n",
       " (7999, 'FS9BMUIT', 'Robinson1975'),\n",
       " (8006, 'JA893DVY', 'Robinson1976'),\n",
       " (8016, 'FNYZEJYX', 'Chen1996'),\n",
       " (8022, 'U7CQVDD6', 'Scheel2000'),\n",
       " (8034, 'IVMUTJ2K', 'Scholtes2004'),\n",
       " (8044, 'LWG9WJYQ', 'Outrata1998'),\n",
       " (8048, '28UMGK7G', 'Fletcher2004'),\n",
       " (8064, 'HE2JLYX2', 'Wilson2021'),\n",
       " (7768, 'A2RABNVG', 'Hu2017'),\n",
       " (7736, 'XRZVNQ8U', 'Zhang2022'),\n",
       " (540, 'CQ7HWCU7', 'Stewart1999'),\n",
       " (542, 'R25M9S8R', 'Stewart1998'),\n",
       " (7617, 'RCMHIXNS', 'Baranwal2024'),\n",
       " (255, 'Q7YDEKGJ', 'Yosida1967'),\n",
       " (273, '7I2IXUME', 'Li2017'),\n",
       " (135, '6XJXGAJK', 'Lebedev2003'),\n",
       " (8069, '4XY63AV6', 'Gale1951'),\n",
       " (2469, 'X6FZTY3K', 'Srinath2009'),\n",
       " (2388, 'TAGXADRL', 'Saxe2001'),\n",
       " (2305, 'TV7EF8NR', 'Sterman2000'),\n",
       " (2401, 'EXSNRXTC', 'Kolmogorov1954'),\n",
       " (2396, '59EDJEQK', 'Kolmogorov1960'),\n",
       " (2317, 'V7HQ9MXB', 'Pritchard2010'),\n",
       " (49, 'CDY9R2U2', 'Palmieri2020'),\n",
       " (2405, '87MZZDKG', 'Kreyszig1989'),\n",
       " (2315, 'HC48X7LX', 'Neamen2010'),\n",
       " (2465, 'X5T7DYXD', 'Das2017'),\n",
       " (2479, 'KYBHKAVS', 'Timoshenko2009'),\n",
       " (6654, 'NPWJRJZ5', 'Costin'),\n",
       " (2509, 'UNBW8EBI', 'Raginsky2019'),\n",
       " (2411, 'SG3VP3PZ', 'Balamurugan2019'),\n",
       " (2428, '45JPUPDP', 'Borkar2020'),\n",
       " (2427, 'G6MQSPLP', 'K2020'),\n",
       " (2312, 'TL5MUV7C', 'Reddy2014'),\n",
       " (6593, '9G5G4W5K', 'Tibshirani'),\n",
       " (7702, '5K3XAJ6L', 'Cherukuri2016'),\n",
       " (7697, 'WSG92MG3', 'Cortes2019'),\n",
       " (7706, 'J9K8KK2I', 'Lygeros2003'),\n",
       " (7712, 'LH7T69A3', 'Rockafellar1971'),\n",
       " (7720, 'SQ726MI9', 'Benzi2005'),\n",
       " (7689, 'TQ4QXSCX', 'Qu2019'),\n",
       " (7723, 'RA3DP3NL', 'Bergamaschi2004'),\n",
       " (7709, 'UQHZW2Y2', 'Tang2020'),\n",
       " (7704, 'IBETDKI2', 'Feijer2010'),\n",
       " (7699, '74BMW36R', 'Dhingra2019'),\n",
       " (8077, '6RNZQYKF', 'Shen2023a'),\n",
       " (949, '4KFX7LWU', 'Bard1984'),\n",
       " (8079, 'QB3EMVYW', 'Du2019'),\n",
       " (8085, 'LBHMTUID', 'Camponogara2016'),\n",
       " (8087, '3JMLJXSI', 'Dauphin2014'),\n",
       " (8091, '7DSXUVHI', 'Pillo1994'),\n",
       " (511, 'SRTWUGVT', 'Steinerberger2021'),\n",
       " (7886, '8LDBL5VL', 'Hoheisel2009a'),\n",
       " (1682, 'AZCSU4V9', 'Khalil2014'),\n",
       " (483, 'VMURAD7R', 'Nesterov2018'),\n",
       " (562, 'JX3BNSI3', 'Centre1985'),\n",
       " (889, 'UX3MGI8B', 'Dutta2011'),\n",
       " (996, 'Y9MKWRDB', 'Shapiro2021'),\n",
       " (998, '2WMXG5V8', 'Borkar2008'),\n",
       " (1012, 'K3B6252W', 'Bazaraa2009'),\n",
       " (1018, '7YSSSS5Y', 'Forst2010'),\n",
       " (1020, 'V47UFD67', 'Nocedal2006'),\n",
       " (1010, 'XLNV8YQT', 'Padregal2003'),\n",
       " (1951, 'EC8HECN7', 'Tihomirov1990'),\n",
       " (1846, 'CQFE96PZ', 'Bertsekas2009'),\n",
       " (1859, 'R37HP73W', 'Boyd2004'),\n",
       " (1961, 'RGL5BVFW', 'Hardt2018'),\n",
       " (1968, 'RNYA5SAG', 'Borkar2023'),\n",
       " (1977, 'LGIPAX7M', 'Bonnans2019'),\n",
       " (2211, 'YPF8RK89', 'Schneider2014'),\n",
       " (7144, 'NA5UHIE8', 'Bazaraa1976'),\n",
       " (7217, 'AZ9RD4MH', 'Borwein2006'),\n",
       " (7485, 'LQW3DPKF', 'Bertsekas1996'),\n",
       " (8083, 'WJP8WVDN', 'Rothvoss2020'),\n",
       " (8051, '5ZVK5NKM', 'RyanTibshirani2019'),\n",
       " (8130, 'HALFM5Q3', 'Magal2022'),\n",
       " (8132, 'LRIPR8GQ', 'Magal2022a'),\n",
       " (8134, '6EZ984XQ', 'Magal2022b'),\n",
       " (8159, 'RQK3L7C9', 'Lei'),\n",
       " (8170, '5ECNRFJ4', 'Pata2019'),\n",
       " (8173, 'RBGAFZ8Z', 'Ray2023'),\n",
       " (8165, 'X2CC7EL5', 'Bach2016'),\n",
       " (8175, 'E3HHTNYJ', 'Anderson1972'),\n",
       " (8188, 'SFR4JTHQ', 'Tibshirani2011'),\n",
       " (8189, 'DRKIX57Q', 'Kovalev2022'),\n",
       " (8199, '78JBZTIU', 'Li2019'),\n",
       " (8201, 'JFRTDCNU', 'Tupitsa2020'),\n",
       " (8205, 'TDY57UIG', 'Nemeth2015'),\n",
       " (8209, 'C8GVBIK8', 'Salmon2024'),\n",
       " (8210, 'UDJBKUTD', 'Pedregosa2013'),\n",
       " (8214, '8IQK94FZ', 'Wang2022'),\n",
       " (8215, 'AE56J47N', 'Yang2019a'),\n",
       " (8217, 'T4EM8DU9', 'Brunk1970'),\n",
       " (8219, '5JU6B2BV', 'Han1988'),\n",
       " (8225, 'H68YWB9L', 'Beck2015'),\n",
       " (8230, '488DV58T', 'Beck2009b'),\n",
       " (8071, 'DLDZGVE5', 'Lu2024'),\n",
       " (8074, 'SDIX8WX2', 'Shen2024'),\n",
       " (8239, 'SH2J5LD2', 'Ruppert2009'),\n",
       " (8240, '5TEQWAX4', 'Yuan2021'),\n",
       " (8185, 'N7PJYTMR', 'Barlow1972'),\n",
       " (8257, '57RVFNFA', 'Monteiro2021'),\n",
       " (8244, 'CQIK4QVI', 'Shakerinava2024'),\n",
       " (8260, 'D8FEABCQ', 'Liu2022b'),\n",
       " (8261, '2RCZYDAS', 'Gupta2019'),\n",
       " (6585, 'CDL7X8G9', 'Frongillo')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8113eb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "queryC = \"\"\"\n",
    "SELECT itemData.itemID, fields.fieldName, itemDataValues.value \n",
    "FROM itemData\n",
    "JOIN fields ON itemData.fieldID = fields.fieldID\n",
    "JOIN itemDataValues ON itemData.valueID = itemDataValues.valueID\n",
    "\"\"\"\n",
    "data = cur.execute(queryC).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "43de65db",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'title', 'slidesAll.pdf'),\n",
       " (2,\n",
       "  'title',\n",
       "  'The role of extrusions and intrusions in fatigue crack initiation'),\n",
       " (2,\n",
       "  'abstractNote',\n",
       "  'The profiles of persistent slip markings produced by uniaxial and biaxial cyclic straining in four different polycrystalline materials with f.c.c. structure were investigated using focused ion beam (FIB) cutting and TEM observation of oriented surface foils. Typical shapes of persistent slip markings are extrusions accompanied by parallel intrusions. In some cases only extrusions were developed and intrusions were produced later in fatigue life. In polycrystalline copper extrusions and intrusions appear on the surface of the grain where persistent slip band characterized by ladder-like dislocation structure egress on the surface. Similar features were observed in fatigued austenitic 316L and Sanicro 25 steels but the extrusion and intrusion shapes were more complicated. Crack-like intrusion shapes produce high stress and strain concentration and primary stage I crack starts to grow from the tip of intrusions. The experimental observations were compared with the predictions of the existing crack initiation models.'),\n",
       " (2, 'date', '2017-11-01 2017-11-01'),\n",
       " (2,\n",
       "  'url',\n",
       "  'https://www.sciencedirect.com/science/article/pii/S0013794416307305?dgcid=raven_sd_recommender_email'),\n",
       " (2, 'accessDate', '2019-10-01'),\n",
       " (2, 'volume', '185'),\n",
       " (2, 'pages', '46-60'),\n",
       " (2, 'publicationTitle', 'Engineering Fracture Mechanics'),\n",
       " (2, 'DOI', '10.1016/J.ENGFRACMECH.2017.03.006'),\n",
       " (2, 'ISSN', '0013-7944'),\n",
       " (3, 'title', 'PDF'),\n",
       " (4,\n",
       "  'title',\n",
       "  'Experimental evidence and physical models of fatigue crack initiation'),\n",
       " (4,\n",
       "  'abstractNote',\n",
       "  'Results of the study of the early fatigue damage in a number of model and structural crystalline materials using modern experimental techniques are presented. The dislocation structure of the persistent slip bands and the evolution of the surface relief resulting in the formation of persistent slip markings during cyclic loading are documented. The dislocation mechanisms leading to production of point defects in cyclic loading are described and point defect production and annihilation rates are derived. The kinetics of point defect migration is characterized. The physically based models of the surface relief formation describing the formation of extrusions and intrusions are presented. The models are confronted with experimental evidence. It is concluded that intrusions representing sharp surface crack-like defects play the principal role in the initiation of fatigue cracks.'),\n",
       " (4, 'date', '2016-00-00 2016'),\n",
       " (4, 'accessDate', '2019-10-01'),\n",
       " (4, 'volume', '91'),\n",
       " (4, 'pages', '294-303'),\n",
       " (4, 'publicationTitle', 'International Journal of Fatigue'),\n",
       " (4, 'DOI', '10.1016/j.ijfatigue.2016.02.021'),\n",
       " (4, 'ISSN', '01421123'),\n",
       " (5, 'title', 'PDF'),\n",
       " (6, 'title', 'The physics of fatigue crack initiation'),\n",
       " (6,\n",
       "  'abstractNote',\n",
       "  'The fatigue life of a component can be expressed as the sum of two segments of life: (a) the number of loading cycles required to initiate a crack and (b) the number of cycles it takes that crack to propagate to failure. In this review, the primary emphasis is relating the fatigue crack initiation to the microstructure of the material. Many studies have focused on this phenomenon over the years and the goal of this paper is to put this work in perspective and encourage future work of fatigue in polycrystals based on the material’s microstructure. In order to address fatigue, it is necessary to understand the mechanisms that facilitate crack initiation. Slip irreversibilities exist in a material and accumulate during fatigue loading. At the defect level, irreversibilities are a result of dislocations: annihilating, cross-slipping, penetrating precipitates, transmitting through grain boundaries, and piling-up. These slip irreversibilities are the early signs of damage during cyclic loading. The dislocations subsequently form low-energy, stable structures as a means to accommodate the irreversible slip processes and increasing dislocation density during cyclic forward and reverse loading. The result is strain localizing in a small region within the materials, i.e. persistent slip bands and dislocation cells/bundles. Strain localization is a precursor to crack initiation. This review paper will focus on experimental observations of strain localization and the theory and numerical analysis of both slip irreversibilities and low energy configuration defect structures. This fundamental understanding is necessary to study persistent slip bands in FCC metals and alloys including the appropriate characterization, theory, and modeling. From this fundamental knowledge both micromechanical and crystal plasticity models can be used to predict crack initiation, which are also reviewed. Finally, this review ends with a discussion of the future of fatigue modeling and experiments.'),\n",
       " (6, 'date', '2013-00-00 2013'),\n",
       " (6, 'accessDate', '2019-10-01'),\n",
       " (6, 'volume', '57'),\n",
       " (6, 'pages', '58-72'),\n",
       " (6, 'publicationTitle', 'International Journal of Fatigue'),\n",
       " (6, 'DOI', '10.1016/j.ijfatigue.2012.10.009'),\n",
       " (6, 'ISSN', '01421123'),\n",
       " (7, 'title', 'PDF'),\n",
       " (8, 'title', 'On the role of point defects in fatigue crack initiation'),\n",
       " (8,\n",
       "  'abstractNote',\n",
       "  'Analysis of inhomogeneous cyclic straining, observation of surface relief at emerging persistent slip bands (PSBs) and data on point defect generation and migration in fatigued metal single cyrstals and polycrystals are the origin of a new model for fatigue crack initiation in PSBs. Because of the high plastic strain amplitude, non-equilibrium point defects are generated in PSBs. They are distributed inhomogeneously according to the inhomogeneous sink distribution. Point defect migration over short distances causes mass transport and results in the formation of extrusions and intrusions on the metal surface. The distribution of extrusions and intrusions is related to the dislocation substructure of the PSB. Cyclic straining under stress together with the strain concentration at sharp intrusions result in environment-assisted formation of new surfaces and therefore in crack initiation.'),\n",
       " (8, 'date', '1987-00-00 1987'),\n",
       " (8, 'accessDate', '2019-10-01'),\n",
       " (8, 'volume', '92'),\n",
       " (8, 'pages', '71-80'),\n",
       " (8, 'publicationTitle', 'Materials Science and Engineering'),\n",
       " (8, 'DOI', '10.1016/0025-5416(87)90157-1'),\n",
       " (8, 'ISSN', '00255416'),\n",
       " (9, 'title', 'PDF'),\n",
       " (10, 'title', 'Fatigue crack initiation - The role of point defects'),\n",
       " (10,\n",
       "  'abstractNote',\n",
       "  'The role of point defects in the formation of surface relief and in the initiation of a fatigue crack in crystalline materials is analyzed. The dislocation interactions in the bands of intensive cyclic slip (persistent slip bands - PSBs) are specified and relations describing the formation and annihilation of interstitial and vacancy type defects in the channels of the ladder-like PSB are derived. The continuous formation, annihilation and primarily the migration of point defects are proposed to be responsible for the mass redistribution within PSB and between PSB and the PSB/matrix boundary. The redistribution of the matter results in local tensile and compressive stresses that are the sources of the principal irreversibility of slip within PSB. Local tensile and compressive stresses are relaxed by dislocation movement within PSB in the direction of the active Burgers vector and lead to the formation of characteristic surface relief in the form of extrusions and intrusions. The intrusions represent crack-like defects and fatigue cracks initiate in the tip of intrusions. © 2013 Elsevier Ltd. All rights reserved.'),\n",
       " (10, 'date', '2014-00-00 2014'),\n",
       " (10, 'accessDate', '2019-10-01'),\n",
       " (10, 'volume', '65'),\n",
       " (10, 'pages', '18-27'),\n",
       " (10, 'publicationTitle', 'International Journal of Fatigue'),\n",
       " (10, 'DOI', '10.1016/j.ijfatigue.2013.10.016'),\n",
       " (10, 'ISSN', '01421123'),\n",
       " (11, 'title', 'PDF'),\n",
       " (12, 'title', 'A survey of methods for time series change point detection'),\n",
       " (12,\n",
       "  'abstractNote',\n",
       "  'Change points are abrupt variations in time series data. Such abrupt changes may represent transitions that occur between states. Detection of change points is useful in modelling and prediction of time series and is found in application areas such as medical condition monitoring, climate change detection, speech and image analysis, and human activity analysis. This survey article enumerates, categorizes, and compares many of the methods that have been proposed to detect change points in time series. The methods examined include both supervised and unsupervised algorithms that have been introduced and evaluated. We introduce several criteria to compare the algorithms. Finally, we present some grand challenges for the community to consider.'),\n",
       " (12, 'date', '2017-00-00 2017'),\n",
       " (12, 'volume', '51'),\n",
       " (12, 'pages', '339-367'),\n",
       " (12, 'publicationTitle', 'Knowledge and Information Systems'),\n",
       " (12, 'DOI', '10.1007/s10115-016-0987-z'),\n",
       " (12, 'issue', '2'),\n",
       " (12, 'ISSN', '02193116'),\n",
       " (13, 'title', 'PDF'),\n",
       " (15, 'title', 'PDF'),\n",
       " (16, 'title', 'Selective review of offline change point detection methods'),\n",
       " (16,\n",
       "  'abstractNote',\n",
       "  'This article presents a selective survey of algorithms for the offline detection of multiple change points in multivariate time series. A general yet structuring methodological strategy is adopted to organize this vast body of work. More precisely, detection algorithms considered in this review are characterized by three elements: a cost function, a search method and a constraint on the number of changes. Each of those elements is described, reviewed and discussed separately. Implementations of the main algorithms described in this article are provided within a Python package called ruptures.'),\n",
       " (16, 'date', '2018-00-00 2018'),\n",
       " (16, 'url', 'http://arxiv.org/abs/1801.00718'),\n",
       " (17, 'title', 'PDF'),\n",
       " (18,\n",
       "  'title',\n",
       "  'Recent advances on the numerical modelling of turbulent flows'),\n",
       " (18,\n",
       "  'abstractNote',\n",
       "  \"This paper reviews the problems and successes of computing turbulent flow. Most of the flow phenomena that are important to modern technology involve turbulence. The review is concerned with methods for turbulent flow computer predictions and their applications, and describes several of them. These computational methods are aimed at simulating either as much detail of the turbulent motion as possible by current computer power or, more commonly, its overall effect on the mean-flow behaviour. The methods are still being developed and some of the most recent concepts involved are discussed. Some success has been achieved with two-equation models for relatively simple hydrodynamic phenomena; indeed, routine design work has been undertaken during the last three decades in several applications of engineering practise, for which extensive studies have optimised these models. Failures are still common for many applications particularly those that involve strong curvature, intermittency, strong buoyancy influences, low-Reynolds-number effects, rapid compression or expansion, strong swirl, and kinetically-influenced chemical reaction. New conceptual developments are needed in these areas, probably along the lines of actually calculating the principal manifestation of turbulence, e.g. intermittency. A start has been made in this direction in the form of 'multi-fluid' models, and full simulations. The turbulence modelling approaches presented here are, Reynolds-Averaged Navier-Stokes (RANS), two-fluid models, Very Large Eddy Simulation (VLES), Unsteady Reynolds-Averaged Navier-Stokes (URANS), Detached Eddy Simulation (DES) and some interesting, relatively recent, hybrid LES/RANS techniques. A large number of relatively recent studies are considered, together with reference to the numerical experiments existing on the subject. The authors hope that they provide the interested reader with most of the appropriate sources of turbulence modelling, exhibiting either as much detail as it is possible, by means of bibliography, or illustrating some of the most recent developments on the numerical modelling of turbulent flows. Thus, the potential user has the appropriate information, for him to select the suitable turbulence model for his own case of interest.\"),\n",
       " (18, 'date', '2015-00-00 2015'),\n",
       " (18, 'url', 'http://dx.doi.org/10.1016/j.apm.2014.07.001'),\n",
       " (18, 'volume', '39'),\n",
       " (18, 'pages', '693-732'),\n",
       " (18, 'publicationTitle', 'Applied Mathematical Modelling'),\n",
       " (18, 'DOI', '10.1016/j.apm.2014.07.001'),\n",
       " (18, 'issue', '2'),\n",
       " (18, 'ISSN', '0307904X'),\n",
       " (19, 'title', 'PDF'),\n",
       " (20, 'title', 'How to read a research paper.'),\n",
       " (20,\n",
       "  'abstractNote',\n",
       "  'A research paper includes several sections, each section having a particular purpose and containing a particular kind of information. This paper is a guide to reading a research paper. It describes the prototypical research paper and explains the purpose for each section. Issues for the astute reader to note are indicated and illustrated with examples from a research paper published in this issue of the journal.'),\n",
       " (20, 'date', '1988-00-00 1988'),\n",
       " (20, 'url', 'http://www.ncbi.nlm.nih.gov/pubmed/19796417'),\n",
       " (20, 'volume', '42'),\n",
       " (20, 'pages', '596-600'),\n",
       " (20,\n",
       "  'publicationTitle',\n",
       "  'The American journal of occupational therapy. : official publication of the American Occupational Therapy Association'),\n",
       " (20, 'issue', '9'),\n",
       " (20, 'ISSN', '0272-9490'),\n",
       " (21, 'title', 'PDF'),\n",
       " (22, 'title', 'A non-singular continuum theory of dislocations'),\n",
       " (22,\n",
       "  'abstractNote',\n",
       "  'We develop a non-singular, self-consistent framework for computing the stress field and the total elastic energy of a general dislocation microstructure. The expressions are self-consistent in that the driving force defined as the negative derivative of the total energy with respect to the dislocation position, is equal to the force produced by stress, through the Peach-Koehler formula. The singularity intrinsic to the classical continuum theory is removed here by spreading the Burgers vector isotropically about every point on the dislocation line using a spreading function characterized by a single parameter a, the spreading radius. A particular form of the spreading function chosen here leads to simple analytic formulations for stress produced by straight dislocation segments, segment self and interaction energies, and forces on the segments. For any value a>0, the total energy and the stress remain finite everywhere, including on the dislocation lines themselves. Furthermore, the well-known singular expressions are recovered for a=0. The value of the spreading radius a can be selected for numerical convenience, to reduce the stiffness of the dislocation equations of motion. Alternatively, a can be chosen to match the atomistic and continuum energies of dislocation configurations. © 2005 Elsevier Ltd. All rights reserved.'),\n",
       " (22, 'date', '2006-00-00 2006'),\n",
       " (22, 'volume', '54'),\n",
       " (22, 'pages', '561-587'),\n",
       " (22, 'publicationTitle', 'Journal of the Mechanics and Physics of Solids'),\n",
       " (22, 'DOI', '10.1016/j.jmps.2005.09.005'),\n",
       " (22, 'issue', '3'),\n",
       " (22, 'ISSN', '00225096'),\n",
       " (23, 'title', 'PDF'),\n",
       " (24, 'title', 'New developments in Contact Problems'),\n",
       " (24,\n",
       "  'abstractNote',\n",
       "  'La profesión –formación- docente es un tema crucial en los actuales debates educativos. La existencia de dos decretos y el desplazamiento del verdadero sentido del ser maestro reclaman de los análisis un ejercicio de comprensión del orden discursivo oficial. La calidad es el sustrato de la sociedad de control. En este marco se agencia nuevas prácticas de subjetivación del maestro los cuales podríamos situar en la calidad, flexibilidad, adaptabilidad, eficiencia, eficacia. En cualquier caso, el esfuerzo por hacer del maestro un intelectual de la educación fue borrado. La gran cuestión consiste en saber que discursos regula el saber del docente a la luz de la sociedad de control.'),\n",
       " (24, 'date', '2012-00-00 2012'),\n",
       " (24, 'volume', '53'),\n",
       " (24, 'ISBN', '978-85-7811-079-6'),\n",
       " (24, 'numPages', '45-52'),\n",
       " (25, 'title', 'PDF'),\n",
       " (26, 'title', 'Reinforcement Learning: An Introduction'),\n",
       " (26,\n",
       "  'abstractNote',\n",
       "  'CiteSeerX - Scientific documents that cite the following paper: Reinforcement learning: An introduction, chapter 11'),\n",
       " (26, 'date', '1998-00-00 1998'),\n",
       " (26, 'volume', '27'),\n",
       " (26, 'pages', '1093-1096'),\n",
       " (26, 'publicationTitle', 'Kybernetes'),\n",
       " (26, 'DOI', '10.1108/k.1998.27.9.1093.3'),\n",
       " (26, 'issue', '9'),\n",
       " (26, 'ISSN', '0368492X'),\n",
       " (27, 'title', 'PDF'),\n",
       " (28,\n",
       "  'title',\n",
       "  'Research directions in computational mechanics, A Report of the United States National Committee on Theoretical and Applied Mechanics'),\n",
       " (28, 'date', '2000-00-00 2000'),\n",
       " (28, 'ISBN', '0-309-04648-3'),\n",
       " (29, 'title', 'PDF'),\n",
       " (30,\n",
       "  'title',\n",
       "  'Bayesian inverse problems for functions and applications to fluid mechanics'),\n",
       " (30,\n",
       "  'abstractNote',\n",
       "  'In this paper we establish a mathematical framework for a range of inverse problems for functions, given a finite set of noisy observations. The problems are hence underdetermined and are often ill-posed. We study these problems from the viewpoint of Bayesian statistics, with the resulting posterior probability measure being defined on a space of functions. We develop an abstract framework for such problems which facilitates application of an infinite-dimensional version of Bayes theorem, leads to a well-posedness result for the posterior measure (continuity in a suitable probability metric with respect to changes in data), and also leads to a theory for the existence of maximizing the posterior probability (MAP) estimators for such Bayesian inverse problems on function space. A central idea underlying these results is that continuity properties and bounds on the forward model guide the choice of the prior measure for the inverse problem, leading to the desired results on well-posedness and MAP estimators; the PDE analysis and probability theory required are thus clearly dileneated, allowing a straightforward derivation of results. We show that the abstract theory applies to some concrete applications of interest by studying problems arising from data assimilation in fluid mechanics. The objective is to make inference about the underlying velocity field, on the basis of either Eulerian or Lagrangian observations. We study problems without model error, in which case the inference is on the initial condition, and problems with model error in which case the inference is on the initial condition and on the driving noise process or, equivalently, on the entire time-dependent velocity field. In order to undertake a relatively uncluttered mathematical analysis we consider the two-dimensional Navier-Stokes equation on a torus. The case of Eulerian observations - direct observations of the velocity field itself - is then a model for weather forecasting. The case of Lagrangian observations - observations of passive tracers advected by the flow - is then a model for data arising in oceanography. The methodology which we describe herein may be applied to many other inverse problems in which it is of interest to find, given observations, an infinite-dimensional object, such as the initial condition for a PDE. A similar approach might be adopted, for example, to determine an appropriate mathematical setting for the inverse problem of determining an unknown tensor arising in a constitutive law for a PDE, given observations of the solution. The paper is structured so that the abstract theory can be read independently of the particular problems in fluid mechanics which are subsequently studied by application of the theory. © 2009 IOP Publishing Ltd.'),\n",
       " (30, 'date', '2009-00-00 2009'),\n",
       " (30, 'accessDate', '2019-11-28'),\n",
       " (30, 'volume', '25'),\n",
       " (30, 'publicationTitle', 'Inverse Problems'),\n",
       " (30, 'DOI', '10.1088/0266-5611/25/11/115008'),\n",
       " (30, 'issue', '11'),\n",
       " (30, 'ISSN', '02665611'),\n",
       " (31,\n",
       "  'title',\n",
       "  'Adaptivity in Bayesian Inverse Finite Element Problems: Learning and Simultaneous Control of Discretisation and Sampling Errors'),\n",
       " (31,\n",
       "  'abstractNote',\n",
       "  'The local size of computational grids used in partial differential equation (PDE)-based probabilistic inverse problems can have a tremendous impact on the numerical results. As a consequence, numerical model identification procedures used in structural or material engineering may yield erroneous, mesh-dependent result. In this work, we attempt to connect the field of adaptive methods for deterministic and forward probabilistic finite-element (FE) simulations and the field of FE-based Bayesian inference. In particular, our target setting is that of exact inference, whereby complex posterior distributions are to be sampled using advanced Markov Chain Monte Carlo (MCMC) algorithms. Our proposal is for the mesh refinement to be performed in a goal-oriented manner. We assume that we are interested in a finite subset of quantities of interest (QoI) such as a combination of latent uncertain parameters and/or quantities to be drawn from the posterior predictive distribution. Next, we evaluate the quality of an approximate inversion with respect to these quantities. This is done by running two chains in parallel: (i) the approximate chain and (ii) an enhanced chain whereby the approximate likelihood function is corrected using an efficient deterministic error estimate of the error introduced by the spatial discretisation of the PDE of interest. One particularly interesting feature of the proposed approach is that no user-defined tolerance is required for the quality of the QoIs, as opposed to the deterministic error estimation setting. This is because our trust in the model, and therefore a good measure for our requirement in terms of accuracy, is fully encoded in the prior. We merely need to ensure that the finite element approximation does not impact the posterior distributions of QoIs by a prohibitively large amount. We will also propose a technique to control the error introduced by the MCMC sampler, and demonstrate the validity of the combined mesh and algorithmic quality control strategy.'),\n",
       " (31, 'date', '2019-02-20 2019-02-20'),\n",
       " (31, 'url', 'http://www.mdpi.com/1996-1944/12/4/642'),\n",
       " (31, 'accessDate', '2019-11-28'),\n",
       " (31, 'volume', '12'),\n",
       " (31, 'pages', '642'),\n",
       " (31, 'publicationTitle', 'Materials'),\n",
       " (31, 'DOI', '10.3390/ma12040642'),\n",
       " (31, 'issue', '4'),\n",
       " (31, 'ISSN', '1996-1944'),\n",
       " (32, 'title', 'Approximation of bayesian inverse problems for PDES'),\n",
       " (32,\n",
       "  'abstractNote',\n",
       "  'Inverse problems are often ill posed, with solutions that depend sensitively on data.n any numerical approach to the solution of such problems, regularization of some form is needed to counteract the resulting instability. This paper is based on an approach to regularization, employing a Bayesian formulation of the problem, which leads to a notion of well posedness for inverse problems, at the level of probability measures. The stability which results from this well posedness may be used as the basis for quantifying the approximation, in finite dimensional spaces, of inverse problems for functions. This paper contains a theory which utilizes this stability property to estimate the distance between the true and approximate posterior distributions, in the Hellinger metric, in terms of error estimates for approximation of the underlying forward problem. This is potentially useful as it allows for the transfer of estimates from the numerical analysis of forward problems into estimates for the solution of the related inverse problem. It is noteworthy that, when the prior is a Gaussian random field model, controlling differences in the Hellinger metric leads to control on the differences between expected values of polynomially bounded functions and operators, including the mean and covariance operator. The ideas are applied to some non-Gaussian inverse problems where the goal is determination of the initial condition for the Stokes or Navier-Stokes equation from Lagrangian and Eulerian observations, respectively. © 2010 Society for Industrial and Applied Mathematics.'),\n",
       " (32, 'date', '2010-00-00 2010'),\n",
       " (32, 'accessDate', '2019-11-28'),\n",
       " (32, 'volume', '48'),\n",
       " (32, 'pages', '322-345'),\n",
       " (32, 'publicationTitle', 'SIAM Journal on Numerical Analysis'),\n",
       " (32, 'DOI', '10.1137/090770734'),\n",
       " (32, 'issue', '1'),\n",
       " (32, 'ISSN', '00361429'),\n",
       " (33, 'title', 'Inverse problems: A Bayesian perspective'),\n",
       " (33,\n",
       "  'abstractNote',\n",
       "  'The subject of inverse problems in differential equations is of enormous practical importance, and has also generated substantial mathematical and computational innovation. Typically some form of regularization is required to ameliorate ill-posed behaviour. In this article we review the Bayesian approach to regularization, developing a function space viewpoint on the subject. This approach allows for a full characterization of all possible solutions, and their relative probabilities, whilst simultaneously forcing significant modelling issues to be addressed in a clear and precise fashion. Although expensive to implement, this approach is starting to lie within the range of the available computational resources in many application areas. It also allows for the quantification of uncertainty and risk, something which is increasingly demanded by these applications. Furthermore, the approach is conceptually important for the understanding of simpler, computationally expedient approaches to inverse problems. © 2010 Cambridge University Press.'),\n",
       " (33, 'date', '2010-00-00 2010'),\n",
       " (33, 'volume', '19'),\n",
       " (33, 'pages', '451-459'),\n",
       " (33, 'publicationTitle', 'Acta Numerica'),\n",
       " (33, 'DOI', '10.1017/S0962492910000061'),\n",
       " (33, 'issue', '2010'),\n",
       " (33, 'ISSN', '09624929'),\n",
       " (34, 'title', 'PDF'),\n",
       " (35, 'title', 'Atomic Pair Distribution Function Analysis'),\n",
       " (35, 'date', '2007-00-00 2007'),\n",
       " (35, 'volume', '76'),\n",
       " (35, 'publicationTitle', 'Phys. Rev. B'),\n",
       " (36, 'title', 'PDF'),\n",
       " (37, 'title', 'Can one hear the shape of a Drum?'),\n",
       " (37, 'date', '1966-00-00 1966'),\n",
       " (37, 'volume', '73'),\n",
       " (37, 'pages', '1-23'),\n",
       " (37, 'publicationTitle', 'The American Mathematical Monthly'),\n",
       " (37, 'DOI', '10.1007/978-4-431-56021-0_7'),\n",
       " (37, 'issue', '4'),\n",
       " (37, 'ISSN', '21941017'),\n",
       " (38, 'title', 'PDF'),\n",
       " (40, 'title', 'PDF'),\n",
       " (41,\n",
       "  'title',\n",
       "  'Rock destruction effect on the stability of a drilling structure'),\n",
       " (41,\n",
       "  'abstractNote',\n",
       "  'The motion of a drilling structure is studied in torsion. The stability of the stationary solution is determined by the direct method of Liapounov, supplemented with results for the linearized method. The stability criterion is firmly based on the form of the boundary condition linked to the rock destruction process. This rock/bit interaction function can be deduced using studies on rock mechanics, based on yield design formalism. Assuming a quasi-static axial evolution, numerical simulations illustrate the instability of the stationary solution: the bit motion can converge on a limit cycle, often called stick-slip. The beam therefore evolves as a complex cone-shaped limit surface. A simple two-degrees-of-freedom system is now considered in both axial and torsional directions, to quantify the quasi-static axial assumption. The instability of the stationary solution is confirmed by the linearized method for the undamped system with the postulated boundary conditions. Even for small damping values the same result is achieved. Even though a limit cycle appears in the axial plane (small amplitude), stick-slip can be described adequately by considering a quasi-static axial evolution.'),\n",
       " (41, 'date', '2000-00-00 2000'),\n",
       " (41, 'volume', '233'),\n",
       " (41, 'pages', '235-254'),\n",
       " (41, 'publicationTitle', 'Journal of Sound and Vibration'),\n",
       " (41, 'DOI', '10.1006/jsvi.1999.2811'),\n",
       " (41, 'issue', '2'),\n",
       " (41, 'ISSN', '0022460X'),\n",
       " (42, 'title', 'PDF'),\n",
       " (43, 'title', 'The effects of nuclear radiation on materials'),\n",
       " (43, 'date', '1959-00-00 1959'),\n",
       " (43, 'volume', '6'),\n",
       " (43, 'pages', '333-336'),\n",
       " (43, 'publicationTitle', 'Journal of Electronics and Control'),\n",
       " (43, 'DOI', '10.1080/00207215908937159'),\n",
       " (43, 'issue', '4'),\n",
       " (43, 'ISSN', '03681947'),\n",
       " (44, 'title', 'PDF'),\n",
       " (45, 'title', 'Definitions and examples of inverse and ill-posed problems'),\n",
       " (45,\n",
       "  'abstractNote',\n",
       "  'The terms \"inverse problems\" and \"ill-posed problems\" have been steadily and surely gaining popularity in modern science since the middle of the 20th century. A little more than fifty years of studying problems of this kind have shown that a great number of problems from various branches of classical mathematics (computational algebra, differential and integral equations, partial differential equations, functional analysis) can be classified as inverse or ill-posed, and they are among the most complicated ones (since they are unstable and usually nonlinear). At the same time, inverse and ill-posed problems began to be studied and applied systematically in physics, geophysics, medicine, astronomy, and all other areas of knowledge where mathematical methods are used. The reason is that solutions to inverse problems describe important properties of media under study, such as density and velocity of wave propagation, elasticity parameters, conductivity, dielectric permittivity and magnetic permeability, and properties and location of inhomogeneities in inaccessible areas, etc. In this paper we consider definitions and classification of inverse and ill-posed problems and describe some approaches which have been proposed by outstanding Russian mathematicians A. N. Tikhonov, V. K. Ivanov and M. M. Lavrentiev. © de Gruyter 2008.'),\n",
       " (45, 'date', '2008-00-00 2008'),\n",
       " (45, 'volume', '16'),\n",
       " (45, 'pages', '317-357'),\n",
       " (45, 'publicationTitle', 'Journal of Inverse and Ill-Posed Problems'),\n",
       " (45, 'DOI', '10.1515/JIIP.2008.019'),\n",
       " (45, 'issue', '4'),\n",
       " (45, 'ISSN', '09280219'),\n",
       " (46, 'title', 'PDF'),\n",
       " (47,\n",
       "  'title',\n",
       "  'The rise of the X-ray atomic pair distribution function method: A series of fortunate events'),\n",
       " (47,\n",
       "  'abstractNote',\n",
       "  \"The atomic pair distribution function (PDF) technique is a powerful approach to gain quantitative insight into the structure of materials where the structural coherence extends only over a few nanometres. In this paper, I focus on PDF from synchrotron X-rays and describe what is the PDF and where it came from, as well as key moments on the journey that have contributed to its enormous recent growth and expanding impact in materials science today. Synchrotron X-ray sources played a starring role in this story. This article is part of the theme issue 'Fifty years of synchrotron science: Achievements and opportunities'.\"),\n",
       " (47, 'date', '2019-00-00 2019'),\n",
       " (47, 'volume', '377'),\n",
       " (47,\n",
       "  'publicationTitle',\n",
       "  'Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences'),\n",
       " (47, 'DOI', '10.1098/rsta.018.0413'),\n",
       " (47, 'issue', '2147'),\n",
       " (47, 'ISSN', '1364503X'),\n",
       " (48, 'title', 'PDF'),\n",
       " (49, 'title', 'Functional Analysis'),\n",
       " (49, 'date', '2020-00-00 2020'),\n",
       " (49, 'place', 'New York, NY'),\n",
       " (49, 'publisher', 'Springer New York'),\n",
       " (49, 'ISBN', '1-4020-0667-5'),\n",
       " (49, 'pages', '1-7'),\n",
       " (50, 'title', 'PDF'),\n",
       " (56, 'title', 'Dynamic Auctions'),\n",
       " (56,\n",
       "  'abstractNote',\n",
       "  'A dynamic trading game is examined in which two uninformed buyers engage in Bertrand-like competition to attempt to purchase a single object of uncertain quality from an informed seller. It is shown that there exists a unique perfect sequential equilibrium. The game is compared to an analogous bargaining game in which a single uninformed buyer makes offers to a single seller. Despite the fact that in the equilibrium of the competitive game, buyers compete away their surplus, it is shown that sellers can often gain a higher ex ante surplus in the bargaining game. © 1990 The Review of Economic Studies Limited.'),\n",
       " (56, 'date', '1990-00-00 1990'),\n",
       " (56, 'volume', '57'),\n",
       " (56, 'pages', '49'),\n",
       " (56, 'publicationTitle', 'The Review of Economic Studies'),\n",
       " (56, 'DOI', '10.2307/2297542'),\n",
       " (56, 'issue', '1'),\n",
       " (56, 'ISSN', '00346527'),\n",
       " (57, 'title', 'PDF'),\n",
       " (58, 'title', 'The speed of evolution'),\n",
       " (58,\n",
       "  'abstractNote',\n",
       "  'In this paper we study the mixing times of a Wright-Fisher model for an asexual population. Typically, such models consist of a population of several genotypes where each genotype reproduces at a different rate, genotypes can mutate to each other, and the population is subject to the evolutionary pressure of selection. While such models have been used extensively to study different types of populations, recently, such stochastic finite population models have been used to model viral populations with the goal of understanding the effect of mutagenic drugs. Here, the time it takes for the population to reach a steady state is important both for carrying out simulations and to determine treatment strength and duration. Despite their importance, and having been widely studied, there has been a lack of such bounds for many relevant ranges of model parameters even for the case of two genotypes (single locus); primarily due to their difficulty, see [Ethll]. The main result of this paper is an analytical bound on the mixing time of a Wright-Fisher model for two genotypes when there is no restriction on the mutation rate or the fitness. Our bound explains (in this setting) the observed phenomena that the mixing time is fast (logarithmic in the state space) for any fitness parameter when the mutation rate is high enough. Theoretically, we overcome the difficulty in proving mixing time bounds by showing that different forces are responsible for the rapid mixing depending on how close it is to its steady state. If the chain is sufficiently close to its stationary distribution, an intricate coupling argument establishes rapid mixing. To show that the chain reaches this state quickly, we connect it to the convergence time of a deterministic model for evolution proposed by Eigen. We are hopeful that our insights and techniques will be helpful in establishing rigorous bounds for more general versions of this model.'),\n",
       " (58, 'date', '2015-00-00 2015'),\n",
       " (58, 'volume', '2015-Janua'),\n",
       " (58, 'pages', '1590-1601'),\n",
       " (58,\n",
       "  'proceedingsTitle',\n",
       "  'Proceedings of the Annual ACM-SIAM Symposium on Discrete Algorithms'),\n",
       " (58, 'DOI', '10.1137/1.9781611973730.105'),\n",
       " (59, 'title', 'PDF'),\n",
       " (61,\n",
       "  'title',\n",
       "  'A finite population model of molecular evolution: Theory and computation'),\n",
       " (61,\n",
       "  'abstractNote',\n",
       "  'This article is concerned with the evolution of haploid organisms that reproduce asexually. In a seminal piece of work, Eigen and coauthors proposed the quasispecies model in an attempt to understand such an evolutionary process. Their work has impacted antiviral treatment and vaccine design strategies. Yet, predictions of the quasispecies model are at best viewed as a guideline, primarily because it assumes an infinite population size, whereas realistic population sizes can be quite small. In this paper we consider a population genetics-based model aimed at understanding the evolution of such organisms with finite population sizes and present a rigorous study of the convergence and computational issues that arise therein. Our first result is structural and shows that, at any time during the evolution, as the population size tends to infinity, the distribution of genomes predicted by our model converges to that predicted by the quasispecies model. This justifies the continued use of the quasispecies model to derive guidelines for intervention. While the stationary state in the quasispecies model is readily obtained, due to the explosion of the state space in our model, exact computations are prohibitive. Our second set of results are computational in nature and address this issue. We derive conditions on the parameters of evolution under which our stochastic model mixes rapidly. Further, for a class of widely used fitness landscapes we give a fast deterministic algorithm which computes the stationary distribution of our model. These computational tools are expected to serve as a framework for the modeling of strategies for the deployment of mutagenic drugs. © Copyright 2012, Mary Ann Liebert, Inc. 2012.'),\n",
       " (61, 'date', '2012-00-00 2012'),\n",
       " (61, 'volume', '19'),\n",
       " (61, 'pages', '1176-1202'),\n",
       " (61, 'publicationTitle', 'Journal of Computational Biology'),\n",
       " (61, 'DOI', '10.1089/cmb.2012.0064'),\n",
       " (61, 'issue', '10'),\n",
       " (61, 'ISSN', '10665277'),\n",
       " (62, 'title', 'PDF'),\n",
       " (91, 'title', 'Bidding Dynamics in Auctions'),\n",
       " (91,\n",
       "  'abstractNote',\n",
       "  'This paper studies bidding dynamics where values and bidding opportunities follow an unrestricted joint Markov process, independent across agents. Bids cannot be retracted, as is frequently the case in auctions. Our main methodological contribution is that we construct a mapping from this general stochastic process into a distribution of values that is independent of the type of auction considered. The equilibria of a static auction with this distribution of values is used to characterize the equilibria of the dynamic auction, making this general class very tractable. As a result of the option of future rebidding, early bids are shaded and under mild conditions increase toward the end of the auction. Our results are consistent with repeated bidding and skewness of the time distribution of winning bids, two puzzling observations in dynamic auctions. As an application, we estimate the model by matching moments from eBay auctions.'),\n",
       " (91, 'date', '2016-00-00 2016'),\n",
       " (92, 'title', 'PDF'),\n",
       " (98,\n",
       "  'title',\n",
       "  'Games and Economic Behavior Budget-constrained sequential auctions with incomplete information'),\n",
       " (98, 'date', '2009-00-00 2009'),\n",
       " (98, 'url', 'http://dx.doi.org/10.1016/j.geb.2008.10.001'),\n",
       " (98, 'volume', '66'),\n",
       " (98, 'pages', '928-949'),\n",
       " (98, 'publicationTitle', 'Games and Economic Behavior'),\n",
       " (98, 'DOI', '10.1016/j.geb.2008.10.001'),\n",
       " (98, 'issue', '2'),\n",
       " (98, 'ISSN', '0899-8256'),\n",
       " (99, 'title', 'PDF'),\n",
       " (102, 'title', 'Draft Auctions'),\n",
       " (102, 'date', '2018-00-00 2018'),\n",
       " (103, 'title', 'PDF'),\n",
       " (105, 'title', 'Statistical Inference in Inverse Problems'),\n",
       " (105, 'date', '2012-00-00 2012'),\n",
       " (105, 'volume', '3'),\n",
       " (105, 'ISBN', '1984081920091'),\n",
       " (105, 'numPages', '1-47'),\n",
       " (106, 'title', 'PDF'),\n",
       " (107,\n",
       "  'title',\n",
       "  'Mixing time of Markov chains, dynamical systems and evolution'),\n",
       " (107,\n",
       "  'abstractNote',\n",
       "  'In this paper we study the mixing time of evolutionary Markov chains over populations of a fixed size (N) in which each individual can be one of m types. These Markov chains have the property that they are guided by a dynamical system from the m-dimensional probability simplex to itself. Roughly, given the current state of the Markov chain, which can be viewed as a probability distribution over the m types, the next state is generated by applying this dynamical system to this distribution, and then sampling from it N times. Many processes in nature, from biology to sociology, are evolutionary and such chains can be used to model them. In this study, the mixing time is of particular interest as it determines the speed of evolution and whether the statistics of the steady state can be efficiently computed. In a recent result [Panageas, Srivastava, Vishnoi, Soda, 2016], it was suggested that the mixing time of such Markov chains is connected to the geometry of this guiding dynamical system. In particular, when the dynamical system has a fixed point which is a global attractor, then the mixing is fast. The limit sets of dynamical systems, however, can exhibit more complex behavior: they could have multiple fixed points that are not necessarily stable, periodic orbits, or even chaos. Such behavior arises in important evolutionary settings such as the dynamics of sexual evolution and that of grammar acquisition. In this paper we prove that the geometry of the dynamical system can also give tight mixing time bounds when the dynamical system has multiple fixed points and periodic orbits. We show that the mixing time continues to remain small in the presence of several unstable fixed points and is exponential in N when there are two or more stable fixed points. As a consequence of our results, we obtain a phase transition result for the mixing time of the sexual/grammar model mentioned above. We arrive at the conclusion that in the interesting parameter regime for these models, i.e., when there are multiple stable fixed points, the mixing is slow. Our techniques strengthen the connections between Markov chains and dynamical systems and we expect that the tools developed in this paper should have a wider applicability.'),\n",
       " (107, 'date', '2016-00-00 2016'),\n",
       " (107, 'volume', '55'),\n",
       " (107, 'ISBN', '978-3-95977-013-2'),\n",
       " (107, 'pages', '1-14'),\n",
       " (107,\n",
       "  'proceedingsTitle',\n",
       "  'Leibniz International Proceedings in Informatics, LIPIcs'),\n",
       " (107, 'DOI', '10.4230/LIPIcs.ICALP.2016.63'),\n",
       " (108, 'title', 'PDF'),\n",
       " (133, 'title', 'Inverse Problems and Applications'),\n",
       " (133, 'date', '1390-00-00 1390'),\n",
       " (133, 'publisher', 'Springer'),\n",
       " (133, 'ISBN', '978-3-319-12498-8'),\n",
       " (133, 'numPages', '368'),\n",
       " (134, 'title', 'PDF'),\n",
       " (135, 'title', 'The Calculus Of Variations And Functional Analysis'),\n",
       " (135, 'date', '2003-00-00 2003'),\n",
       " (135, 'url', 'http://www.worldscientific.com/worldscibooks/10.1142/5374'),\n",
       " (135, 'volume', '12'),\n",
       " (135, 'ISBN', '978-981-238-581-9'),\n",
       " (135, 'numPages', '1-420'),\n",
       " (136, 'title', 'PDF'),\n",
       " (138, 'title', 'PDF'),\n",
       " (139, 'title', 'Making evolution rigorous: The error threshold'),\n",
       " (139, 'date', '2013-00-00 2013'),\n",
       " (139, 'ISBN', '978-1-4503-1859-4'),\n",
       " (139, 'pages', '59-60'),\n",
       " (139,\n",
       "  'proceedingsTitle',\n",
       "  'ITCS 2013 - Proceedings of the 2013 ACM Conference on Innovations in Theoretical Computer Science'),\n",
       " (139, 'DOI', '10.1145/2422436.2422445'),\n",
       " (140, 'title', 'PDF'),\n",
       " (141,\n",
       "  'title',\n",
       "  'Auctions with interdependent valuations: theoretical and empirical analysis, in particular of internet auctions'),\n",
       " (141,\n",
       "  'abstractNote',\n",
       "  'The thesis investigates a number of auction formats both theoretically and empirically. The effect of different auction rules on the final price and on bidder valuations is analysed. Results from an experimental sale of real goods, testing revenue equivalence of the open and sealed- bid second-price auction do not conform to theoretical predictions: the open auction leading to significantly lower prices than the sealed-bid auction. It turns out that the open auction format allows bidders to satisfy a tendency to “stick together” with their valuations. The empirical results motivate a dynamic bidding model of interdependent valuations, bidders being uncertain about their valuations and learning from the exit-prices of their rivals. Furthermore, bidding behaviour on the Internet is investigated in the hard close and the automatically extended auction. Late bidding is shown to be a rational strategy in the hard close auction, but not in the automatically extended auction. Theoretical results show that the expected final price and seller revenue is lower in the hard close auction than in the automatically extended auction, where prestige-concerns can lead to an explosive final price. Moreover, Yahoo auction data confirms the strong presence of late bidding in the hard-close auction and the seller’s preference for the automatically extended auction.'),\n",
       " (141, 'date', '2003-00-00 2003'),\n",
       " (142, 'title', 'PDF'),\n",
       " (143,\n",
       "  'title',\n",
       "  \"Molecular Quasl-Speciest The formulation of a tractable chemical model based on Darwin's principle may be understood in several steps: 1. The major constituents of the system have to be inherently self-reproductive. Only two classes of molecules are prese\"),\n",
       " (143,\n",
       "  'abstractNote',\n",
       "  \"The molecular quasi-species model describes the physicochemical organization of monomers into an ensemble of heteropolymers with combinatorial complexity by ongoing template polymerization. Polynucleotides belong to the simplest class of such molecules. The quasi-species itself represents the stationary distribution of macromolecular sequences maintained by chemical reactions effecting error-prone replication and by transport processes. It is obtained deterministically, by mass-action kinetics, as the dominant eigenvalue of a value matrix, W, which is derived directly from chemical rate coefficients, but it also exhibits stochastic features, being composed to a significant fraction of unique individual macromolecular sequences. The quasi-species model demonstrates how macromolecular information originates through specific nonequilibrium autocatalytic reactions and thus forms a bridge between reaction kinetics and molecular evolution. Selection and evolutionary optimization appear as new features in physical chemistry. Concentration bias in the production of mutants is a new concept in population genetics, relevant to frequently mutating populations, which is shown to greatly enhance the optimization properties. The present theory relates to asexually replicating ensembles, but this restriction is not essential. A sharp transition is exhibited between a drifting population of essentially random macromolecular sequences and a localized population of close relatives. This transition at a threshold error rate was found to depend on sequence lengths, distributions of selective values, and population sizes. It has been determined generically for complex landscapes and for special cases, and, it was shown to persist genetically in the presence of nearly neutral mutants. Replication dynamics has much in common with the equilibrium statistics of complex spin systems: the error threshold is equivalent to a magnetic order-disorder transition. A rational function of the replication accuracy plays the role of temperature. Experimental data obtained from test-tube evolution of polynucleotides and from studies of natural virus populations support the quasi-species model. The error threshold seems to set a limit to the genome lengths of several classes of RNA viruses. In addition, the results are relevant even in eucaryotes where they contribute to the exon-intron debate. 1. Molecular Selection Our knowledge of physical and chemical systems is, in a final analysis, based on models derived from repeatable experiments. While none of the classic and rather besieged list of properties rounded up to support the intuition of a distinction between the living and nonliving-metabolism, self-reproduction, irritability, and adaptability, for example-intrinsically limit the application of the scientific method, a determining role by unique or individual entities comes into conflict with the requirement of repeatability. Combinatorial variety, such as that in heteropolymers based on even very small numbers of different bases, even just two, readily provides numbers of different entities so enormous that neither consecutive nor parallel physical realization is possible. The physical chemistry of finite systems of such macromolecules must deal with both known regularities and the advent of unique co-polymeric sequences. Normally this would present no difficulty in a statistical mechanical analysis of typical behavior, where rare events play no significant role, but with autocatalytic polymeri-zation processes even unique single molecules may be amplified to determine the fate of the entire system. Potentially creative, self-organizing around unique events, the dynamics of this simplest living chemical system is invested with regularities that both allow and limit efficient adaptation. The quasi-species model is a study of these regularities. The fundamental regularity in living organisms that has invited explanation is adaptation. Why are organisms so well fitted to their environments? At a more chemical level, why are enzymes fThis is an abridged account of the quasi-species theory that has been submitted in comprehensive form to Advances in Chemical Physics.' 0022-3654/88/2092-6881 $01.50/0 optimal catalysts? Darwin's theory of natural selection has provided biologists with a framework for the answer to this question. The present model is constructed along Darwinian lines but in terms of specific macromolecules, chemical reactions, and physical processes that make the notion of survival of the fittest precise. Not only does the model give an understanding of the physical limitations of adaptation, but also it provides new insight into the role of chance in the process. For an understanding of the structure of this minimal chemical model it is first necessary to recall the conceptual basis of Darwin's theory. Darwin recognized that new inheritable adaptive properties were not induced by the environment but arose independently in the production of offspring. Lasting adaptive changes in a population could only come about by natural selection of the heritable traits or genotype based on the full characteristics or phenotype relevant for producing offspring. A process of chance, i.e., uncorrelated with the developed phenotype, controls changes in the genotype from one generation to the next and generates the diversity necessary for selection. Three factors have probably prevented chemists from gaining a clear insight into these phenomena in the past, despite the discovery of the polymeric nature of the genotype (DNA): the complexity of a minimum replication phenotype, the problem of dealing with a huge number of variants, and the nonequilibrium nature of these ongoing processes.\"),\n",
       " (143, 'date', '1988-00-00 1988'),\n",
       " (143, 'url', 'https://pubs.acs.org/sharingguidelines'),\n",
       " (143, 'accessDate', '2020-06-05'),\n",
       " (143, 'pages', '36'),\n",
       " (143, 'institution', 'UTC'),\n",
       " (144, 'title', 'PDF'),\n",
       " (145, 'title', 'Evolutionary dynamics in finite populations mix rapidly'),\n",
       " (145,\n",
       "  'abstractNote',\n",
       "  'In this paper we prove that the mixing time of a broad class of evolutionary dynamics in finite, unstructured populations is roughly logarithmic in the size of the state space. An important special case of such a stochastic process is the Wright-Fisher model from evolutionary biology (with selection and mutation) on a population of size N over m genotypes. Our main result implies that the mixing time of this process is O(log N) for all mutation rates and fitness landscapes, and solves the main open problem from [4].In particular, it significantly extends the main result in [18] who proved this for m = 2. Biologically, such models have been used to study the evolution of viral populations with applications to drug design strategies countering them. Here the time it takes for the population to reach a steady state is important both for the estimation of the steady-state structure of the population as well in the modeling of the treatment strength and duration. Our result, that such populations exhibit rapid mixing, makes both of these approaches sound. Technically, we make a novel connection between Markov chains arising in evolutionary dynamics and dynamical systems on the probability simplex. This allows us to use the local and global stability properties of the fixed points of such dynamical systems to construct a contractive coupling in a fairly general setting. We expect that our mixing time result would be useful beyond the evolutionary biology setting, and the techniques used here would find applications in bounding the mixing times of Markov chains which have a natural underlying dynamical system.'),\n",
       " (145, 'date', '2016-00-00 2016'),\n",
       " (145, 'accessDate', '2020-06-05'),\n",
       " (145, 'volume', '1'),\n",
       " (145, 'publisher', 'Association for Computing Machinery'),\n",
       " (145, 'ISBN', '978-1-5108-1967-2'),\n",
       " (145, 'pages', '480-497'),\n",
       " (145,\n",
       "  'proceedingsTitle',\n",
       "  'Proceedings of the Annual ACM-SIAM Symposium on Discrete Algorithms'),\n",
       " (145, 'DOI', '10.1137/1.9781611974331.ch36'),\n",
       " (146, 'title', 'PDF'),\n",
       " (147, 'title', 'The markov chain monte carlo revolution'),\n",
       " (147,\n",
       "  'abstractNote',\n",
       "  'The use of simulation for high-dimensional intractable computations has revolutionized applied mathematics. Designing, improving and understanding the new tools leads to (and leans on) fascinating mathematics, from representation theory through micro-local analysis. © 2008 American Mathematical Society.'),\n",
       " (147, 'date', '2009-00-00 2009'),\n",
       " (147, 'pages', '179-205'),\n",
       " (148, 'title', 'PDF'),\n",
       " (149, 'title', 'Handbook of uncertainty quantification'),\n",
       " (149,\n",
       "  'abstractNote',\n",
       "  'The topic of Uncertainty Quantification (UQ) has witnessed massive developments in response to the promise of achieving risk mitigation through scientific prediction. It has led to the integration of ideas from mathematics, statistics and engineering being used to lend credence to predictive assessments of risk but also to design actions (by engineers, scientists and investors) that are consistent with risk aversion. The objective of this Handbook is to facilitate the dissemination of the forefront of UQ ideas to their audiences. We recognize that these audiences are varied, with interests ranging from theory to application, and from research to development and even execution.'),\n",
       " (149, 'date', '2017-00-00 2017'),\n",
       " (149, 'ISBN', '978-3-319-12385-1'),\n",
       " (149, 'numPages', '1-2053'),\n",
       " (151, 'title', 'PDF'),\n",
       " (255, 'title', 'Functional analysis'),\n",
       " (255,\n",
       "  'abstractNote',\n",
       "  'This chapter focuses on the theory of functional analysis. First, the Euler equation was obtained using techniques of dubious validity. Then the linearity of the equation was exploited to establish the existence of a solution of this equation and then to demonstrate its uniqueness. Finally, the quadratic nature of the functional was used to show that the function so obtained provided an absolute minimum. The chapter discusses the fundamental concepts of Hilbert space and then applies these ideas to the basic variational problem. Following this, the analytic aspects of solving the Euler equation and some of the computational problems are described in the chapter. © 1967, Academic Press, Inc.'),\n",
       " (255, 'date', '1967-00-00 1967'),\n",
       " (255, 'volume', '40'),\n",
       " (255, 'ISBN', '978-3-540-58654-8'),\n",
       " (255, 'numPages', '217-240'),\n",
       " (256, 'title', 'PDF'),\n",
       " (270, 'title', 'Uncertainty Quantification and Bayesian Inversion'),\n",
       " (270,\n",
       "  'abstractNote',\n",
       "  'Probabilistic thinking is of growing importance in many areas of mathemat-ics. This paper highlights the beautiful mathematical framework, coupled with practical algorithms, which results from thinking probabilistically about inverse problems arising in partial differential equations. Many inverse problems in the physical sciences require the determination of an un-known field from a finite set of indirect measurements. Examples include oceanography, oil recovery, water resource management and weather forecasting. In the Bayesian ap-proach to these problems, the unknown and the data are modelled as a jointly varying random variable, typically linked through solution of a partial differential equation, and the solution of the inverse problem is the distribution of the unknown given the data. This approach provides a natural way to provide estimates of the unknown field, to-gether with a quantification of the uncertainty associated with the estimate. It is hence a useful practical modelling tool. However it also provides a very elegant mathematical framework for inverse problems: whilst the classical approach to inverse problems leads to ill-posedness, the Bayesian approach leads to a natural well-posedness and stability theory. Furthermore this framework provides a way of deriving and developing algo-rithms which are well-suited to the formidable computational challenges which arise from the conjunction of approximations arising from the numerical analysis of partial differen-tial equations, together with approximations of central limit theorem type arising from sampling of measures.'),\n",
       " (270, 'date', '2017-00-00 2017'),\n",
       " (270, 'pages', '1-51'),\n",
       " (270,\n",
       "  'publicationTitle',\n",
       "  'Encyclopedia of Computational Mechanics Second Edition'),\n",
       " (270, 'DOI', '10.1002/9781119176817.ecm2071'),\n",
       " (271, 'title', 'PDF'),\n",
       " (273, 'title', 'Introduction to banach spaces: Analysis and probability'),\n",
       " (273,\n",
       "  'abstractNote',\n",
       "  'This two-volume text provides a complete overview of the theory of Banach spaces, emphasising its interplay with classical and harmonic analysis (particularly Sidon sets) and probability. The authors give a full exposition of all results, as well as numerous exercises and comments to complement the text and aid graduate students in functional analysis. The book will also be an invaluable reference volume for researchers in analysis. Volume 1 covers the basics of Banach space theory, operatory theory in Banach spaces, harmonic analysis and probability. The authors also provide an annex devoted to compact Abelian groups. Volume 2 focuses on applications of the tools presented in the first volume, including Dvoretzky’s theorem, spaces without the approximation property, Gaussian processes, and more. In volume 2, four leading experts also provide surveys outlining major developments in the field since the publication of the original French edition.'),\n",
       " (273, 'date', '2017-00-00 2017'),\n",
       " (273, 'volume', '1'),\n",
       " (273, 'ISBN', '978-1-316-67576-2'),\n",
       " (273, 'numPages', '1-431'),\n",
       " (274, 'title', 'PDF'),\n",
       " (275,\n",
       "  'title',\n",
       "  'The Lax-Milgram Theorem. A detailed proof to be formalized in Coq'),\n",
       " (275,\n",
       "  'abstractNote',\n",
       "  'To guarantee the correction of numerical simulation programs implementing the finite element method, it is necessary to formalize the mathematical notions and results that allow to establish the soundness of the method. The Lax-Milgram theorem is one of those theoretical cornerstones: under some completeness and coercivity assumptions, it states existence and uniqueness of the solution to the weak formulation of boundary value problems. The purpose of this document is to provide the formal proof community with a very detailed pen-and-paper proof of the Lax-Milgram theorem.'),\n",
       " (275, 'date', '2016-00-00 2016'),\n",
       " (275, 'url', 'http://arxiv.org/abs/1607.03618'),\n",
       " (275, 'issue', 'July'),\n",
       " (276, 'title', 'PDF'),\n",
       " (277,\n",
       "  'title',\n",
       "  'Optimal feedback control, linear first-order PDE systems, and obstacle problems'),\n",
       " (277,\n",
       "  'abstractNote',\n",
       "  'We introduce an alternative approach for the analysis and numerical approximation of the optimal feedback control mapping. It consists in looking at a typical optimal control problem in such a way that feasible controls are mappings depending both in time and space. In this way, the feedback form of the problem is built-in from the very beginning. Optimality conditions are derived for one such optimal mapping, which by construction is the optimal feedback mapping of the problem. In formulating optimality conditions, costates in feedback form are solutions of linear, first-order transport systems, while optimal descent directions are solutions of appropriate obstacle problems. We treat situations with no constraint-sets for control and state, as well as the more general case where a constraint-set is considered for the control variable.'),\n",
       " (277, 'date', '2017-00-00 2017'),\n",
       " (277, 'url', 'http://dx.doi.org/10.1016/j.jfranklin.2017.02.023'),\n",
       " (277, 'volume', '354'),\n",
       " (277, 'pages', '3225-3236'),\n",
       " (277, 'publicationTitle', 'Journal of the Franklin Institute'),\n",
       " (277, 'DOI', '10.1016/j.jfranklin.2017.02.023'),\n",
       " (277, 'issue', '8'),\n",
       " (277, 'ISSN', '00160032'),\n",
       " (278, 'title', 'PDF'),\n",
       " (279, 'title', 'A TEX Mathematical Symbols'),\n",
       " (280, 'title', 'PDF'),\n",
       " (281, 'title', 'Real Analysis'),\n",
       " (282, 'title', 'PDF'),\n",
       " (291, 'title', 'PDF'),\n",
       " (292, 'title', 'Modern regularization methods for inverse problems'),\n",
       " (292,\n",
       "  'abstractNote',\n",
       "  'Regularization methods are a key tool in the solution of inverse problems. They are used to introduce prior knowledge and allow a robust approximation of ill-posed (pseudo-) inverses. In the last two decades interest has shifted from linear to nonlinear regularization methods, even for linear inverse problems. The aim of this paper is to provide a reasonably comprehensive overview of this shift towards modern nonlinear regularization methods, including their analysis, applications and issues for future research. In particular we will discuss variational methods and techniques derived from them, since they have attracted much recent interest and link to other fields, such as image processing and compressed sensing. We further point to developments related to statistical inverse problems, multiscale decompositions and learning theory.'),\n",
       " (292, 'date', '2018-05-01 2018-05-01'),\n",
       " (292, 'volume', '27'),\n",
       " (292, 'pages', '1-111'),\n",
       " (292, 'publicationTitle', 'Acta Numerica'),\n",
       " (292, 'DOI', '10.1017/S0962492918000016'),\n",
       " (292, 'ISSN', '14740508'),\n",
       " (295, 'title', 'Linear convergence of iterative soft-thresholding'),\n",
       " (295,\n",
       "  'abstractNote',\n",
       "  'In this article a unified approach to iterative soft-thresholding algorithms for the solution of linear operator equations in infinite dimensional Hilbert spaces is presented. We formulate the algorithm in the framework of generalized gradient methods and present a new convergence analysis. As main result we show that the algorithm converges with linear rate as soon as the underlying operator satisfies the so-called finite basis injectivity property or the minimizer possesses a so-called strict sparsity pattern. Moreover it is shown that the constants can be calculated explicitly in special cases (i.e. for compact operators). Furthermore, the techniques also can be used to establish linear convergence for related methods such as the iterative thresholding algorithm for joint sparsity and the accelerated gradient projection method. © 2008 Birkhäuser Boston.'),\n",
       " (295, 'date', '2008-00-00 2008'),\n",
       " (295, 'volume', '14'),\n",
       " (295, 'pages', '813-837'),\n",
       " (295, 'publicationTitle', 'Journal of Fourier Analysis and Applications'),\n",
       " (295, 'DOI', '10.1007/s00041-008-9041-1'),\n",
       " (295, 'issue', '5-6'),\n",
       " (295, 'ISSN', '10695869'),\n",
       " (296, 'title', 'PDF'),\n",
       " (297, 'title', 'Multiplier and gradient methods'),\n",
       " (297,\n",
       "  'abstractNote',\n",
       "  \"The main purpose of this paper is to suggest a method for finding the minimum of a function f(x) subject to the constraint g(x)=0. The method consists of replacing f by F=f+λg+1/2 cg2, where c is a suitably large constant, and computing the appropriate value of the Lagrange multiplier. Only the simplest algorithm is presented. The remaining part of the paper is devoted to a survey of known methods for finding unconstrained minima, with special emphasis on the various gradient techniques that are available. This includes Newton's method and the method of conjugate gradients. © 1969 Plenum Publishing Corporation.\"),\n",
       " (297, 'date', '1969-00-00 1969'),\n",
       " (297, 'volume', '4'),\n",
       " (297, 'pages', '303-320'),\n",
       " (297, 'publicationTitle', 'Journal of Optimization Theory and Applications'),\n",
       " (297, 'DOI', '10.1007/BF00927673'),\n",
       " (297, 'issue', '5'),\n",
       " (297, 'ISSN', '00223239'),\n",
       " (298, 'title', 'PDF'),\n",
       " (301,\n",
       "  'title',\n",
       "  'A dual approach to solving nonlinear programming problems by unconstrained optimization'),\n",
       " (301,\n",
       "  'abstractNote',\n",
       "  'Several recent algorithms for solving nonlinear programming problems with equality constraints have made use of an augmented \"penalty\" Lagrangian function, where terms involving squares of the constraint functions are added to the ordinary Lagrangian. In this paper, the corresponding penalty Lagrangian for problems with inequality constraints is described, and its relationship with the theory of duality is examined. In the convex case, the modified dual problem consists of maximizing a differentiable concave function (indirectly defined) subject to no constraints at all. It is shown that any maximizing sequence for the dual can be made to yield, in a general way, an asymptotically minimizing sequence for the primal which typically converges at least as rapidly. © 1973 The Mathematical Programming Society.'),\n",
       " (301, 'date', '1973-00-00 1973'),\n",
       " (301, 'volume', '5'),\n",
       " (301, 'pages', '354-373'),\n",
       " (301, 'publicationTitle', 'Mathematical Programming'),\n",
       " (301, 'DOI', '10.1007/BF01580138'),\n",
       " (301, 'issue', '1'),\n",
       " (301, 'ISSN', '00255610'),\n",
       " (302,\n",
       "  'title',\n",
       "  'Augmented Lagrangian Methods: Applications to the Numerical Solution of Boundary-Value Problems'),\n",
       " (302,\n",
       "  'abstractNote',\n",
       "  'The purpose of this volume is to present the principles of the Augmented Lagrangian Method, together with numerous applications of this method to the numerical solution of boundary-value problems for partial differential equations or inequalities arising in Mathematical Physics, in the Mechanics of Continuous Media and in the Engineering Sciences.'),\n",
       " (302, 'date', '2000-00-00 2000'),\n",
       " (302, 'url', 'https://books.google.com/books?id=s6_5EeBjQnkC&pgis=1'),\n",
       " (302, 'ISBN', '0-08-087536-X'),\n",
       " (302, 'numPages', '339'),\n",
       " (303, 'title', 'PDF'),\n",
       " (304,\n",
       "  'title',\n",
       "  'Iterative Shrinkage / Thresholding Algorithms : Some History and Recent Development'),\n",
       " (304,\n",
       "  'abstractNote',\n",
       "  'ppt slides on ISTA algos: 4 derivations on IST algos (as Expectation Minimization (Nowak), Majorization-Minimization (Daubechies), Forward-Backward Splitting (Bruck, Combettes, Wasj), Separable Approximation(Wright, Nowak)), convergence results, TwIST (two step IST) and SpaRSA, warm start and continuation'),\n",
       " (304, 'date', '2009-00-00 2009'),\n",
       " (304, 'proceedingsTitle', 'CS Workshop'),\n",
       " (305, 'title', 'PDF'),\n",
       " (306, 'title', 'The Crandall-Liggett generation theorem'),\n",
       " (306, 'date', '2015-00-00 2015'),\n",
       " (306,\n",
       "  'url',\n",
       "  'papers3://publication/uuid/401BF174-A473-4D6A-B5A1-CCAF305D2341'),\n",
       " (306, 'pages', '1-9'),\n",
       " (306, 'issue', 'October'),\n",
       " (307, 'title', 'PDF'),\n",
       " (308, 'title', 'Learning theory of randomized Kaczmarz algorithm'),\n",
       " (308,\n",
       "  'abstractNote',\n",
       "  'A relaxed randomized Kaczmarz algorithm is investigated in a least squares regression setting by a learning theory approach. When the sampling values are accurate and the regression function (conditional means) is linear, such an algorithm has been well studied in the community of non-uniform sampling. In this paper, we are mainly interested in the different case of either noisy random measurements or a nonlinear regression function. In this case, we show that relaxation is needed. A necessary and sufficient condition on the sequence of relaxation parameters or step sizes for the convergence of the algorithm in expectation is presented. Moreover, polynomial rates of convergence, both in expectation and in probability, are provided explicitly. As a result, the almost sure convergence of the algorithm is proved by applying the Borel-Cantelli Lemma.'),\n",
       " (308, 'date', '2015-00-00 2015'),\n",
       " (308, 'volume', '16'),\n",
       " (308, 'pages', '3341-3365'),\n",
       " (308, 'publicationTitle', 'Journal of Machine Learning Research'),\n",
       " (308, 'ISSN', '15337928'),\n",
       " (309, 'title', 'PDF'),\n",
       " (310, 'title', 'Open problem: Efficient online sparse regression'),\n",
       " (310,\n",
       "  'abstractNote',\n",
       "  'In practical scenarios, it is often necessary to be able to make predictions with very limited access to the features of any example. We provide one natural formulation as an online sparse regression problem with squared loss, and ask whether it is possible to achieve sublinear regret with efficient algorithms (i.e. polynomial running time in the natural parameters of the problem).'),\n",
       " (310, 'date', '2014-00-00 2014'),\n",
       " (310, 'volume', '35'),\n",
       " (310, 'pages', '1299-1301'),\n",
       " (310, 'publicationTitle', 'Journal of Machine Learning Research'),\n",
       " (310, 'ISSN', '15337928'),\n",
       " (311, 'title', 'PDF'),\n",
       " (312, 'title', 'On the Exponential Convergence of the Kaczmarz Algorithm'),\n",
       " (312,\n",
       "  'abstractNote',\n",
       "  'The Kaczmarz algorithm (KA) is a popular method for solving a system of linear equations. In this note we derive a new exponential convergence result for the KA. The key allowing us to establish the new result is to rewrite the KA in such a way that its solution path can be interpreted as the output from a particular dynamical system. The asymptotic stability results of the corresponding dynamical system can then be leveraged to prove exponential convergence of the KA. The new bound is also compared to existing bounds.'),\n",
       " (312, 'date', '2015-00-00 2015'),\n",
       " (312, 'volume', '22'),\n",
       " (312, 'pages', '1571-1574'),\n",
       " (312, 'publicationTitle', 'IEEE Signal Processing Letters'),\n",
       " (312, 'DOI', '10.1109/LSP.2015.2412253'),\n",
       " (312, 'issue', '10'),\n",
       " (312, 'ISSN', '10709908'),\n",
       " (313, 'title', 'PDF'),\n",
       " (316, 'title', 'PDF'),\n",
       " (322, 'title', 'Successive projections on hyperplanes'),\n",
       " (322,\n",
       "  'abstractNote',\n",
       "  'Any sequence of points in Rn obtained by successive projections of a point on elements of a finite set of hyperplanes is bounded. © 1984.'),\n",
       " (322, 'date', '1984-00-00 1984'),\n",
       " (322, 'volume', '103'),\n",
       " (322, 'pages', '134-138'),\n",
       " (322,\n",
       "  'publicationTitle',\n",
       "  'Journal of Mathematical Analysis and Applications'),\n",
       " (322, 'DOI', '10.1016/0022-247X(84)90163-X'),\n",
       " (322, 'issue', '1'),\n",
       " (322, 'ISSN', '10960813'),\n",
       " (325,\n",
       "  'title',\n",
       "  'Optimal k-thresholding algorithms for sparse optimization problems'),\n",
       " (325,\n",
       "  'abstractNote',\n",
       "  'The simulations indicate that the existing hard thresholding technique independent of the residual function may cause a dramatic increase or numerical oscillation of the residual. This inherent drawback of the hard thresholding renders the traditional thresholding algorithms unstable and thus generally inefficient for solving practical sparse optimization problems. How to overcome this weakness and develop a truly efficient thresholding method is a fundamental question in this field. The aim of this paper is to address this question by proposing a new thresholding technique based on the notion of optimal k-thresholding. The central idea for this new development is to connect the k-thresholding directly to the residual reduction during the course of algorithms. This leads to a natural design principle for the efficient thresholding methods. Under the restricted isometry property, we prove that the optimal thresholding based algorithms are globally convergent to the solution of sparse optimization problems. The numerical experiments demonstrate that when solving sparse optimization problems, the traditional hard thresholding methods have been significantly transcended by the proposed algorithms which can even outperform the classic `1-minimization method in many situations.'),\n",
       " (325, 'date', '2020-01-02 2020-01-02'),\n",
       " (325, 'url', 'https://epubs.siam.org/doi/10.1137/18M1219187'),\n",
       " (325, 'volume', '30'),\n",
       " (325, 'pages', '31-55'),\n",
       " (325, 'publicationTitle', 'SIAM Journal on Optimization'),\n",
       " (325, 'DOI', '10.1137/18M1219187'),\n",
       " (325, 'issue', '1'),\n",
       " (325, 'ISSN', '10526234'),\n",
       " (326, 'title', 'PDF'),\n",
       " (327,\n",
       "  'title',\n",
       "  'Randomized projection methods for linear systems with arbitrarily large sparse corruptions'),\n",
       " (327,\n",
       "  'abstractNote',\n",
       "  'In applications like medical imaging, error correction, and sensor networks, one needs to solve large-scale linear systems that may be corrupted by a small number of arbitrarily large corruptions. We consider solving such large-scale systems of linear equations Ax = b that are inconsistent due to corruptions in the measurement vector b. With this as our motivating example, we develop an approach for this setting that allows detection of the corrupted entries and thus convergence to the \"\"true\"\" solution of the original system. We provide analytical justification for our approaches as well as experimental evidence on real and synthetic systems.'),\n",
       " (327, 'date', '2019-00-00 2019'),\n",
       " (327, 'volume', '41'),\n",
       " (327, 'pages', 'S19-S36'),\n",
       " (327, 'publicationTitle', 'SIAM Journal on Scientific Computing'),\n",
       " (327, 'DOI', '10.1137/18M1179213'),\n",
       " (327, 'issue', '5'),\n",
       " (327, 'ISSN', '10957197'),\n",
       " (328, 'title', 'PDF'),\n",
       " (329,\n",
       "  'title',\n",
       "  'Stochastic reformulations of linear systems: Algorithms and convergence theory'),\n",
       " (329,\n",
       "  'abstractNote',\n",
       "  'We develop a family of reformulations of an arbitrary consistent linear system into a stochastic problem. The reformulations are governed by two user-defined parameters: a positive definite matrix defining a norm, and an arbitrary discrete or continuous distribution over random matrices. Our reformulation has several equivalent interpretations, allowing for researchers from various communities to leverage their domain specific insights. In particular, our reformulation can be equivalently seen as a stochastic optimization problem, stochastic linear system, stochastic fixed point problem and a probabilistic intersection problem. We prove sufficient, and necessary and sufficient conditions for the reformulation to be exact. Further, we propose and analyze three stochastic algorithms for solving the reformulated problem—basic, parallel and accelerated methods—with global linear convergence rates. The rates can be interpreted as condition numbers of a matrix which depends on the system matrix and on the reformulation parameters. This gives rise to a new phenomenon which we call stochastic preconditioning, and which refers to the problem of finding parameters (matrix and distribution) leading to a sufficiently small condition number. Our basic method can be equivalently interpreted as stochastic gradient descent, stochastic Newton method, stochastic proximal point method, stochastic fixed point method, and stochastic projection method, with fixed stepsize (relaxation parameter), applied to the reformulations.'),\n",
       " (329, 'date', '2017-00-00 2017'),\n",
       " (329, 'volume', '41'),\n",
       " (329, 'pages', '365-388'),\n",
       " (329, 'publicationTitle', 'arXiv'),\n",
       " (329, 'issue', '2'),\n",
       " (330, 'title', 'PDF'),\n",
       " (331,\n",
       "  'title',\n",
       "  'Rows versus Columns: Randomized Kaczmarz or Gauss--Seidel for Ridge Regression'),\n",
       " (331,\n",
       "  'abstractNote',\n",
       "  'The Kaczmarz and Gauss-Seidel methods aim to solve a linear $m \\\\times n$ system $\\\\boldsymbol{X} \\\\boldsymbol{\\\\beta} = \\\\boldsymbol{y}$ by iteratively refining the solution estimate; the former uses random rows of $\\\\boldsymbol{X}$ {to update $\\\\boldsymbol{\\\\beta}$ given the corresponding equations} and the latter uses random columns of $\\\\boldsymbol{X}$ {to update corresponding coordinates in $\\\\boldsymbol{\\\\beta}$}. Interest in these methods was recently revitalized by a proof of Strohmer and Vershynin showing linear convergence in expectation for a \\\\textit{randomized} Kaczmarz method variant (RK), and a similar result for the randomized Gauss-Seidel algorithm (RGS) was later proved by Lewis and Leventhal. Recent work unified the analysis of these algorithms for the overcomplete and undercomplete systems, showing convergence to the ordinary least squares (OLS) solution and the minimum Euclidean norm solution respectively. This paper considers the natural follow-up to the OLS problem, ridge regression, which solves $(\\\\boldsymbol{X}^* \\\\boldsymbol{X} + \\\\lambda \\\\boldsymbol{I}) \\\\boldsymbol{\\\\beta} = \\\\boldsymbol{X}^* \\\\boldsymbol{y}$. We present particular variants of RK and RGS for solving this system and derive their convergence rates. We compare these to a recent proposal by Ivanov and Zhdanov to solve this system, that can be interpreted as randomly sampling both rows and columns, which we argue is often suboptimal. Instead, we claim that one should always use RGS (columns) when $m > n$ and RK (rows) when $m < n$. This difference in behavior is simply related to the minimum eigenvalue of two related positive semidefinite matrices, $\\\\boldsymbol{X}^* \\\\boldsymbol{X} + \\\\lambda \\\\boldsymbol{I}_n$ and $\\\\boldsymbol{X} \\\\boldsymbol{X}^* + \\\\lambda \\\\boldsymbol{I}_m$ when $m > n$ or $m < n$.'),\n",
       " (331, 'date', '2017-00-00 2017'),\n",
       " (331, 'volume', '39'),\n",
       " (331, 'pages', 'S528-S542'),\n",
       " (331, 'publicationTitle', 'SIAM Journal on Scientific Computing'),\n",
       " (331, 'DOI', '10.1137/16m1077891'),\n",
       " (331, 'issue', '5'),\n",
       " (331, 'ISSN', '1064-8275'),\n",
       " (332, 'title', 'PDF'),\n",
       " (333,\n",
       "  'title',\n",
       "  'Book Review: A mathematical introduction to compressive sensing'),\n",
       " (333, 'date', '2016-00-00 2016'),\n",
       " (333, 'volume', '54'),\n",
       " (333, 'pages', '151-165'),\n",
       " (333, 'publicationTitle', 'Bulletin of the American Mathematical Society'),\n",
       " (333, 'DOI', '10.1090/bull/1546'),\n",
       " (333, 'issue', '1'),\n",
       " (333, 'ISSN', '0273-0979'),\n",
       " (334, 'title', 'PDF'),\n",
       " (335,\n",
       "  'title',\n",
       "  'A Sampling Kaczmarz--Motzkin Algorithm for Linear Feasibility'),\n",
       " (335,\n",
       "  'abstractNote',\n",
       "  'We combine two iterative algorithms for solving large-scale systems of linear inequalities, the relaxation method of Agmon, Motzkin et al. and the randomized Kaczmarz method. In doing so, we obtain a family of algorithms that generalize and extend both techniques. We prove several convergence results, and our computational experiments show our algorithms often outperform the original methods.'),\n",
       " (335, 'date', '2017-00-00 2017'),\n",
       " (335, 'volume', '39'),\n",
       " (335, 'pages', 'S66-S87'),\n",
       " (335, 'publicationTitle', 'SIAM Journal on Scientific Computing'),\n",
       " (335, 'DOI', '10.1137/16m1073807'),\n",
       " (335, 'issue', '5'),\n",
       " (335, 'ISSN', '1064-8275'),\n",
       " (337, 'title', 'A smoothing method for sparse optimization over convex sets'),\n",
       " (337,\n",
       "  'abstractNote',\n",
       "  'In this paper, we investigate a class of heuristic schemes to solve the NP-hard problem of minimizing ℓ-norm over a convex set. A well-known approximation is to consider the convex problem of minimizing ℓ1-norm. We are interested in finding improved results in cases where the problem in ℓ1-norm does not provide an optimal solution to the ℓ-norm problem. We consider a relaxation technique using a family of smooth concave functions depending on a parameter. Some other relaxations have already been tried in the literature and the aim of this paper is to provide a more general context. This motivation allows deriving new theoretical results that are valid for general constraint set. We use a homotopy algorithm, starting from a solution to the problem in ℓ1-norm and ending in a solution of the problem in ℓ-norm. The new results are existence of the solutions of the subproblem, convergence of the scheme, a monotonicity of the solutions and an exact penalization theorem independent of the data.'),\n",
       " (337, 'date', '2020-00-00 2020'),\n",
       " (337, 'url', 'https://doi.org/10.1007/s11590-019-01408-x'),\n",
       " (337, 'volume', '14'),\n",
       " (337, 'pages', '1053-1069'),\n",
       " (337, 'publicationTitle', 'Optimization Letters'),\n",
       " (337, 'DOI', '10.1007/s11590-019-01408-x'),\n",
       " (337, 'issue', '5'),\n",
       " (337, 'ISSN', '18624480'),\n",
       " (338, 'title', 'PDF'),\n",
       " (339, 'title', 'On Motzkin’s method for inconsistent linear systems'),\n",
       " (339,\n",
       "  'abstractNote',\n",
       "  'Iterative linear solvers have gained recent popularity due to their computational efficiency and low memory footprint for large-scale linear systems. The relaxation method, or Motzkin’s method, can be viewed as an iterative method that projects the current estimation onto the solution hyperplane corresponding to the most violated constraint. Although this leads to an optimal selection strategy for consistent systems, for inconsistent least square problems, the strategy presents a tradeoff between convergence rate and solution accuracy. We provide a theoretical analysis that shows Motzkin’s method offers an initially accelerated convergence rate and this acceleration depends on the dynamic range of the residual. We quantify this acceleration for Gaussian systems as a concrete example. Lastly, we include experimental evidence on real and synthetic systems that support the analysis.'),\n",
       " (339, 'date', '2019-00-00 2019'),\n",
       " (339, 'url', 'https://doi.org/10.1007/s10543-018-0737-6'),\n",
       " (339, 'volume', '59'),\n",
       " (339, 'pages', '387-401'),\n",
       " (339, 'publicationTitle', 'BIT Numerical Mathematics'),\n",
       " (339, 'DOI', '10.1007/s10543-018-0737-6'),\n",
       " (339, 'issue', '2'),\n",
       " (339, 'ISSN', '15729125'),\n",
       " (340, 'title', 'PDF'),\n",
       " (341, 'title', 'Enhancing sparsity by reweightedℓ1 minimization'),\n",
       " (341,\n",
       "  'abstractNote',\n",
       "  'It is now well understood that (1) it is possible to reconstruct sparse signals exactly from what appear to be highly incomplete sets of linear measurements and (2) that this can be done by constrained ℓ1 minimization. In this paper, we study a novel method for sparse signal recovery that in many situations outperforms ℓ1 minimization in the sense that substantially fewer measurements are needed for exact recovery. The algorithm consists of solving a sequence of weighted ℓ1- minimization problems where the weights used for the next iteration are computed from the value of the current solution. We present a series of experiments demonstrating the remarkable performance and broad applicability of this algorithm in the areas of sparse signal recovery, statistical estimation, error correction and image processing. Interestingly, superior gains are also achieved when our method is applied to recover signals with assumed near-sparsity in overcomplete representations-not by reweighting theℓ1 norm of the coefficient sequence as is common, but by reweighting the ℓ1 norm of the transformed object. An immediate consequence is the possibility of highly efficient data acquisition protocols by improving on a technique known as Compressive Sensing. © 2008 Birkhäuser Boston.'),\n",
       " (341, 'date', '2008-00-00 2008'),\n",
       " (341, 'volume', '14'),\n",
       " (341, 'pages', '877-905'),\n",
       " (341, 'publicationTitle', 'Journal of Fourier Analysis and Applications'),\n",
       " (341, 'DOI', '10.1007/s00041-008-9045-x'),\n",
       " (341, 'issue', '5-6'),\n",
       " (341, 'ISSN', '10695869'),\n",
       " (342, 'title', 'PDF'),\n",
       " (343, 'title', 'Novel min-max reformulations of linear inverse problems'),\n",
       " (343,\n",
       "  'abstractNote',\n",
       "  'In this article we dwell into the class of so called ill-posed Linear Inverse Problems (LIP) which simply refers to the task of recovering the entire signal from its relatively few random linear measurements. Such problems arise in variety of settings with applications ranging from medical image processing, recommender systems, etc. We propose a slightly generalised version of the error constrained linear inverse problem and obtain a novel and equivalent convex-concave min-max reformulation by providing an exposition to its convex geometry. Saddle points of the min-max problem are completely characterised in terms of a solution to the LIP, and vice versa. Applying simple saddle point seeking ascend-descent type algorithms to solve the min-max problems provides novel and simple algorithms to find a solution to the LIP. Moreover, reformulation of an LIP as the min-max problem provided in this article is crucial in developing methods to solve the dictionary learning problem with almost sure recovery constraints.'),\n",
       " (343, 'date', '2020-00-00 2020'),\n",
       " (343, 'publicationTitle', 'arXiv'),\n",
       " (344, 'title', 'PDF'),\n",
       " (345,\n",
       "  'title',\n",
       "  'The Kaczmarz algorithm, row action methods,                    and statistical learning algorithms'),\n",
       " (345, 'date', '2018-00-00 2018'),\n",
       " (345, 'volume', '706'),\n",
       " (345, 'pages', '115-127'),\n",
       " (345, 'DOI', '10.1090/conm/706/14216'),\n",
       " (345, 'ISSN', '10983627'),\n",
       " (346, 'title', 'PDF'),\n",
       " (347,\n",
       "  'title',\n",
       "  'A randomized kaczmarz algorithm with exponential convergence'),\n",
       " (347,\n",
       "  'abstractNote',\n",
       "  'The Kaczmarz method for solving linear systems of equations is an iterative algorithm that has found many applications ranging from computer tomography to digital signal processing. Despite the popularity of this method, useful theoretical estimates for its rate of convergence are still scarce. We introduce a randomized version of the Kaczmarz method for consistent, overdetermined linear systems and we prove that it converges with expected exponential rate. Furthermore, this is the first solver whose rate does not depend on the number of equations in the system. The solver does not even need to know the whole system but only a small random part of it. It thus outperforms all previously known methods on general extremely overdetermined systems. Even for moderately overdetermined systems, numerical simulations as well as theoretical analysis reveal that our algorithm can converge faster than the celebrated conjugate gradient algorithm. Furthermore, our theory and numerical simulations confirm a prediction of Feichtinger et al. in the context of reconstructing bandlimited functions from nonuniform sampling. © 2008 Birkhäuser Boston.'),\n",
       " (347, 'date', '2009-00-00 2009'),\n",
       " (347, 'volume', '15'),\n",
       " (347, 'pages', '262-278'),\n",
       " (347, 'publicationTitle', 'Journal of Fourier Analysis and Applications'),\n",
       " (347, 'DOI', '10.1007/s00041-008-9030-4'),\n",
       " (347, 'issue', '2'),\n",
       " (347, 'ISSN', '10695869'),\n",
       " (348, 'title', 'PDF'),\n",
       " (353, 'title', 'Faster Randomized Block Kaczmarz Algorithms ∗'),\n",
       " (353,\n",
       "  'abstractNote',\n",
       "  'The Kaczmarz algorithm is a simple iterative scheme for solving consistent linear systems. At each step, the method projects the current iterate onto the solution space of a single constraint. Hence, it requires very low cost per iteration and storage, and it has a linear rate of convergence. Distributed implementations of Kaczmarz have become, in recent years, the de facto architectural choice for large-scale linear systems. Therefore, in this paper we develop a family of randomized block Kaczmarz algorithms that uses at each step a subset of the constraints and extrapolated stepsizes, and can be deployed on distributed computing units. Our approach is based on several new ideas and tools, including stochastic selection rule for the blocks of rows, stochastic conditioning of the linear system, and novel strategies for designing extrapolated stepsizes. We prove that randomized block Kaczmarz algorithm converges linearly in expectation, with a rate depending on the geometric properties of the matrix and its submatrices and on the size of the blocks. Our convergence analysis reveals that the algorithm is most effective when it is given a good sampling of the rows into well-conditioned blocks. Besides providing a general framework for the design and analysis of randomized block Kaczmarz methods, our results resolve an open problem in the literature related to the theoretical understanding of observed practical efficiency of extrapolated block Kaczmarz methods.'),\n",
       " (353, 'date', '2019-00-00 2019'),\n",
       " (353, 'volume', '18'),\n",
       " (353, 'pages', '1046-1060'),\n",
       " (353, 'publicationTitle', 'arXiv'),\n",
       " (353, 'issue', '3'),\n",
       " (354, 'title', 'PDF'),\n",
       " (357, 'title', 'PDF'),\n",
       " (359, 'title', 'The randomized Kaczmarz method with mismatched adjoint'),\n",
       " (359,\n",
       "  'abstractNote',\n",
       "  'This paper investigates the randomized version of the Kaczmarz method to solve linear systems in the case where the adjoint of the system matrix is not exact—a situation we refer to as “mismatched adjoint”. We show that the method may still converge both in the over- and underdetermined consistent case under appropriate conditions, and we calculate the expected asymptotic rate of linear convergence. Moreover, we analyze the inconsistent case and obtain results for the method with mismatched adjoint as for the standard method. Finally, we derive a method to compute optimized probabilities for the choice of the rows and illustrate our findings with numerical examples.'),\n",
       " (359, 'date', '2018-00-00 2018'),\n",
       " (359, 'url', 'https://doi.org/10.1007/s10543-018-0717-x'),\n",
       " (359, 'volume', '58'),\n",
       " (359, 'pages', '1079-1098'),\n",
       " (359, 'publicationTitle', 'BIT Numerical Mathematics'),\n",
       " (359, 'DOI', '10.1007/s10543-018-0717-x'),\n",
       " (359, 'issue', '4'),\n",
       " (359, 'ISSN', '15729125'),\n",
       " (360, 'title', 'PDF'),\n",
       " (361, 'title', 'Atomic decomposition by basis pursuit'),\n",
       " (361,\n",
       "  'abstractNote',\n",
       "  'The time-frequency and time-scale communities have recently developed a large number of overcomplete waveform dictionaries-stationary wavelets, wavelet packets, cosine packets, chirplets, and warplets, to name a few. Decomposition into overcomplete systems is not unique, and several methods for decomposition have been proposed, including the method of frames (MOF), matching pursuit (MP), and, for special dictionaries, the best orthogonal basis (BOB). Basis pursuit (BP) is a principle for decomposing a signal into an \"optimal\" superposition of dictionary elements, where optimal means having the smallest l1 norm of coefficients among all such decompositions. We give examples exhibiting several advantages over MOF, MP, and BOB, including better sparsity and superresolution. BP has interesting relations to ideas in areas as diverse as ill-posed problems, abstract harmonic analysis, total variation denoising, and multiscale edge denoising. BP in highly overcomplete dictionaries leads to large-scale optimization problems. With signals of length 8192 and a wavelet packet dictionary, one gets an equivalent linear program of size 8192 by 212,992. Such problems can be attacked successfully only because of recent advances in linear and quadratic programming by interior-point methods. We obtain reasonable success with a primal-dual logarithmic barrier method and conjugate-gradient solver.'),\n",
       " (361, 'date', '2001-00-00 2001'),\n",
       " (361, 'volume', '43'),\n",
       " (361, 'pages', '129-159'),\n",
       " (361, 'publicationTitle', 'SIAM Review'),\n",
       " (361, 'DOI', '10.1137/S003614450037906X'),\n",
       " (361, 'issue', '1'),\n",
       " (361, 'ISSN', '00361445'),\n",
       " (362, 'title', 'PDF'),\n",
       " (363,\n",
       "  'title',\n",
       "  'The linearized bregman method via split feasibility problems: Analysis and generalizations'),\n",
       " (363,\n",
       "  'abstractNote',\n",
       "  'The linearized Bregman method is a method to calculate sparse solutions to systems of linear equations. We formulate this problem as a split feasibility problem, propose an algorithmic framework based on Bregman projections, and prove a general convergence result for this framework. Convergence of the linearized Bregman method will be obtained as a special case. Our approach also allows for several generalizations such as other objective functions, incremental iterations, incorporation of non-Gaussian noise models, and box constraints. © 2014 Society for Industrial and Applied Mathematics.'),\n",
       " (363, 'date', '2014-00-00 2014'),\n",
       " (363, 'volume', '7'),\n",
       " (363, 'pages', '1237-1262'),\n",
       " (363, 'publicationTitle', 'SIAM Journal on Imaging Sciences'),\n",
       " (363, 'DOI', '10.1137/130936269'),\n",
       " (363, 'issue', '2'),\n",
       " (363, 'ISSN', '19364954'),\n",
       " (364, 'title', 'PDF'),\n",
       " (365,\n",
       "  'title',\n",
       "  'Sparse Recovery beyond Compressed Sensing: Separable Nonlinear Inverse Problems'),\n",
       " (365,\n",
       "  'abstractNote',\n",
       "  'Extracting information from nonlinear measurements is a fundamental challenge in data analysis. In this work, we consider separable inverse problems, where the data are modeled as a linear combination of functions that depend nonlinearly on certain parameters of interest. These parameters may represent neuronal activity in a human brain, frequencies of electromagnetic waves, fluorescent probes in a cell, or magnetic relaxation times of biological tissues. Separable nonlinear inverse problems can be reformulated as underdetermined sparse-recovery problems, and solved using convex programming. This approach has had empirical success in a variety of domains, from geophysics to medical imaging, but lacks a theoretical justification. In particular, compressed-sensing theory does not apply, because the measurement operators are deterministic and violate incoherence conditions such as the restricted-isometry property. Our main contribution is a theory for sparse recovery adapted to deterministic settings. We show that convex programming succeeds in recovering the parameters of interest, as long as their values are sufficiently distinct with respect to the correlation structure of the measurement operator. The theoretical results are illustrated through numerical experiments for two applications: heat-source localization and estimation of brain activity from electroencephalography data.'),\n",
       " (365, 'date', '2020-00-00 2020'),\n",
       " (365, 'volume', '66'),\n",
       " (365, 'pages', '5904-5926'),\n",
       " (365, 'publicationTitle', 'IEEE Transactions on Information Theory'),\n",
       " (365, 'DOI', '10.1109/TIT.2020.2985015'),\n",
       " (365, 'issue', '9'),\n",
       " (365, 'ISSN', '15579654'),\n",
       " (366, 'title', 'PDF'),\n",
       " (367, 'title', 'An Overview on Algorithms for Sparse Recovery'),\n",
       " (368, 'title', 'PDF'),\n",
       " (369,\n",
       "  'title',\n",
       "  'Maximum Hands-Off Control: A Paradigm of Control Effort Minimization'),\n",
       " (369,\n",
       "  'abstractNote',\n",
       "  'In this paper, we propose a paradigm of control, called a maximum hands-off control. A hands-off control is defined as a control that has a short support per unit time. The maximum hands-off control is the minimum support (or sparsest) per unit time among all controls that achieve control objectives. For finite horizon continuous-time control, we show the equivalence between the maximum hands-off control and L1-optimal control under a uniqueness assumption called normality. This result rationalizes the use of L1 optimality in computing a maximum hands-off control. The same result is obtained for discrete-time hands-off control. We also propose an L1/L2-optimal control to obtain a smooth hands-off control. Furthermore, we give a self-triggered feedback control algorithm for linear time-invariant systems, which achieves a given sparsity rate and practical stability in the case of plant disturbances. An example is included to illustrate the effectiveness of the proposed control.'),\n",
       " (369, 'date', '2016-00-00 2016'),\n",
       " (369, 'volume', '61'),\n",
       " (369, 'pages', '735-747'),\n",
       " (369, 'publicationTitle', 'IEEE Transactions on Automatic Control'),\n",
       " (369, 'DOI', '10.1109/TAC.2015.2452831'),\n",
       " (369, 'issue', '3'),\n",
       " (369, 'ISSN', '00189286'),\n",
       " (370, 'title', 'PDF'),\n",
       " (371, 'title', 'Sparse approximate solutions to linear systems'),\n",
       " (371,\n",
       "  'abstractNote',\n",
       "  'The following problem is considered: given a matrix A in Rm×n, (m rows and n columns), a vector b in Rm, and ε > 0, compute a vector x satisfying ∥Ax - b∥2 ≤ ε if such exists, such that x has the fewest number of non-zero entries over all such vectors. It is shown that the problem is NP-hard, but that the well-known greedy heuristic is good in that it computes a solution with at most [18 Opt(ε/2)∥A+∥22 ln(∥b∥2/ε)] non-zero entries, where Opt(ε/2) is the optimum number of nonzero entries at error ε/2, A is the matrix obtained by normalizing each column of A with respect to the L2 norm, and A+ is its pseudo-inverse.'),\n",
       " (371, 'date', '1995-00-00 1995'),\n",
       " (371, 'volume', '24'),\n",
       " (371, 'pages', '227-234'),\n",
       " (371, 'publicationTitle', 'SIAM Journal on Computing'),\n",
       " (371, 'DOI', '10.1137/S0097539792240406'),\n",
       " (371, 'issue', '2'),\n",
       " (371, 'ISSN', '00975397'),\n",
       " (372, 'title', 'PDF'),\n",
       " (374,\n",
       "  'title',\n",
       "  'Almost Sure Convergence of the Kaczmarz Algorithm with Random Measurements'),\n",
       " (374,\n",
       "  'abstractNote',\n",
       "  'The Kaczmarz algorithm is an iterative method for reconstructing a signal x∈ℝd from an overcomplete collection of linear measurements yn = 〈x,φn〉, n ≥ 1. We prove quantitative bounds on the rate of almost sure exponential convergence in the Kaczmarz algorithm for suitable classes of random measurement vectors {φn}n=1∞ ⊂ ℝd. Refined convergence results are given for the special case when each φn has i.i.d. Gaussian entries and, more generally, when each φn/{double pipe}φn{double pipe} is uniformly distributed on Sd-1. This work on almost sure convergence complements the mean squared error analysis of Strohmer and Vershynin for randomized versions of the Kaczmarz algorithm. © 2012 Springer Science+Business Media, LLC.'),\n",
       " (374, 'date', '2012-00-00 2012'),\n",
       " (374, 'volume', '18'),\n",
       " (374, 'pages', '1195-1214'),\n",
       " (374, 'publicationTitle', 'Journal of Fourier Analysis and Applications'),\n",
       " (374, 'DOI', '10.1007/s00041-012-9237-2'),\n",
       " (374, 'issue', '6'),\n",
       " (374, 'ISSN', '10695869'),\n",
       " (375, 'title', 'PDF'),\n",
       " (376, 'title', 'Row-action methods for compressed sensing'),\n",
       " (376,\n",
       "  'abstractNote',\n",
       "  'Compressed Sensing uses a small number of random, linear measurements to acquire a sparse signal. Nonlinear algorithms, such as ℓ1 minimization, are used to reconstruct the signal from the measured data. This paper proposes row-action methods as a computational approach to solving the ℓ1 optimization problem. This paper presents a specific row-action method and provides extensive empirical evidence that it is an effective technique for signal reconstruction. This approach offers several advantages over interior-point methods, including minimal storage and computational requirements, scalability, and robustness. © 2006 IEEE.'),\n",
       " (376, 'date', '2006-00-00 2006'),\n",
       " (376, 'volume', '3'),\n",
       " (376, 'pages', '868-871'),\n",
       " (376,\n",
       "  'publicationTitle',\n",
       "  'ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings'),\n",
       " (376, 'DOI', '10.1109/icassp.2006.1660792'),\n",
       " (376, 'ISSN', '15206149'),\n",
       " (377, 'title', 'PDF'),\n",
       " (378,\n",
       "  'title',\n",
       "  'Solving basis pursuit: Heuristic optimality check and solver comparison'),\n",
       " (378,\n",
       "  'abstractNote',\n",
       "  'The problem of finding a minimum ℓ1-norm solution to an underdetermined linear system is an important problem in compressed sensing, where it is also known as basis pursuit. We propose a heuristic optimality check as a general tool for ℓ1-minimization, which often allows for early termination by \"guessing\" a primaldual optimal pair based on an approximate support.Moreover,we provide an extensive numerical comparison of various state-of-the-art ℓ1-solvers that have been proposed during the last decade, on a large test set with a variety of explicitly given matrices and several right-hand sides per matrix reflecting different levels of solution difficulty. The results, as well as improvements by the proposed heuristic optimality check, are analyzed in detail to provide an answer to the question which algorithm is the best.'),\n",
       " (378, 'date', '2015-00-00 2015'),\n",
       " (378, 'volume', '41'),\n",
       " (378, 'publicationTitle', 'ACM Transactions on Mathematical Software'),\n",
       " (378, 'DOI', '10.1145/2689662'),\n",
       " (378, 'issue', '2'),\n",
       " (378, 'ISSN', '15577295'),\n",
       " (379, 'title', 'PDF'),\n",
       " (380, 'title', 'Approximate Solution for Systems of Linear Equations'),\n",
       " (380, 'date', '1937-00-00 1937'),\n",
       " (380, 'volume', '35'),\n",
       " (380, 'pages', '355-357'),\n",
       " (380,\n",
       "  'publicationTitle',\n",
       "  \"Bulletin International de l'Académie Polonaise des Sciences et des Lettres. Classe des Sciences Mathématiques et Naturelles. Série A, Sciences Mathématiques\"),\n",
       " (382, 'title', 'PDF'),\n",
       " (386,\n",
       "  'title',\n",
       "  'Accelerating the distributed Kaczmarz algorithm by strong over-relaxation'),\n",
       " (386,\n",
       "  'abstractNote',\n",
       "  'The distributed Kaczmarz algorithm is an adaptation of the standard Kaczmarz algorithm to the situation in which data is distributed throughout a network represented by a tree. We isolate substructures of the network and study convergence of the distributed Kaczmarz algorithm for relatively large relaxation parameters associated to these substructures. If the system is consistent, then the algorithm converges to the solution of minimal norm; however, if the system is inconsistent, then the algorithm converges to an approximated least-squares solution that is dependent on the parameters and the network topology. We show that the relaxation parameters may be larger than the standard upper-bound in literature in this context and provide numerical experiments to support our results.'),\n",
       " (386, 'date', '2020-00-00 2020'),\n",
       " (386, 'pages', '1-18'),\n",
       " (386, 'publicationTitle', 'Linear Algebra and Its Applications'),\n",
       " (386, 'DOI', '10.1016/j.laa.2020.10.035'),\n",
       " (386, 'ISSN', '00243795'),\n",
       " (387, 'title', 'PDF'),\n",
       " (388,\n",
       "  'title',\n",
       "  'Projection method for solving a singular system of linear equations and its applications'),\n",
       " (388,\n",
       "  'abstractNote',\n",
       "  'The iterative method for solving system of linear equations, due to Kaczmarz [2], is investigated. It is shown that the method works well for both singular and non-singular systems and it determines the affine space formed by the solutions if they exist. The method also provides an iterative procedure for computing a generalized inverse of a matrix. © 1971 Springer-Verlag.'),\n",
       " (388, 'date', '1971-00-00 1971'),\n",
       " (388, 'volume', '17'),\n",
       " (388, 'pages', '203-214'),\n",
       " (388, 'publicationTitle', 'Numerische Mathematik'),\n",
       " (388, 'DOI', '10.1007/BF01436376'),\n",
       " (388, 'issue', '3'),\n",
       " (388, 'ISSN', '0029599X'),\n",
       " (390, 'title', 'PDF'),\n",
       " (405, 'title', 'Natterer'),\n",
       " (406, 'title', 'PDF'),\n",
       " (407,\n",
       "  'title',\n",
       "  'Enhancement of the Kaczmarz algorithm with projection adjustment'),\n",
       " (407,\n",
       "  'abstractNote',\n",
       "  'Projection adjustment is a technique that improves the rate of convergence, as measured by the number of iterations needed to achieve a given level of performance, of the Kaczmarz algorithm (KA) for iteratively solving a system of consistent linear equations, however at the cost of requiring additional time per iteration and increased storage. This hinders the applicability of the previously published Kaczmarz algorithm with projection adjustment (KAPA) to large-scale problems. An enhancement EKAPA of KAPA that uses projection adjustment only for a small subset of the equations is proposed for significantly reducing the time and storage requirements. An analysis of the behavior of EKAPA is provided. An illustration is given to show that EKAPA using a small subset of the equations for projection adjustment can achieve a speed-up over KA similar to that of KAPA in terms of the number of iterations, but requires much less computer time and storage; hence, it is more suitable for large-scale problems.'),\n",
       " (407, 'date', '2020-00-00 2020'),\n",
       " (407, 'volume', '85'),\n",
       " (407, 'pages', '713-736'),\n",
       " (407, 'publicationTitle', 'Numerical Algorithms'),\n",
       " (407, 'DOI', '10.1007/s11075-019-00834-3'),\n",
       " (407, 'issue', '2'),\n",
       " (407, 'ISSN', '15729265'),\n",
       " (408, 'title', 'PDF'),\n",
       " (409,\n",
       "  'title',\n",
       "  'A convex integer programming approach for optimal sparse PCA'),\n",
       " (409,\n",
       "  'abstractNote',\n",
       "  'Principal component analysis (PCA) is one of the most widely used dimensionality reduction tools in scientific data analysis. The PCA direction, given by the leading eigenvector of a covariance matrix, is a linear combination of all features with nonzero loadingsthis impedes interpretability. Sparse principal component analysis (SPCA) is a framework that enhances interpretability by incorporating an additional sparsity requirement in the feature weights (factor loadings) while finding a direction that explains the maximal variation in the data. However, unlike PCA, the optimization problem associated with the SPCA problem is NP-hard. While many heuristic algorithms based on variants of the power method are used to obtain good solutions, they do not provide certificates of optimality on the solution-quality via associated dual bounds. Dual bounds are available via standard semidefinite programming (SDP) based relaxations, which may not be tight and the SDPs are difficult to scale using off-The-shelf solvers. In this paper, we present a convex integer programming (IP) framework to solve the SPCA problem to near-optimality, with an emphasis on deriving associated dual bounds. We present worst-case results on the quality of the dual bound provided by the convex IP. We empirically observe that the dual bounds are significantly better than worst-case performance, and are superior to the SDP bounds on some real-life instances. Moreover, solving the convex IP model using commercial IP solvers appears to scale much better that solving the SDP-relaxation using commercial solvers. To the best of our knowledge, we obtain the best dual bounds for real and artificial instances for SPCA problems involving covariance matrices of size up to 2000 × 2000.'),\n",
       " (409, 'date', '2018-00-00 2018'),\n",
       " (409, 'publicationTitle', 'arXiv'),\n",
       " (409, 'ISSN', '23318422'),\n",
       " (410, 'title', 'PDF'),\n",
       " (411,\n",
       "  'title',\n",
       "  'Sparse regression at scale: Branch-and-bound rooted in first-order optimization'),\n",
       " (411,\n",
       "  'abstractNote',\n",
       "  'We consider the least squares regression problem, penalized with a combination of the `0 and `2 norms (a.k.a. `0`2 regularization). Recent work presents strong evidence that the resulting `0-based estimators can outperform popular sparse learning methods, under many important high-dimensional settings. However, exact computation of `0-based estimators remains a major challenge. Indeed, state-of-the-art mixed integer programming (MIP) methods for `0`2-regularized regression face difficulties in solving many statistically interesting instances when the number of features p ∼ 104. In this work, we present a new exact MIP framework for `0`2-regularized regression that can scale to p ∼ 107, achieving over 3600x speed-ups compared to the fastest exact methods. Unlike recent work, which relies on modern MIP solvers, we design a specialized nonlinear BnB framework, by critically exploiting the problem structure. A key distinguishing component in our algorithm lies in efficiently solving the node relaxations using specialized first-order methods, based on coordinate descent (CD). Our CD-based method effectively leverages information across the BnB nodes, through using warm starts, active sets, and gradient screening. In addition, we design a novel method for obtaining dual bounds from primal solutions, which certifiably works in high dimensions. Experiments on synthetic and real high-dimensional datasets demonstrate that our method is not only significantly faster than the state of the art, but can also deliver certifiably optimal solutions to statistically challenging instances that cannot be handled with existing methods. We open source the implementation through our toolkit L0BnB.'),\n",
       " (411, 'date', '2020-00-00 2020'),\n",
       " (411, 'pages', '1-36'),\n",
       " (411, 'publicationTitle', 'arXiv'),\n",
       " (411, 'ISSN', '23318422'),\n",
       " (412, 'title', 'PDF'),\n",
       " (413,\n",
       "  'title',\n",
       "  'The method of projections for finding the common point of convex sets'),\n",
       " (413,\n",
       "  'abstractNote',\n",
       "  'MANY mathematical and applied problems can be reduced to finding some common point of a system (finite or infinite) of convex sets. Usually each of the sets is such that it is not difficult to find the projection of any point on to this set. In this paper we shall consider various methods of finding points from the intersection of sets, using projection on to a separate set as an elementary operation. The strong convergence of the sequences obtained in this way is proved. Applications are given to various problems, including the problem of best approximation and problems of optimal control. Particular attention is paid in the latter case to problems with restrictions on the phase coordinates. © 1970.'),\n",
       " (413, 'date', '1967-00-00 1967'),\n",
       " (413, 'volume', '7'),\n",
       " (413, 'pages', '1-24'),\n",
       " (413,\n",
       "  'publicationTitle',\n",
       "  'USSR Computational Mathematics and Mathematical Physics'),\n",
       " (413, 'DOI', '10.1016/0041-5553(67)90113-9'),\n",
       " (413, 'issue', '6'),\n",
       " (413, 'ISSN', '00415553'),\n",
       " (414, 'title', 'PDF'),\n",
       " (415,\n",
       "  'title',\n",
       "  'Block-iterative methods for consistent and inconsistent linear equations'),\n",
       " (415,\n",
       "  'abstractNote',\n",
       "  'We shall in this paper consider the problem of computing a generalized solution of a given linear system of equations. The matrix will be partitioned by blocks of rows or blocks of columns. The generalized inverses of the blocks are then used as data to Jacobi- and SOR-types of iterative schemes. It is shown that the methods based on partitioning by rows converge towards the minimum norm solution of a consistent linear system. The column methods converge towards a least squares solution of a given system. For the case with two blocks explicit expressions for the optimal values of the iteration parameters are obtained. Finally an application is given to the linear system that arises from reconstruction of a two-dimensional object by its one-dimensional projections. © 1980 Springer-Verlag.'),\n",
       " (415, 'date', '1980-00-00 1980'),\n",
       " (415, 'volume', '35'),\n",
       " (415, 'pages', '1-12'),\n",
       " (415, 'publicationTitle', 'Numerische Mathematik'),\n",
       " (415, 'DOI', '10.1007/BF01396365'),\n",
       " (415, 'issue', '1'),\n",
       " (415, 'ISSN', '0029599X'),\n",
       " (416, 'title', 'PDF'),\n",
       " (417,\n",
       "  'title',\n",
       "  'Row-Action Methods for Huge and Sparse Systems and Their Applications'),\n",
       " (417,\n",
       "  'abstractNote',\n",
       "  'This paper brings together and discusses theory and applications of methods, identified and labelled as row-action methods, for linear feasibility problems (find $x \\\\in {\\\\bf R}^n $, such that $Ax \\\\leqq b$), linearly constrained optimization problems (minimize $f(x)$, subject to $Ax \\\\leqq b$) and some interval convex programming problems (minimize $f(x)$, subject to $c \\\\leqq Ax \\\\leqq b$).\\\\n\\\\nThe main feature of row-action methods is that they are iterative procedures which, without making any changes to the original matrix A, use the rows of A, one row at a time. Such methods are important and have demonstrated effectiveness for problems with large or huge matrices which do not enjoy any detectable or usable structural pattern, apart from a high degree of sparaseness.\\\\n\\\\nFields of application where row-action methods are used in various ways include image reconstruction from projection, operations research and game theory, learning theory, pattern recognition and transportation theory. A row-action method for the nonlinear convex feasibility problem is also presented.\\\\n\\\\n\\\\n\\\\nRead More: http://epubs.siam.org.proxy-um.researchport.umd.edu/doi/abs/10.1137/1023097'),\n",
       " (417, 'date', '1981-00-00 1981'),\n",
       " (417, 'volume', '23'),\n",
       " (417, 'pages', '444-466'),\n",
       " (417, 'publicationTitle', 'SIAM Review'),\n",
       " (417, 'DOI', '10.1137/1023097'),\n",
       " (417, 'issue', '4'),\n",
       " (417, 'ISSN', '0036-1445'),\n",
       " (418, 'title', 'PDF'),\n",
       " (419, 'title', 'Projection methods for solving sparse linear systems'),\n",
       " (419,\n",
       "  'abstractNote',\n",
       "  'Some methods of successive approximation for the solution of simultaneous linear equations are discussed. The coefficient matrix A of the linear system is assumed to be sparse. It is shown that savings in the computer storage and the computing time are possible, if there exists a subset of the rows (columns) of A, consisting of only orthogonal rows (columns). Such savings are also possible, if for some permutation matrices P and Q, PAQ has a particular structure, viz., singly bordered block diagonal form. It is shown that the set of orthogonal rows (columns) of A, as well as P and Q can be determined by using some results from graph theory (e.g., incidence matrices, row and column graphs, points of attachment). Geometrical interpretations of the methods and their inter-relatiohip are given. © 1969 The British Computer Society.'),\n",
       " (419, 'date', '1969-00-00 1969'),\n",
       " (419, 'volume', '12'),\n",
       " (419, 'pages', '77-80'),\n",
       " (419, 'publicationTitle', 'Computer Journal'),\n",
       " (419, 'DOI', '10.1093/comjnl/12.1.77'),\n",
       " (419, 'issue', '1'),\n",
       " (419, 'ISSN', '00104620'),\n",
       " (421, 'title', 'PDF'),\n",
       " (423,\n",
       "  'title',\n",
       "  \"Strong Underrelaxation in Kaczmarz ' s Method for Inconsistent Systems *\"),\n",
       " (423, 'date', '1983-00-00 1983'),\n",
       " (423, 'volume', '92'),\n",
       " (423, 'pages', '83-92'),\n",
       " (423, 'publicationTitle', 'Image Processing'),\n",
       " (424, 'title', 'PDF'),\n",
       " (425, 'title', 'The Mathematics of Computed Tomography'),\n",
       " (425, 'date', '2001-00-00 2001'),\n",
       " (425, 'ISBN', '0-89871-493-1'),\n",
       " (426, 'title', 'PDF'),\n",
       " (427,\n",
       "  'title',\n",
       "  'A sparse Kaczmarz solver and a linearized Bregman method for online compressed sensing arXiv : 1403 . 7543v1 [ math . OC ] 28 Mar 2014'),\n",
       " (429,\n",
       "  'title',\n",
       "  'TRex: A Tomography Reconstruction Proximal Framework for Robust Sparse View X-Ray Applications'),\n",
       " (429,\n",
       "  'abstractNote',\n",
       "  'We present TRex, a flexible and robust Tomographic Reconstruction framework using proximal algorithms. We provide an overview and perform an experimental comparison between the famous iterative reconstruction methods in terms of reconstruction quality in sparse view situations. We then derive the proximal operators for the four best methods. We show the flexibility of our framework by deriving solvers for two noise models: Gaussian and Poisson; and by plugging in three powerful regularizers. We compare our framework to state of the art methods, and show superior quality on both synthetic and real datasets.'),\n",
       " (429, 'date', '2016-06-11 2016-06-11'),\n",
       " (429, 'url', 'http://arxiv.org/abs/1606.03601'),\n",
       " (429, 'accessDate', '2021-05-27'),\n",
       " (430, 'title', 'PDF'),\n",
       " (431,\n",
       "  'title',\n",
       "  'On greedy randomized kaczmarz method for solving large sparse linear systems'),\n",
       " (431,\n",
       "  'abstractNote',\n",
       "  'For solving large-scale systems of linear equations by iteration methods, we introduce an effective probability criterion for selecting the working rows from the coefficient matrix and construct a greedy randomized Kaczmarz method. It is proved that this method converges to the unique least-norm solution of the linear system when it is consistent. Theoretical analysis demonstrates that the convergence rate of the greedy randomized Kaczmarz method is much faster than the randomized Kaczmarz method, and numerical results also show that the greedy randomized Kaczmarz method is more efficient than the randomized Kaczmarz method.'),\n",
       " (431, 'date', '2018-00-00 2018'),\n",
       " (431, 'volume', '40'),\n",
       " (431, 'pages', 'A592-A606'),\n",
       " (431, 'publicationTitle', 'SIAM Journal on Scientific Computing'),\n",
       " (431, 'DOI', '10.1137/17M1137747'),\n",
       " (431, 'issue', '1'),\n",
       " (431, 'ISSN', '10957197'),\n",
       " (432,\n",
       "  'title',\n",
       "  'On relaxed greedy randomized Kaczmarz methods for solving large sparse linear systems'),\n",
       " (432,\n",
       "  'abstractNote',\n",
       "  'For solving large sparse systems of linear equations by iteration methods, we further generalize the greedy randomized Kaczmarz method by introducing a relaxation parameter in the involved probability criterion, obtaining a class of relaxed greedy randomized Kaczmarz methods. We prove the convergence of these methods when the linear system is consistent, and show that these methods can be more efficient than the greedy randomized Kaczmarz method if the relaxation parameter is chosen appropriately.'),\n",
       " (432, 'date', '2018-00-00 2018'),\n",
       " (432, 'volume', '83'),\n",
       " (432, 'publicationTitle', 'Applied Mathematics Letters'),\n",
       " (432, 'DOI', '10.1016/j.aml.2018.03.008'),\n",
       " (432, 'ISSN', '18735452'),\n",
       " (433,\n",
       "  'title',\n",
       "  'On partially randomized extended Kaczmarz method for solving large sparse overdetermined inconsistent linear systems'),\n",
       " (433,\n",
       "  'abstractNote',\n",
       "  'For solving large sparse, overdetermined, and inconsistent system of linear equations by iteration methods, by further reconstructing the randomized extended Kaczmarz method proposed by Zouzias and Freris in 2013 (SIAM J. Matrix Anal. Appl. 34 (2013), 773–793), we propose a partially randomized extended Kaczmarz method. When the coefficient matrix is assumed to be of full column rank, we prove the convergence and derive an upper bound for the expected convergence rate of the partially randomized extended Kaczmarz method. This bound could be smaller than that of the randomized extended Kaczmarz method under certain conditions. Moreover, with numerical results we show that the partially randomized extended Kaczmarz method can be much more effective than the randomized extended Kaczmarz method.'),\n",
       " (433, 'date', '2019-10-01 2019-10-01'),\n",
       " (433, 'accessDate', '2021-05-27'),\n",
       " (433, 'volume', '578'),\n",
       " (433, 'pages', '225-250'),\n",
       " (433, 'publicationTitle', 'Linear Algebra and Its Applications'),\n",
       " (433, 'DOI', '10.1016/j.laa.2019.05.005'),\n",
       " (433, 'ISSN', '00243795'),\n",
       " (434, 'title', 'PDF'),\n",
       " (435,\n",
       "  'title',\n",
       "  'Component averaging: An efficient iterative parallel algorithm for large and sparse unstructured problems'),\n",
       " (435,\n",
       "  'abstractNote',\n",
       "  \"Component averaging (CAV) is introduced as a new iterative parallel technique suitable for large and sparse unstructured systems of linear equations. It simultaneously projects the current iterate onto all the system's hyperplanes, and is thus inherently parallel. However, instead of orthogonal projections and scalar weights (as used, for example, in Cimmino's method), it uses oblique projections and diagonal weighting matrices, with weights related to the sparsity of the system matrix. These features provide for a practical convergence rate which approaches that of algebraic reconstruction technique (ART) (Kaczmarz's row-action algorithm) - even on a single processor. Furthermore, the new algorithm also converges in the inconsistent case. A proof of convergence is provided for unit relaxation, and the fast convergence is demonstrated on image reconstruction problems of the Herman head phantom obtained within the SNARK93 image reconstruction software package. Both reconstructed images and convergence plots are presented. The practical consequences of the new technique are far reaching for real-world problems in which iterative algorithms are used for solving large, sparse, unstructured and often inconsistent systems of linear equations. © 2001 Elsevier Science B.V.\"),\n",
       " (435, 'date', '2001-00-00 2001'),\n",
       " (435, 'volume', '27'),\n",
       " (435, 'publicationTitle', 'Parallel Computing'),\n",
       " (435, 'DOI', '10.1016/S0167-8191(00)00100-9'),\n",
       " (435, 'issue', '6'),\n",
       " (435, 'ISSN', '01678191'),\n",
       " (436,\n",
       "  'title',\n",
       "  'BICAV: A block-iterative parallel algorithm for sparse systems with pixel-related weighting'),\n",
       " (436,\n",
       "  'abstractNote',\n",
       "  'Component averaging (CAV) was recently introduced by Censor, Gordon, and Gordon as a new iterative parallel technique suitable for large and sparse unstructured systems of linear equations. Based on earlier work of Byrne and Censor, it uses diagonal weighting matrices, with pixel-related weights determined by the sparsity of the system matrix. CAV is inherently parallel (similar to the very slowly converging Cimmino method) but its practical convergence on problems of image reconstruction from projections is similar to that of the algebraic reconstruction technique (ART). Parallel techniques are becoming more important for practical image reconstruction since they are relevant not only for supercomputers but also for the increasingly prevalent multiprocessor workstations. This paper reports on experimental results with a block-iterative version of component averaging (BICAV). When BICAV is optimized for block size and relaxation parameters, its very first iterates are far superior to those of CAV, and more or less on a par with ART. Similar to CAV, BICAV is also inherently parallel. The fast convergence is demonstrated on problems of image reconstruction from projections, using the SNARK93 image reconstruction software package. Detailed plots of various measures of convergence, and reconstructed images are presented. © 2001 IEEE.'),\n",
       " (436, 'date', '2001-00-00 2001'),\n",
       " (436, 'volume', '20'),\n",
       " (436, 'publicationTitle', 'IEEE Transactions on Medical Imaging'),\n",
       " (436, 'DOI', '10.1109/42.959302'),\n",
       " (436, 'issue', '10'),\n",
       " (436, 'ISSN', '02780062'),\n",
       " (437, 'title', 'Beyond Moore-Penrose: Sparse pseudoinverse'),\n",
       " (437,\n",
       "  'abstractNote',\n",
       "  'Frequently, we use the Moore-Penrose pseudoinverse (MPP) even in cases when we do not require all of its defining properties. But if the running time and the storage size are critical, we can do better. By discarding some constraints needed for the MPP, we gain freedom to optimize other aspects of the new pseudoinverse. A sparser pseudoinverse reduces the amount of computation and storage. We propose a method to compute a sparse pseudoinverse and show that it offers sizable improvements in speed and storage, with a small loss in the least-squares performance. Differently from previous approaches, we do not attempt to approximate the MPP, but rather to produce an exact but sparse pseudoinverse. In the underdetermined (compressed sensing) scenario we prove that the rescaled sparse pseudoinverse yields an unbiased estimate of the unknown vector, and we demonstrate its potential in iterative sparse recovery algorithms, pointing out directions for future research. © 2013 IEEE.'),\n",
       " (437, 'date', '2013-00-00 2013'),\n",
       " (437,\n",
       "  'proceedingsTitle',\n",
       "  'ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings'),\n",
       " (437, 'DOI', '10.1109/ICASSP.2013.6638923'),\n",
       " (438,\n",
       "  'title',\n",
       "  'Sparse Randomized Kaczmarz for Support Recovery of Jointly Sparse Corrupted Multiple Measurement Vectors'),\n",
       " (438,\n",
       "  'abstractNote',\n",
       "  'While single measurement vector (SMV) models have been widely studied in signal processing, there is a surging interest in addressing the multiple measurement vectors (MMV) problem. In the MMV setting, more than one measurement vector is available and the multiple signals to be recovered share some commonalities such as a common support. Applications in which MMV is a naturally occurring phenomenon include online streaming, medical imaging, and video recovery. This work presents a stochastic iterative algorithm for the support recovery of jointly sparse corrupted MMV. We present a variant of the sparse randomized Kaczmarz algorithm for corrupted MMV and compare our proposed method with an existing Kaczmarz type algorithm for MMV problems. We also showcase the usefulness of our approach in the online (streaming) setting and provide empirical evidence that suggests the robustness of the proposed method to the number of corruptions and the distribution from which the corruptions are drawn.'),\n",
       " (438, 'date', '2019-00-00 2019'),\n",
       " (438, 'volume', '17'),\n",
       " (438, 'pages', '1-14'),\n",
       " (438, 'bookTitle', 'Association for Women in Mathematics Series'),\n",
       " (439,\n",
       "  'title',\n",
       "  'A Wavelet Based Sparse Row-Action Method for Image Reconstruction in Magnetic Particle Imaging'),\n",
       " (439,\n",
       "  'abstractNote',\n",
       "  'Magnetic Particle Imaging (MPI) is a preclinical imaging technique capable of visualizing the spatio-temporal distribution of magnetic nanoparticles. The image reconstruction of this fast and dynamic process relies on efficiently solving an ill-posed inverse problem. Current approaches to reconstruct the tracer concentration from its measurements are either adapted to image characteristics of MPI but suffer from higher computational complexity and slower convergence or are fast but lack in the image quality of the reconstructed images. In this work we propose a novel MPI reconstruction method to combine the advantages of both approaches into a single algorithm. The underlying sparsity prior is based on an undecimated wavelet transform and is integrated into a fast row-action framework to solve the corresponding MPI minimization problem. Its performance is numerically evaluated against a classical FISTA approach on simulated and real MPI data. We also compare the results to the state-of-the-art MPI reconstruction methods. In all cases, our approach shows better reconstruction results and at the same time accelerates the convergence rate of the underlying row-action algorithm.'),\n",
       " (439, 'date', '2020-03-30 2020-03-30'),\n",
       " (439, 'url', 'http://arxiv.org/abs/2003.13787'),\n",
       " (439, 'accessDate', '2021-05-27'),\n",
       " (439, 'publicationTitle', 'Medical Physics'),\n",
       " (440, 'title', 'PDF'),\n",
       " (441,\n",
       "  'title',\n",
       "  'Microkicking for fast convergence of sparse kaczmarz and sparse LMS'),\n",
       " (441,\n",
       "  'abstractNote',\n",
       "  'Algorithms based on linearized Bregman iterations are able to perform sparse reconstruction at a low computational complexity. Especially the Least-Mean-Squares (LMS) and Kacz-marz variants of linearized Bregman iterations proved to be very feasible for fixed-point digital hardware implementation. We present a method that we call microkicking for improving the convergence speed of linearized Bregman based algorithms. This method can be implemented with only a negligible complexity overhead leading to significantly faster convergence for both variants of the linearized Bregman iterations. We furthermore show simulation results demonstrating the performance gains achievable by microkicking.'),\n",
       " (441, 'date', '2018-00-00 2018'),\n",
       " (441, 'volume', '2017-Decem'),\n",
       " (441, 'ISBN', '978-1-5386-1251-4'),\n",
       " (441, 'pages', '1-5'),\n",
       " (441,\n",
       "  'proceedingsTitle',\n",
       "  '2017 IEEE 7th International Workshop on Computational Advances in Multi-Sensor Adaptive Processing, CAMSAP 2017'),\n",
       " (441, 'DOI', '10.1109/CAMSAP.2017.8313085'),\n",
       " (443, 'title', 'PDF'),\n",
       " (446,\n",
       "  'title',\n",
       "  'Randomized iterative hard thresholding for sparse approximations'),\n",
       " (446,\n",
       "  'abstractNote',\n",
       "  'Typical greedy algorithms for sparse reconstruction problems, such as orthogonal matching pursuit and iterative thresholding, seek strictly sparse solutions. Recent work in the literature suggests that given a priori knowledge of the distribution of the sparse signal coefficients, better results can be obtained by a weighted averaging of several sparse solutions. Such a combination of solutions, while not strictly sparse, approximates an MMSE estimator and can outperform strictly sparse solvers in terms of l-2 reconstruction error. We introduce a novel method for obtaining such an approximate MMSE estimator by replacing the deterministic thresholding operator of Iterative Hard Thresholding with a randomized version. We demonstrate the improvement in performance experimentally for both synthetic 1D signals and real images. © 2014 IEEE.'),\n",
       " (446, 'date', '2014-00-00 2014'),\n",
       " (446, 'pages', '403'),\n",
       " (446, 'publicationTitle', 'Data Compression Conference Proceedings'),\n",
       " (446, 'DOI', '10.1109/DCC.2014.25'),\n",
       " (446, 'ISSN', '10680314'),\n",
       " (447, 'title', 'PDF'),\n",
       " (448, 'title', 'Randomized Kaczmarz with averaging'),\n",
       " (448, 'date', '2021-03-11 2021-03-11'),\n",
       " (448, 'url', 'https://link.springer.com/10.1007/s10543-020-00824-1'),\n",
       " (448, 'volume', '61'),\n",
       " (448, 'pages', '337-359'),\n",
       " (448, 'publicationTitle', 'BIT Numerical Mathematics'),\n",
       " (448, 'DOI', '10.1007/s10543-020-00824-1'),\n",
       " (448, 'issue', '1'),\n",
       " (448, 'ISSN', '0006-3835'),\n",
       " (449, 'title', 'PDF'),\n",
       " (450,\n",
       "  'title',\n",
       "  'The Averaged Kaczmarz Iteration for Solving Inverse Problems'),\n",
       " (450, 'date', '2018-00-00 2018'),\n",
       " (450, 'volume', '11'),\n",
       " (450, 'pages', '618-642'),\n",
       " (450, 'publicationTitle', 'SIAM Journal on Imaging Sciences'),\n",
       " (450, 'DOI', '10.1137/17M1146178'),\n",
       " (450, 'issue', '1'),\n",
       " (451, 'title', 'PDF'),\n",
       " (452, 'title', 'Fast proximal gradient methods'),\n",
       " (452, 'date', '2013-00-00 2013'),\n",
       " (452, 'accessDate', '2022-01-29'),\n",
       " (452, 'pages', '1-32'),\n",
       " (453, 'title', 'PDF'),\n",
       " (454, 'title', 'Inverse problems and regularization - An Introduction'),\n",
       " (455, 'title', 'PDF'),\n",
       " (456, 'title', 'Inexact accelerated high-order proximal-point methods'),\n",
       " (456,\n",
       "  'abstractNote',\n",
       "  'In this paper, we present a new framework of bi-level unconstrained minimization for development of accelerated methods in Convex Programming. These methods use approximations of the high-order proximal points, which are solutions of some auxiliary parametric optimization problems. For computing these points, we can use different methods, and, in particular, the lower-order schemes. This opens a possibility for the latter methods to overpass traditional limits of the Complexity Theory. As an example, we obtain a new second-order method with the convergence rate O(k- 4) , where k is the iteration counter. This rate is better than the maximal possible rate of convergence for this type of methods, as applied to functions with Lipschitz continuous Hessian. We also present new methods with the exact auxiliary search procedure, which have the rate of convergence O(k-(3p+1)/2) , where p≥ 1 is the order of the proximal operator. The auxiliary problem at each iteration of these schemes is convex.'),\n",
       " (456, 'date', '2021-00-00 2021'),\n",
       " (456, 'url', 'https://doi.org/10.1007/s10107-021-01727-x'),\n",
       " (456, 'accessDate', '2022-01-30'),\n",
       " (456, 'publicationTitle', 'Mathematical Programming'),\n",
       " (456, 'DOI', '10.1007/s10107-021-01727-x'),\n",
       " (456, 'ISSN', '14364646'),\n",
       " (457, 'title', 'PDF'),\n",
       " (458, 'title', 'Gradient methods for minimizing composite functions'),\n",
       " (458,\n",
       "  'abstractNote',\n",
       "  'In this paper we analyze several new methods for solving optimization problems with the objective function formed as a sum of two terms: one is smooth and given by a black-box oracle, and another is a simple general convex function with known structure. Despite the absence of good properties of the sum, such problems, both in convex and nonconvex cases, can be solved with efficiency typical for the first part of the objective. For convex problems of the above structure, we consider primal and dual variants of the gradient method (with convergence rate O(1/k)), and an accelerated multistep version with convergence rate O(k2/1), where k is the iteration counter. For nonconvex problems with this structure, we prove convergence to a point from which there is no descent direction. In contrast, we show that for general nonsmooth, nonconvex problems, even resolving the question of whether a descent direction exists from a point is NP-hard. For all methods, we suggest some efficient \"line search\" procedures and show that the additional computational work necessary for estimating the unknown problem class parameters can only multiply the complexity of each iteration by a small constant factor. We present also the results of preliminary computational experiments, which confirm the superiority of the accelerated scheme. © 2012 Springer-Verlag Berlin Heidelberg and Mathematical Optimization Society.'),\n",
       " (458, 'date', '2013-00-00 2013'),\n",
       " (458, 'accessDate', '2022-01-30'),\n",
       " (458, 'volume', '140'),\n",
       " (458, 'pages', '125-161'),\n",
       " (458, 'publicationTitle', 'Mathematical Programming'),\n",
       " (458, 'DOI', '10.1007/s10107-012-0629-5'),\n",
       " (458, 'issue', '1'),\n",
       " (458, 'ISSN', '00255610'),\n",
       " (459, 'title', 'PDF'),\n",
       " (460,\n",
       "  'title',\n",
       "  'Gradient methods for minimizing composite objective function'),\n",
       " (460,\n",
       "  'abstractNote',\n",
       "  'In this paper we analyze several new methods for solving optimization problems with the objective function formed as a sum of two convex terms: one is smooth and given by a black-box oracle, and another is general but simple and its structure is known. Despite to the bad properties of the sum, such problems, both in convex and nonconvex cases, can be solved with efficiency typical for the good part of the objective. For convex problems of the above structure, we consider primal and dual variants of the gradient method (converge as O 1 k), and an accelerated multistep version with convergence rate O 1 k 2 , where k is the iteration counter. For all methods, we suggest some efficient \"line search\" procedures and show that the additional computational work necessary for estimating the unknown problem class parameters can only multiply the complexity of each iteration by a small constant factor. We present also the results of preliminary computational experiments, which confirm the superiority of the accelerated scheme.'),\n",
       " (460, 'date', '2007-00-00 2007'),\n",
       " (460, 'accessDate', '2022-02-01'),\n",
       " (461, 'title', 'PDF'),\n",
       " (462, 'title', 'Inverse problems'),\n",
       " (462, 'date', '2019-00-00 2019'),\n",
       " (462, 'pages', '1-102'),\n",
       " (462, 'publicationTitle', 'Lecture Notes, Uni Cambridge'),\n",
       " (462, 'ISSN', '14370859'),\n",
       " (463, 'title', 'PDF'),\n",
       " (465, 'title', 'A Note On Convergence Rate of Randomized Kaczmarz Method'),\n",
       " (465,\n",
       "  'abstractNote',\n",
       "  'In this paper, we propose an alternative version of the randomized Kaczmarz method, which chooses each row of the coefficient matrix A with probability proportional to the square of the Euclidean norm of the residual of each corresponding equation. We prove that it converges with expected linear rate and the convergence rate of this method is better than the Strohmer and Vershynin’s RK method. Numerical experiments also show this method is more efficient than the RK method.'),\n",
       " (465, 'date', '2020-00-00 2020'),\n",
       " (465, 'url', 'https://doi.org/10.1007/s10092-020-00376-4'),\n",
       " (465, 'volume', '57'),\n",
       " (465, 'pages', '1-11'),\n",
       " (465, 'publicationTitle', 'Calcolo'),\n",
       " (465, 'DOI', '10.1007/s10092-020-00376-4'),\n",
       " (465, 'issue', '3'),\n",
       " (465, 'ISSN', '11265434'),\n",
       " (466, 'title', 'PDF'),\n",
       " (467, 'title', 'Remarks on Kaczmarz algorithm for solving'),\n",
       " (467, 'date', '2020-00-00 2020'),\n",
       " (467, 'pages', '1-12'),\n",
       " (467, 'DOI', '10.1007/978-3-030-50417-5'),\n",
       " (468, 'title', 'PDF'),\n",
       " (469,\n",
       "  'title',\n",
       "  'Convergence rates for greedy Kaczmarz algorithms, and faster randomized Kaczmarz rules using the orthogonality graph'),\n",
       " (469,\n",
       "  'abstractNote',\n",
       "  'The Kaczmarz method is an iterative algorithm for solving systems of linear equalities and inequalities, that iteratively projects onto these constraints. Recently, Strohmer and Vershynin [J. Fourier Anal. Appl., 15(2):262-278, 2009] gave a non-asymptotic convergence rate analysis for this algorithm, spurring numerous extensions and generalizations of the Kaczmarz method. Rather than the randomized selection rule analyzed in that work, in this paper we instead discuss greedy and approximate greedy selection rules. We show that in some applications the computational costs of greedy and random selection are comparable, and that in many cases greedy selection rules give faster convergence rates than random selection rules. Further, we give the first multi-step analysis of Kaczmarz methods for a particular greedy rule, and propose a provably-faster randomized selection rule for matrices with many pairwise-orthogonal rows.'),\n",
       " (469, 'date', '2016-00-00 2016'),\n",
       " (469, 'pages', '547-556'),\n",
       " (469,\n",
       "  'publicationTitle',\n",
       "  '32nd Conference on Uncertainty in Artificial Intelligence 2016, UAI 2016'),\n",
       " (470, 'title', 'PDF'),\n",
       " (471,\n",
       "  'title',\n",
       "  'Convergence analysis for Kaczmarz-type methods in a Hilbert space framework'),\n",
       " (471,\n",
       "  'abstractNote',\n",
       "  'Using the concept of stable Hilbert space splittings, we provide a unified approach to the convergence analysis for multiplicative Schwarz methods (a version of alternating directions methods), and in particular Kaczmarz-type methods for solving linear systems. We consider both fixed cyclic and randomized ordering strategies, and cover block versions as well. For the classical Kaczmarz method with cyclic ordering for solving general linear systems Ax=b, a new convergence rate estimate in terms of the generalized condition number of A and logarithmically depending on the rank of A is presented.'),\n",
       " (471, 'date', '2015-00-00 2015'),\n",
       " (471, 'volume', '478'),\n",
       " (471, 'pages', '131-161'),\n",
       " (471, 'publicationTitle', 'Linear Algebra and Its Applications'),\n",
       " (471, 'DOI', '10.1016/j.laa.2015.03.028'),\n",
       " (471, 'ISSN', '00243795'),\n",
       " (472, 'title', 'PDF'),\n",
       " (473, 'title', 'Convergence rates for Kaczmarz-type algorithms'),\n",
       " (473,\n",
       "  'abstractNote',\n",
       "  'In this paper, we make a theoretical analysis of the convergence rates of Kaczmarz and extended Kaczmarz projection algorithms for some of the most practically used control sequences. We first prove an at least linear convergence rate for the Kaczmarz-Tanabe and its extended version methods (the one in which a complete set of projections using row/column indices is performed in each iteration). Then, we apply the main ideas of this analysis in establishing an at least sublinear, respectively, linear convergence rate for the Kaczmarz algorithm with almost cyclic and the remotest set control strategies, and their extended versions, respectively. These results complete the existing ones related to the random selection procedures.'),\n",
       " (473, 'date', '2018-00-00 2018'),\n",
       " (473, 'volume', '79'),\n",
       " (473, 'pages', '1-17'),\n",
       " (473, 'publicationTitle', 'Numerical Algorithms'),\n",
       " (473, 'DOI', '10.1007/s11075-017-0425-7'),\n",
       " (473, 'issue', '1'),\n",
       " (473, 'ISSN', '15729265'),\n",
       " (474, 'title', 'PDF'),\n",
       " (475,\n",
       "  'abstractNote',\n",
       "  'We consider the class of iterative shrinkage-thresholding algorithms (ISTA) for solving linear inverse problems arising in signal/image processing. This class of methods, which can be viewed as an ex- tension of the classical gradient algorithm, is attractive due to its simplicity and thus is adequate for solving large-scale problems even with dense matrix data. However, such methods are also known to converge quite slowly. In this paper we present a new fast iterative shrinkage-thresholding algorithm (FISTA) which preserves the computational simplicity of ISTA but with a global rate of convergence which is proven to be significantly better, both theoretically and practically. Initial promising nu- merical results for wavelet-based image deblurring demonstrate the capabilities of FISTA which is shown to be faster than ISTA by several orders of magnitude. Key'),\n",
       " (475, 'date', '2009-00-00 2009'),\n",
       " (475, 'volume', '2'),\n",
       " (475, 'pages', '183-202'),\n",
       " (475,\n",
       "  'publicationTitle',\n",
       "  'Society for Industrial and Applied Mathematics Journal on Imaging Sciences'),\n",
       " (475, 'issue', '1'),\n",
       " (475, 'ISSN', '1936-4954'),\n",
       " (476, 'title', 'PDF'),\n",
       " (477, 'title', 'A mathematical introduction to compressive sensing'),\n",
       " (477, 'date', '2013-00-00 2013'),\n",
       " (477, 'ISBN', '978-0-8176-4947-0'),\n",
       " (477, 'numPages', '1-615'),\n",
       " (478, 'title', 'PDF'),\n",
       " (479, 'title', 'The Method of Orthogonal Projections'),\n",
       " (479, 'date', '1977-00-00 1977'),\n",
       " (479, 'pages', '524-532'),\n",
       " (479,\n",
       "  'publicationTitle',\n",
       "  'Variational Methods in Mathematics, Science and Engineering'),\n",
       " (479, 'DOI', '10.1007/978-94-011-6450-4_46'),\n",
       " (480, 'title', 'PDF'),\n",
       " (483, 'title', 'Lectures on Convex Optimization'),\n",
       " (483, 'date', '2018-00-00 2018'),\n",
       " (483, 'url', 'http://link.springer.com/10.1007/978-3-319-91578-4'),\n",
       " (483, 'volume', '137'),\n",
       " (483, 'place', 'Cham'),\n",
       " (483, 'publisher', 'Springer International Publishing'),\n",
       " (483, 'ISBN', '978-3-319-91577-7'),\n",
       " (483, 'numPages', 'i-589'),\n",
       " (484, 'title', 'PDF'),\n",
       " (485, 'title', 'Numerical Analysis Second Edition'),\n",
       " (485,\n",
       "  'abstractNote',\n",
       "  'The Agricultural Ingeneer Dr. Teruo Higa, professor of Horticulture at the University of The Ryukyus in Okinawa, Japan creates a technique in the 80th de- cade related with the use of efficient microorganism. This technology is the basis of the present review aim at providing information about groups of benevolent microorganisms such as: lactic acid bacteria, photo- trophic bacteria, actinomycetes group, yeast group and fungi present in natural ecosystems which are physiologically compatible with each other. Efficient Microorganisms, as a microbial inoculantion, resto- re soil microbiological balance, improve its physical and chemical conditions, increase crop production and protection, preserve natural resources, and ge- nerate a more sustainable agriculture and environ- ment. They can be used in the livestock (cattle, por- ciculture and poultry) for animal husbandry and the increase of productive variables. All this maximizes the efficiency of the systems and the management of excreta and facilities.'),\n",
       " (485, 'date', '2012-00-00 2012'),\n",
       " (485, 'ISBN', '978-0-8176-8258-3'),\n",
       " (485, 'numPages', '68-70'),\n",
       " (486, 'title', 'PDF'),\n",
       " (487,\n",
       "  'title',\n",
       "  'The restricted isometry property and its implications for compressed sensing'),\n",
       " (487,\n",
       "  'abstractNote',\n",
       "  'It is now well-known that one can reconstruct sparse or compressible signals accurately from a very limited number of measurements, possibly contaminated with noise. This technique known as \"compressed sensing\" or \"compressive sampling\" relies on properties of the sensing matrix such as the restricted isometry property. In this Note, we establish new results about the accuracy of the reconstruction from undersampled measurements which improve on earlier estimates, and have the advantage of being more elegant. To cite this article: E.J. Candès, C. R. Acad. Sci. Paris, Ser. I 346 (2008). © 2008 Académie des sciences.'),\n",
       " (487, 'date', '2008-00-00 2008'),\n",
       " (487, 'volume', '346'),\n",
       " (487, 'pages', '589-592'),\n",
       " (487, 'publicationTitle', 'Comptes Rendus Mathematique'),\n",
       " ...]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1aae72bf",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 (885, '23TVFPBQ', 'Nunemacher2003')\n",
      "2 (603, '253SZQ85', 'Fliege2006')\n",
      "3 (572, '25ZYRQ7W', 'Shen2016')\n",
      "4 (830, '2CW4LMYK', 'Bergounioux2006')\n",
      "5 (766, '2G9MIUPZ', 'Ji2021')\n",
      "6 (1648, '2JZ9EPED', 'Parlett1974')\n",
      "7 (750, '2M38SR4F', 'Bui2022')\n",
      "8 (983, '2PLBN8GG', 'Rockafellar2006')\n",
      "9 (2023, '2RCYD5AE', 'Kumar2023')\n",
      "10 (105, '2UD7Y8NR', 'Xun2012')\n",
      "11 (998, '2WMXG5V8', 'Borkar2008')\n",
      "12 (796, '2Z7RQCNR', 'Hao2020')\n",
      "13 (1002, '32XPMGIN', 'Yang2021')\n",
      "14 (427, '34J9TKNK', 'Lorenz2014a')\n",
      "15 (32, '37AE3RYY', 'Cotter2010')\n",
      "16 (2326, '3ADSLNVF', 'Chung2000')\n",
      "17 (2224, '3HUSHI9S', 'Guibas1986')\n",
      "18 (6512, '3MLSF4GH', 'Dewaskar')\n",
      "19 (2225, '3N8SVK7A', 'sir2006')\n",
      "20 (699, '3QN8LT7X', 'Gao2022')\n",
      "21 (630, '3RZCATAN', 'Gupta2022')\n",
      "22 (2428, '45JPUPDP', 'Borkar2020')\n",
      "23 (2100, '4AI3NRNX', 'Evans2021')\n",
      "24 (1617, '4BFIEB8W', 'Agaskar2015')\n",
      "25 (770, '4FGKJDIE', 'Mehlitz2021')\n",
      "26 (107, '4J8VQFF3', 'Panageas2016a')\n",
      "27 (2478, '4JXH2XM2', 'Sontag1998')\n",
      "28 (967, '4KF9M5N8', 'Ben-ayed1993')\n",
      "29 (949, '4KFX7LWU', 'Bard1984')\n",
      "30 (10, '4MFHUNBV', 'Polak2014')\n",
      "31 (624, '4P8XUHCA', 'Dempe2005')\n",
      "32 (800, '4QQ8SMFX', 'Bae2020')\n",
      "33 (376, '4T9KPRM5', 'Sra2006')\n",
      "34 (1904, '4TCHH3DV', 'Grant2018')\n",
      "35 (679, '5367LCVU', 'Yang2022')\n",
      "36 (2332, '53RJAB6B', 'Villani2023')\n",
      "37 (568, '57F2NNHD', 'Debnath2020')\n",
      "38 (2396, '59EDJEQK', 'Kolmogorov1960')\n",
      "39 (367, '5EDIRAG2', 'Fornasier2015')\n",
      "40 (485, '5FHLXDFU', 'WalterGautschi2012')\n",
      "41 (746, '5K3AE3BT', 'Sato2021')\n",
      "42 (681, '5Q6BWBWV', 'Lamontagne2022')\n",
      "43 (327, '5R635KPF', 'Haddock2019a')\n",
      "44 (306, '5SW9HXKK', 'Rulla2015')\n",
      "45 (612, '5U45VJFY', 'Candler1988')\n",
      "46 (1610, '5VAKT6FZ', 'Adams2012')\n",
      "47 (2002, '68KXJH92', 'Kerdreux2020')\n",
      "48 (6, '6DGRHLYU', 'Sangid2013')\n",
      "49 (471, '6E5SB8X8', 'Oswald2015')\n",
      "50 (641, '6EJKG24B', 'Antil2023')\n",
      "51 (2025, '6G2NIYY2', 'Aubin2009')\n",
      "52 (586, '6KGNZXBY', 'Ralphs2009')\n",
      "53 (523, '6KLGI39Z', 'Perez2006')\n",
      "54 (1024, '6RXCS6CX', 'Borkar1997')\n",
      "55 (695, '6TXXEUBF', 'Liu2022')\n",
      "56 (2274, '6WN9AGEY', 'Touchette')\n",
      "57 (135, '6XJXGAJK', 'Lebedev2003')\n",
      "58 (719, '72DJH6VL', 'Dagreou2022')\n",
      "59 (2049, '733KMJE5', 'Boyd2009')\n",
      "60 (824, '74QDVUN8', 'Grazzi2020')\n",
      "61 (337, '79HTE7YU', 'Haddou2020')\n",
      "62 (275, '7CEDI6HD', 'Clement2016')\n",
      "63 (437, '7CZVWJYY', 'Dokmanic2013')\n",
      "64 (550, '7FCS6PNW', 'Shah2018')\n",
      "65 (499, '7FKHG94Q', 'Cho2003')\n",
      "66 (507, '7GYZYRFR', 'Korhonen2016')\n",
      "67 (273, '7I2IXUME', 'Li2017')\n",
      "68 (778, '7M5M5AFA', 'Taninmis2019')\n",
      "69 (438, '7NVZXNFN', 'Durgin2019')\n",
      "70 (6520, '7UWS8YYW', 'Neumann1979')\n",
      "71 (2425, '7VJIRWUM', 'Grimmett2007')\n",
      "72 (701, '7WAJK947', 'Tarzanagh2022')\n",
      "73 (2064, '7YBJL2W4', 'Allen2014')\n",
      "74 (1018, '7YSSSS5Y', 'Forst2010')\n",
      "75 (521, '82ZMDNIX', 'Wang2006')\n",
      "76 (20, '84HAVXWN', 'Mitzenmacher1988')\n",
      "77 (2405, '87MZZDKG', 'Kreyszig1989')\n",
      "78 (697, '87SPSVBW', 'Chen2022c')\n",
      "79 (312, '88HI884G', 'Dai2015')\n",
      "80 (930, '88L5JPY3', 'Lei2018')\n",
      "81 (295, '89QEPBBX', 'Bredies2008')\n",
      "82 (2077, '89X4LYKI', 'Livan2018')\n",
      "83 (6504, '8FISQTEU', 'zotero-6504')\n",
      "84 (149, '8GHW6KVJ', 'Ghanem2017')\n",
      "85 (717, '8GI6HS77', 'Suonpera2022')\n",
      "86 (47, '8JHH63I2', 'Billinge2019')\n",
      "87 (2424, '8KUMGKB2', 'Hajek2015')\n",
      "88 (343, '8M6PSFR3', 'Sheriff2020')\n",
      "89 (475, '8P3KFZIB', 'Beck2009')\n",
      "90 (515, '8SLY8AR8', 'Xu2018')\n",
      "91 (41, '8T89CPZT', 'Challamel2000')\n",
      "92 (806, '8TDTCJW6', 'Li2020')\n",
      "93 (560, '8UQYDKBD', 'Mandal2018')\n",
      "94 (872, '8VNM7A2I', 'Dempe2012')\n",
      "95 (1841, '8ZG6NSPD', 'Rudin2008')\n",
      "96 (2271, '94Y24AA9', 'Lucet1997')\n",
      "97 (436, '95D9N8YW', 'Censor2001')\n",
      "98 (331, '9CXPLTPC', 'Hefny2017')\n",
      "99 (6494, '9DHKR7F5', 'Ross2013a')\n",
      "100 (2290, '9EY452RP', 'Hoeksema2023')\n",
      "101 (487, '9FRTCCVW', 'Candes2008')\n",
      "102 (2462, '9Q4PUE3N', 'Thorpe')\n",
      "103 (2213, '9R87NLTA', 'Gardner2002')\n",
      "104 (974, '9VZSGX4V', 'Du2020')\n",
      "105 (881, '9Y76YMZI', 'Rockafellar1993')\n",
      "106 (133, 'A36NLYGH', 'Beilina1390')\n",
      "107 (419, 'A6NAGGAQ', 'Tewarson1969')\n",
      "108 (976, 'ACY2Z6BX', 'Yuan2014')\n",
      "109 (651, 'ADGV57P4', 'Lu2023')\n",
      "110 (971, 'AEKQM9AP', 'Hegde2019')\n",
      "111 (691, 'AGQTPT2D', 'Hu2022')\n",
      "112 (28, 'AH3PKCWJ', 'Press2000')\n",
      "113 (776, 'AJWYDT2V', 'Bai2022a')\n",
      "114 (415, 'AL9X9BHE', 'Elfving1980')\n",
      "115 (2097, 'AM5T36AN', 'Terrell1999')\n",
      "116 (2539, 'ASN6GNPX', 'Perko2009')\n",
      "117 (1000, 'AVTWNRQ4', 'Hong2023')\n",
      "118 (1682, 'AZCSU4V9', 'Khalil2023')\n",
      "119 (1574, 'AZSERNWI', 'Ailon2008')\n",
      "120 (2223, 'B3M9YCEK', 'Milenkovic2007')\n",
      "121 (879, 'B64TVTKP', 'Iyer')\n",
      "122 (361, 'BEYVQHE4', 'Chen2001')\n",
      "123 (993, 'BGFRXZZU', 'Hataya2023')\n",
      "124 (2166, 'BGW8YZT4', 'keshav')\n",
      "125 (855, 'BIBXG6VL', 'Liu2016')\n",
      "126 (2549, 'BLK28USM', 'Howe2012')\n",
      "127 (669, 'BMZEGTY7', 'Lu2023a')\n",
      "128 (792, 'BNVSNCSI', 'Yousefian2021')\n",
      "129 (493, 'BNY6DVKZ', 'Ikeda2019')\n",
      "130 (2284, 'BPS4WRHB', 'Chizat')\n",
      "131 (991, 'BQAQ6S6P', 'Domke2012')\n",
      "132 (356, 'BUHNFE55', 'Cegielski2020')\n",
      "133 (741, 'BWLLQG63', 'Chen2021')\n",
      "134 (544, 'BXLPZVV6', 'Pelillo2012')\n",
      "135 (987, 'BXQFVNZG', 'Deutsch2001')\n",
      "136 (39, 'C24R4FL4', 'Mallick2011')\n",
      "137 (2164, 'C25WS4RA', 'Roscoe')\n",
      "138 (374, 'C74YT8SQ', 'Chen2012')\n",
      "139 (804, 'C7M2VSLM', 'Tong2022')\n",
      "140 (277, 'C85XBEGL', 'Pedregal2017')\n",
      "141 (2071, 'CA5RTJC3', 'Danilova2022')\n",
      "142 (49, 'CDY9R2U2', 'Palmieri2020')\n",
      "143 (675, 'CESDTW8Q', 'Jiang2022')\n",
      "144 (667, 'CFXI8JAW', 'Guglielmi2023')\n",
      "145 (540, 'CQ7HWCU7', 'Stewart1999')\n",
      "146 (1846, 'CQFE96PZ', 'Bertsekas2009')\n",
      "147 (582, 'CRLGH2XJ', 'Mjolsness1990')\n",
      "148 (2116, 'CUN9HMSZ', 'Meshulam1996')\n",
      "149 (2514, 'CYF3H42W', 'Zhao2022')\n",
      "150 (302, 'CYTFSGT7', 'Fortin2000')\n",
      "151 (932, 'CZL7RGSJ', 'Tanaka1981')\n",
      "152 (388, 'D4MI8XVT', 'Tanabe1971')\n",
      "153 (731, 'DCNALP8B', 'Friedemann2022')\n",
      "154 (341, 'DH56LMMZ', 'Candes2008a')\n",
      "155 (335, 'DIQ7LN5L', 'DeLoera2017')\n",
      "156 (2297, 'DL84AV32', 'Borkar2005')\n",
      "157 (594, 'DM8QYP53', 'Ben-ayed1990')\n",
      "158 (760, 'DPGI86W5', 'Sundar2021')\n",
      "159 (2329, 'DPXUVVDG', 'Bietti2021')\n",
      "160 (1026, 'DSBFU4IC', 'Avrachenkov2022')\n",
      "161 (687, 'DT386PQ6', 'Sharrock2022')\n",
      "162 (665, 'DXKQUNMN', 'Cunis2022')\n",
      "163 (893, 'DYHMYXDI', 'Parikh2014')\n",
      "164 (2463, 'E2XJGMA3', 'Bach')\n",
      "165 (491, 'E3I2UYZD', 'Combettes2005')\n",
      "166 (727, 'E557DDDB', 'Li2022')\n",
      "167 (768, 'E64CWC84', 'Buchheim2022')\n",
      "168 (509, 'E7PULVTQ', 'Saaty2013')\n",
      "169 (592, 'EAHRA8M8', 'Colson2005')\n",
      "170 (2289, 'EB7CBIL2', 'Ma2019')\n",
      "171 (1951, 'EC8HECN7', 'Tihomirov1990')\n",
      "172 (308, 'EEVMXG2E', 'Lin2015')\n",
      "173 (756, 'EFARBJCI', 'Crockett2022')\n",
      "174 (473, 'ENVTBAC7', 'Popa2018')\n",
      "175 (709, 'EST4982Y', 'Shirai2022')\n",
      "176 (610, 'ET3MS7NC', 'Gale2016')\n",
      "177 (2401, 'EXSNRXTC', 'Kolmogorov1954')\n",
      "178 (409, 'EYTHARPP', 'Dey2018')\n",
      "179 (6535, 'F79MNIM5', 'Ekeland2006')\n",
      "180 (588, 'F7I8V8EZ', 'WEn1991')\n",
      "181 (633, 'F86H99HR', 'Sinha2018')\n",
      "182 (647, 'FA7WG6GX', 'Zhou2022')\n",
      "183 (808, 'FCM6R2BF', 'Chang2021')\n",
      "184 (279, 'FQAEDEPB', 'Kumar2020')\n",
      "185 (663, 'FQB6LZJ4', 'Cleach2022')\n",
      "186 (564, 'FQV6EMP5', 'Dempe1987')\n",
      "187 (737, 'FTX5EYAJ', 'Zucchet2022')\n",
      "188 (469, 'FUT5TMMB', 'Nutini2016')\n",
      "189 (1022, 'FZ2J6UW9', 'Colson2007')\n",
      "190 (2323, 'G5RGAS9N', 'Zhao2023')\n",
      "191 (2427, 'G6MQSPLP', 'K2020')\n",
      "192 (989, 'G83MTYHE', 'Finn2017')\n",
      "193 (6525, 'G8AXY2EP', 'Jurg1992')\n",
      "194 (325, 'G9459S3D', 'Zhao2020')\n",
      "195 (1892, 'G9GAP343', 'Rajeswaran2019')\n",
      "196 (671, 'GDC37DJN', 'Chen2023')\n",
      "197 (723, 'GFD27SSZ', 'Huang2022')\n",
      "198 (6526, 'GH7IVJP2', 'Raghavan1970')\n",
      "199 (452, 'GI6925RD', 'Vandenberghe2013')\n",
      "200 (301, 'GIUQPQU6', 'Rockafellar1973')\n",
      "201 (788, 'GL3T9J78', 'Sinha2020')\n",
      "202 (2076, 'GLKPGHK9', 'Edelman2005')\n",
      "203 (963, 'GUWQGCPC', 'Mansour2013')\n",
      "204 (941, 'GVSL54Z6', 'Mousavi2016')\n",
      "205 (896, 'GX8DJPYF', 'Das2022')\n",
      "206 (2083, 'GXRUTHIA', 'Quadrat2006a')\n",
      "207 (2068, 'H4SILD9W', 'Akhtar2022')\n",
      "208 (961, 'H6ZP6LB5', 'Donoho2006')\n",
      "209 (2315, 'HC48X7LX', 'Neamen2010')\n",
      "210 (1029, 'HG63K2JX', 'Chen2022')\n",
      "211 (495, 'HG8KKJXW', 'Shah2018a')\n",
      "212 (433, 'HHLDD6VN', 'Bai2019')\n",
      "213 (2074, 'HHSUQXD6', 'Bush2011')\n",
      "214 (703, 'HINCII67', 'Yu2023')\n",
      "215 (270, 'HJ2GJUKM', 'Stuart2017')\n",
      "216 (538, 'HLCYPHXQ', 'Steinerberger2021a')\n",
      "217 (525, 'HM8M7WRJ', 'Wang2017')\n",
      "218 (450, 'HQUMWS5X', 'Li2018')\n",
      "219 (6523, 'HUFFMRIC', 'Nisan2007')\n",
      "220 (2475, 'HVV5DMT2', 'Ross2013')\n",
      "221 (887, 'HYPCVZJM', 'Brezhneva2012')\n",
      "222 (1014, 'HZ6RZ4WV', 'Fletcher2000')\n",
      "223 (546, 'HZTU3C3U', 'Jiao2017')\n",
      "224 (519, 'I323E2P2', 'Emrouznejad2017')\n",
      "225 (764, 'I5CUKMSD', 'Ma2021')\n",
      "226 (2206, 'IBGVHNXB', 'Borkar2023a')\n",
      "227 (501, 'IFISIJDE', 'Zyoud2017')\n",
      "228 (353, 'IME9ZZQJ', 'Necoara2019')\n",
      "229 (677, 'IQ825D6K', 'Shen2022')\n",
      "230 (840, 'IQUPQ2Y2', 'Mori2021')\n",
      "231 (758, 'IWCFKRWW', 'Liu2021')\n",
      "232 (2426, 'IWI8WTJT', 'Kumar2012')\n",
      "233 (1980, 'IXUQD4AS', 'Udell2016')\n",
      "234 (816, 'IZG9TLMD', 'Seth2021')\n",
      "235 (735, 'IZJ6SK2J', 'Dyro2022')\n",
      "236 (460, 'J3D4KXAP', 'Nesterov2007')\n",
      "237 (794, 'J3F734QF', 'Wang2020')\n",
      "238 (497, 'J3L29JHJ', 'Nagahara2020')\n",
      "239 (2001, 'J6T5QPNQ', 'DiGuglielmo1977')\n",
      "240 (928, 'J6UB5VSV', 'Tomkins1968')\n",
      "241 (2520, 'J8RIPNJ2', 'Salsa2016')\n",
      "242 (790, 'JBN9BIKY', 'Benko2021')\n",
      "243 (6534, 'JDDMSGAY', 'Axler2024')\n",
      "244 (844, 'JFMRWU7H', 'Sun2022')\n",
      "245 (2420, 'JHARL7CQ', 'Malliavin1995')\n",
      "246 (752, 'JLKZL69I', 'Bao2021')\n",
      "247 (772, 'JLYLC8QJ', 'Basciftci2020')\n",
      "248 (56, 'JP828P7I', 'Vincent1990')\n",
      "249 (304, 'JQV64ZJ9', 'Figueiredo2009')\n",
      "250 (26, 'JRFNKVVT', 'Andrew1998')\n",
      "251 (562, 'JX3BNSI3', 'Centre1985')\n",
      "252 (1012, 'K3B6252W', 'Bazaraa2009')\n",
      "253 (529, 'K3G2NVWK', 'Moreno-jimenez2016')\n",
      "254 (645, 'K3KMVWJU', 'Li2023a')\n",
      "255 (673, 'KAE6K34D', 'Bai2022')\n",
      "256 (503, 'KCGTZC8X', 'Sciences2014')\n",
      "257 (2, 'KHW2498F', 'Polak2017')\n",
      "258 (91, 'KMHD79T9', 'Hopenhayn2016')\n",
      "259 (14, 'KN3GNAXU', 'Acharya')\n",
      "260 (980, 'KNYR4QI4', 'Osher2023')\n",
      "261 (98, 'KVHB88B3', 'Pitchik2009')\n",
      "262 (810, 'KW42Z4UD', 'Ji2020')\n",
      "263 (782, 'KX88B488', 'Mordukhovich2020')\n",
      "264 (870, 'KX96NT9Y', 'Lin2014')\n",
      "265 (2479, 'KYBHKAVS', 'Timoshenko2009')\n",
      "266 (748, 'KYW7LMT4', 'Poon2021')\n",
      "267 (6502, 'L67A9ZG3', 'oksendal2013')\n",
      "268 (2104, 'L6GK6MFG', 'Evans')\n",
      "269 (2141, 'L84AZXSF', 'Lasserre2002')\n",
      "270 (635, 'LBNYQCYI', 'Helou2017')\n",
      "271 (441, 'LDBE7W2Q', 'Lunglmayr2018')\n",
      "272 (2142, 'LFHGKH5R', 'zotero-2142')\n",
      "273 (1977, 'LGIPAX7M', 'Bonnans2019')\n",
      "274 (2409, 'LHNL6N4K', 'Brezis2011')\n",
      "275 (1004, 'LK4LNP5M', 'Ghadimi2018')\n",
      "276 (985, 'LMIT9F5F', 'Haurie1990')\n",
      "277 (578, 'LUQX94JI', 'Carrasco-gutierrez2019')\n",
      "278 (347, 'LVTDCDNJ', 'Strohmer2009')\n",
      "279 (798, 'LXJZFP6M', 'Gao2020')\n",
      "280 (30, 'LYZ2SXKL', 'Cotter2009')\n",
      "281 (1937, 'M5QUNHC3', 'Strassen1969')\n",
      "282 (570, 'M5ZNE2YE', 'Sriperumbudur2012')\n",
      "283 (580, 'M7S7S55S', 'Lipp2016')\n",
      "284 (1940, 'M8742NRY', 'Bertsekas2016')\n",
      "285 (784, 'MAVUFTDA', 'Ye2020')\n",
      "286 (659, 'MB3PV964', 'Mehlitz2022')\n",
      "287 (729, 'MCJ7CNMD', 'En-naciri2022')\n",
      "288 (429, 'MF96NTJF', 'Aly2016')\n",
      "289 (37, 'MHWV4VWA', 'Kac1966')\n",
      "290 (2062, 'MJBT3QGE', 'Bressan2014')\n",
      "291 (456, 'MLJV6IUI', 'Nesterov2021')\n",
      "292 (407, 'MMP9VPRQ', 'Lin2020')\n",
      "293 (417, 'MT6UGWKU', 'Censor1981')\n",
      "294 (1985, 'MVNAP3IH', 'Tardella1990')\n",
      "295 (2139, 'MYG5HYEB', 'DeOliveira2020')\n",
      "296 (937, 'N44RWBNR', 'Ikeda2021')\n",
      "297 (628, 'N4F8UE57', 'Chatterjee2005')\n",
      "298 (2081, 'NAZ52QSA', 'Quadrat2006')\n",
      "299 (1930, 'NDAXQPTT', 'okten2005')\n",
      "300 (2147, 'NDDCVQTC', 'zotero-2147')\n",
      "301 (959, 'NDN3GLWN', 'Deutsch1997')\n",
      "302 (2145, 'NEVPSSTE', 'Polyakova')\n",
      "303 (1031, 'NF9328RX', 'Ghadimi2013')\n",
      "304 (2066, 'NIS9MWY6', 'Couellan2016')\n",
      "305 (683, 'NK5ANRRN', 'Chen2022a')\n",
      "306 (2113, 'NUIVC8XC', 'Kirchheim2009')\n",
      "307 (442, 'NW34PSF3', 'Qu2018')\n",
      "308 (22, 'NYN9JAFT', 'Cai2006')\n",
      "309 (2263, 'P3Q77HTP', 'Griewank1990')\n",
      "310 (35, 'P6YBSNEN', 'Page2007')\n",
      "311 (527, 'P8SC88WX', 'Yang2019')\n",
      "312 (802, 'PCAWQ2X3', 'Shaker2021')\n",
      "313 (423, 'PF5E9JAI', 'Censor1983')\n",
      "314 (711, 'PIINZZSZ', 'Bian2022')\n",
      "315 (369, 'PSU4NWDX', 'Nagahara2016')\n",
      "316 (1933, 'PZQ39PPS', 'Wasow1952')\n",
      "317 (255, 'Q7YDEKGJ', 'Yosida1967')\n",
      "318 (2219, 'QAXRATHA', 'Peternell2007')\n",
      "319 (439, 'QB74Q2ZG', 'Lieb2020')\n",
      "320 (852, 'QD2ZTDNF', 'Brannstrom2016')\n",
      "321 (2008, 'QDVZRKW6', 'Fradelizi2017')\n",
      "322 (2159, 'QHYBBJVD', 'zotero-2159')\n",
      "323 (826, 'QLGS3FMX', 'Kontonis2020')\n",
      "324 (462, 'QM8QLV4W', 'HanneKekkonen2019')\n",
      "325 (2545, 'QME944RQ', 'Santambrogio2015')\n",
      "326 (425, 'QMPN8RRB', 'Natterer2001')\n",
      "327 (45, 'QT7I59WW', 'Kabanikhin2008')\n",
      "328 (818, 'QTV3M7T9', 'Ji2021a')\n",
      "329 (2091, 'QWUDCTW7', 'Lewis2016')\n",
      "330 (2246, 'QYXVKXWR', 'Abbasi2018')\n",
      "331 (4, 'QZ4IPAQV', 'Polak2016')\n",
      "332 (859, 'R22I3PLQ', 'Ye1997')\n",
      "333 (542, 'R25M9S8R', 'Stewart1998')\n",
      "334 (1008, 'R2GG9C3C', 'Bazaraa2006')\n",
      "335 (1859, 'R37HP73W', 'Boyd2004')\n",
      "336 (780, 'R4XU57TF', 'Mehlitz2020')\n",
      "337 (935, 'R69KWBZB', 'Crandall1971')\n",
      "338 (584, 'R6HPJ6C5', 'Rangarajan1996')\n",
      "339 (1990, 'R8NYDN9B', 'Cassels1975')\n",
      "340 (754, 'RCTEHWMF', 'Fan2021')\n",
      "341 (6513, 'REDSUE33', 'Li2020a')\n",
      "342 (365, 'RF983IJ3', 'Bernstein2020')\n",
      "343 (1961, 'RGL5BVFW', 'Hardt2018')\n",
      "344 (143, 'RJQX4CAD', 'Eigen1988')\n",
      "345 (12, 'RK9FV68J', 'Aminikhanghahi2017')\n",
      "346 (380, 'RLMUI567', 'Kaczmarz1937')\n",
      "347 (1968, 'RNYA5SAG', 'Borkar2023')\n",
      "348 (1988, 'RPTLPDGS', 'Kerdreux2018')\n",
      "349 (590, 'RPU6LD4M', 'Yeh1996')\n",
      "350 (2021, 'RPWK3AG3', 'Starr2008')\n",
      "351 (2267, 'RQLTTV5T', 'Lucet1996')\n",
      "352 (147, 'RRJJLKRQ', 'Diaconis2009')\n",
      "353 (2088, 'RSZDH8XB', 'Quadrat2003a')\n",
      "354 (639, 'RUGZRURD', 'Dagreou2023')\n",
      "355 (477, 'RVDFLBZR', 'Foucart2013')\n",
      "356 (832, 'RZVEM7E9', 'Gutfraind2011')\n",
      "357 (685, 'S68TSLNF', 'Chen2022b')\n",
      "358 (596, 'SD7778N4', 'Dempe2020')\n",
      "359 (2411, 'SG3VP3PZ', 'Balamurugan2019')\n",
      "360 (2535, 'SHUUHMFJ', '2013a')\n",
      "361 (1932, 'SI7RCS5P', 'Acebron2020')\n",
      "362 (517, 'SJSRIGG4', 'BanaECosta2008')\n",
      "363 (511, 'SRTWUGVT', 'Steinerberger2021')\n",
      "364 (2092, 'SSG4LNHE', 'Terrell2018')\n",
      "365 (814, 'SUTM5Q9U', 'Picallo2022')\n",
      "366 (448, 'SUWRC5Y9', 'Moorman2021')\n",
      "367 (1556, 'SX9HFZV6', 'Ailon2012')\n",
      "368 (689, 'SXLZI77J', 'Gao2022a')\n",
      "369 (2162, 'SYGSGKBS', 'Whitesides2004')\n",
      "370 (378, 'SYH8KZNE', 'Lorenz2015')\n",
      "371 (978, 'SYLW8VD7', 'Dax2006')\n",
      "372 (2265, 'SZFQTK9W', 'Brighi1994')\n",
      "373 (822, 'T23ECC4V', 'Kosmas2022')\n",
      "374 (2280, 'T3XYVZIP', 'Dyke2014')\n",
      "375 (31, 'T57E9LWP', 'Kerfriden2019')\n",
      "376 (842, 'T5IYSD49', 'Cao2022')\n",
      "377 (345, 'T6ALGK5Z', 'Chen2018')\n",
      "378 (713, 'T9IPBG26', 'Pan2022')\n",
      "379 (2248, 'TAC5CCVG', 'Li2021')\n",
      "380 (2388, 'TAGXADRL', 'Saxe2001')\n",
      "381 (944, 'TD6I25LG', 'Bubeck2015')\n",
      "382 (2423, 'TDBC275P', 'Billingsley1995')\n",
      "383 (505, 'TFA9C9J3', 'Bian2017')\n",
      "384 (2060, 'TIHSDGWC', 'Bulicek2014')\n",
      "385 (857, 'TIMTYM6M', 'Holler2018')\n",
      "386 (2249, 'TKXBP9NY', 'Oberman2008')\n",
      "387 (705, 'TL4JSHM6', 'Sow2022')\n",
      "388 (2312, 'TL5MUV7C', 'Reddy2014')\n",
      "389 (139, 'TLRSTBP4', 'Vishnoi2013')\n",
      "390 (969, 'TLT6W8WR', 'Moscoso2012')\n",
      "391 (102, 'TM77ICP7', 'Nov2018')\n",
      "392 (618, 'TM8EZDRS', 'unlu1987')\n",
      "393 (548, 'TPFVSV5G', 'Jin2019')\n",
      "394 (489, 'TRNCG7SV', 'Wilson2018')\n",
      "395 (2305, 'TV7EF8NR', 'Sterman2000')\n",
      "396 (956, 'TV93ENB2', 'Schopfer2019')\n",
      "397 (413, 'TWW88JF8', 'Gubin1967')\n",
      "398 (431, 'TX46X33P', 'Bai2018')\n",
      "399 (339, 'TX9IQDAE', 'Haddock2019')\n",
      "400 (2052, 'TYT7VIK5', 'Duchi2021')\n",
      "401 (739, 'U4FTS88Y', 'Huang2021')\n",
      "402 (850, 'U4WGC3PK', 'Ouattara2018')\n",
      "403 (566, 'U7EYCXPJ', 'Yuille2001')\n",
      "404 (707, 'U7VSMGYS', 'Ghosh2022')\n",
      "405 (2415, 'U8DG8YD2', 'Folland1999')\n",
      "406 (965, 'U9D53TQ3', 'Schmidt2015')\n",
      "407 (2050, 'UAMAF28K', 'Blasjo2005')\n",
      "408 (558, 'UEQL35AD', 'Pardalos2017')\n",
      "409 (314, 'UIKWGIQ8', 'Terra2012')\n",
      "410 (1989, 'UITHCF6S', 'Aubin1976')\n",
      "411 (661, 'UL4U7BAC', 'Doron2022')\n",
      "412 (891, 'ULQGKA8I', 'Borwein1981')\n",
      "413 (2526, 'ULZP4LQ7', 'Jiang2023')\n",
      "414 (290, 'UN4UEVVP', 'Evans2010')\n",
      "415 (2509, 'UNBW8EBI', 'Raginsky2019')\n",
      "416 (848, 'UQ3ZP9WV', 'Sinha2020a')\n",
      "417 (838, 'USKLZNPB', 'Khanduri2021')\n",
      "418 (2106, 'UVDSIKLJ', 'Netrapalli2014')\n",
      "419 (889, 'UX3MGI8B', 'Dutta2011')\n",
      "420 (574, 'UZMJ8VMK', 'Ahmadi2018')\n",
      "421 (812, 'V296MRRP', 'Borsos2021')\n",
      "422 (2247, 'V3I97W65', 'Carlier2012')\n",
      "423 (1020, 'V47UFD67', 'Nocedal2006')\n",
      "424 (513, 'V67NRB5K', 'Saaty2003')\n",
      "425 (721, 'V6KYW76N', 'Lee2022')\n",
      "426 (2317, 'V7HQ9MXB', 'Pritchard2010')\n",
      "427 (141, 'V7IPA6PF', 'Schindler2003')\n",
      "428 (531, 'VFP28VF9', 'Feichtinger1992')\n",
      "429 (386, 'VGZGUCNJ', 'Borgard2020')\n",
      "430 (836, 'VLEB98Z3', 'Mackay2019')\n",
      "431 (483, 'VMURAD7R', 'Nesterov2018')\n",
      "432 (467, 'VNIYDAD9', 'Huang2020')\n",
      "433 (359, 'VPRN5GHF', 'Lorenz2018')\n",
      "434 (2480, 'VPXP2AK5', 'Spivak1965')\n",
      "435 (458, 'VSMIFTFG', 'Nesterov2013')\n",
      "436 (432, 'VSS69KW9', 'Bai2018a')\n",
      "437 (363, 'VUTD2L26', 'Lorenz2014')\n",
      "438 (1934, 'VUTZNP8L', 'Forsythe1950')\n",
      "439 (947, 'VXNQ6MRC', 'Aggarwal2014')\n",
      "440 (435, 'VZRC3QKE', 'Censor2001a')\n",
      "441 (145, 'W34DX4WH', 'Panageas2016')\n",
      "442 (534, 'W4QW6JRQ', 'Crandall1951')\n",
      "443 (2102, 'W53XHJFE', 'Evans2021a')\n",
      "444 (576, 'W5LFHVT6', 'Sriperumbudur2009')\n",
      "445 (883, 'WBVL9TSH', 'Borwein2016')\n",
      "446 (405, 'WDLVFYRL', 'Natterer2001a')\n",
      "447 (465, 'WETI5VIH', 'Guan2020')\n",
      "448 (1931, 'WKEIB3P2', 'Wu2019')\n",
      "449 (18, 'WQ2CIXZK', 'Argyropoulos2015')\n",
      "450 (2464, 'WQHNK93Z', 'Jain2017')\n",
      "451 (292, 'WRHMBIR4', 'Benning2018')\n",
      "452 (915, 'WRNQAA7T', 'Ehrhardt2021')\n",
      "453 (828, 'WSU6GPVE', 'Roh2020')\n",
      "454 (786, 'WTDSXS2A', 'Sessa2020')\n",
      "455 (322, 'WVB3X3QG', 'Aharoni1984')\n",
      "456 (774, 'WW7N4GQ6', 'Fliege2021')\n",
      "457 (643, 'WWPIMYFJ', 'Alesiani2023')\n",
      "458 (297, 'WY8SJYFL', 'Hestenes1969')\n",
      "459 (43, 'X5NTIP8Q', 'Flanagan1959')\n",
      "460 (2465, 'X5T7DYXD', 'Das2017')\n",
      "461 (2469, 'X6FZTY3K', 'Srinath2009')\n",
      "462 (6537, 'X6U8BEQ8', 'Parthasarathy1971')\n",
      "463 (653, 'X9GETDTD', 'Shen2023')\n",
      "464 (762, 'XCLF7C4H', 'Ke2022')\n",
      "465 (1999, 'XCXC32JT', 'Khan1974')\n",
      "466 (16, 'XD2QDF6R', 'Truong2018')\n",
      "467 (649, 'XDKFE4C6', 'Ji2023')\n",
      "468 (411, 'XE8H3Y3H', 'Hazimeh2020')\n",
      "469 (622, 'XGAGTYCV', 'Gallo1977')\n",
      "470 (24, 'XGN74PFB', 'PeterWriggers2012')\n",
      "471 (615, 'XICLQE69', 'Wen1989')\n",
      "472 (1010, 'XLNV8YQT', 'Padregal2003')\n",
      "473 (2245, 'XMRSNPK4', 'Oberman2007')\n",
      "474 (972, 'XP2LVELR', 'Mailhe2011')\n",
      "475 (693, 'XQ492XTK', 'Ding2022')\n",
      "476 (608, 'XSW4NMKH', 'Blair1992')\n",
      "477 (58, 'XVP6ESUA', 'Vishnoi2015')\n",
      "478 (6541, 'XVZDCPUL', 'Rockafellar1996')\n",
      "479 (2250, 'Y2ZTWYCK', 'Oberman2017')\n",
      "480 (333, 'Y3XFX8EP', 'Tropp2016')\n",
      "481 (2085, 'Y5ZRIZFC', 'Quadrat2003')\n",
      "482 (996, 'Y9MKWRDB', 'Shapiro2021')\n",
      "483 (329, 'YBSBB3MW', 'Richtarik2017')\n",
      "484 (454, 'YEPXJDM3', 'StefanKidermann')\n",
      "485 (8, 'YGGUDQCL', 'Polak1987')\n",
      "486 (655, 'YIBEDJ89', 'Li2023')\n",
      "487 (657, 'YJRCTNBF', 'Huang2023')\n",
      "488 (2211, 'YPF8RK89', 'Schneider2014')\n",
      "489 (637, 'YPVY67LG', 'Xu2023')\n",
      "490 (715, 'YS9ZQ4HQ', 'Han2021')\n",
      "491 (2512, 'YVSPZIPB', 'Liberzon2012')\n",
      "492 (834, 'YYCFLDCQ', 'Ochs2016')\n",
      "493 (33, 'Z3P3I79V', 'Stuart2010')\n",
      "494 (479, 'Z5LR3UYK', 'Rektorys1977')\n",
      "495 (371, 'Z79VBMX2', 'Natarajan1995')\n",
      "496 (446, 'Z9S8PHGD', 'Crandall2014')\n",
      "497 (606, 'ZF6XYRPG', 'Bard1983')\n",
      "498 (2199, 'ZGBMABRG', 'Diakonikolas2019')\n",
      "499 (2293, 'ZHUPKXN2', 'Chizat2020')\n",
      "500 (743, 'ZHVKVS69', 'Huang2022a')\n",
      "501 (2251, 'ZI55TRFC', 'Oberman2011')\n",
      "502 (320, 'ZIJP72GB', 'Aboud2019')\n",
      "503 (725, 'ZJYHTBDY', 'Liu2022a')\n",
      "504 (2252, 'ZKNC9EJH', 'Vese1999')\n",
      "505 (556, 'ZMN6RFF5', 'Heckel2018')\n",
      "506 (310, 'ZSMZTCTJ', 'Kale2014')\n",
      "507 (733, 'ZTE5RHYL', 'Garcia2022')\n",
      "508 (61, 'ZV3MJCRK', 'Dixit2012')\n",
      "509 (281, 'ZWI5BE2X', 'Royden2010')\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "for x in temp:\n",
    "    if x[2]!=None:\n",
    "        print(i, x)\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce02e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item_id, item_key, field, value, citekey in conn.fetchall():\n",
    "    if item_id not in d:\n",
    "        d[item_id] = {'zotkey': item_key, 'alastnm': ''}\n",
    "    d[item_id][field] = value\n",
    "    d[item_id][citekey] = citekey"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
